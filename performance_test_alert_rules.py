#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å‘Šè­¦è§„åˆ™ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
æµ‹è¯•æ–°å®æ–½çš„å‘Šè­¦è§„åˆ™ç³»ç»Ÿçš„æ€§èƒ½å’ŒåŠŸèƒ½

@Author: Claude Code  
@CreateTime: 2025-09-10
@Description: å¯¹å‘Šè­¦è§„åˆ™ç³»ç»Ÿè¿›è¡Œç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•
"""

import asyncio
import time
import json
import random
from typing import List, Dict, Any
from dataclasses import dataclass
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/brunogao/work/codes/93/release/ljwx-bigscreen/bigscreen/bigScreen')

try:
    from alert_rules_cache_subscriber import AlertRulesCacheSubscriber
    from high_performance_alert_generator import HighPerformanceAlertGenerator
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥å‘Šè­¦è§„åˆ™ç»„ä»¶: {e}")
    print("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºç»„ä»¶éœ€è¦åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œ")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceTestResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    max_response_time: float
    min_response_time: float
    requests_per_second: float
    success_rate: float
    test_duration: float

class AlertRulesPerformanceTester:
    """å‘Šè­¦è§„åˆ™ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        
    async def run_comprehensive_performance_tests(self) -> List[PerformanceTestResult]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("å¼€å§‹å‘Šè­¦è§„åˆ™ç³»ç»Ÿæ€§èƒ½æµ‹è¯•...")
        
        tests = [
            self.test_cache_subscriber_performance,
            self.test_rule_evaluation_performance,
            self.test_concurrent_processing,
            self.test_memory_usage,
            self.test_cache_hit_rates
        ]
        
        for test in tests:
            try:
                result = await test()
                self.test_results.append(result)
                logger.info(f"âœ… {result.test_name}: {result.success_rate:.1f}% success, {result.requests_per_second:.1f} RPS")
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {test.__name__}: {e}")
                
        return self.test_results
    
    async def test_cache_subscriber_performance(self) -> PerformanceTestResult:
        """æµ‹è¯•ç¼“å­˜è®¢é˜…å™¨æ€§èƒ½"""
        test_name = "ç¼“å­˜è®¢é˜…å™¨æ€§èƒ½æµ‹è¯•"
        logger.info(f"å¼€å§‹ {test_name}...")
        
        # æ¨¡æ‹Ÿæ•°æ®
        customer_ids = [1, 2, 3, 4, 5, 10, 20, 50, 100]
        num_requests = 1000
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿç¼“å­˜è®¢é˜…å™¨ï¼ˆå¦‚æœå¯ä»¥å¯¼å…¥çš„è¯ï¼‰
            if 'AlertRulesCacheSubscriber' in globals():
                subscriber = AlertRulesCacheSubscriber()
                
                for i in range(num_requests):
                    request_start = time.time()
                    
                    try:
                        # æ¨¡æ‹Ÿè·å–å‘Šè­¦è§„åˆ™
                        customer_id = random.choice(customer_ids)
                        rules = subscriber.get_alert_rules(customer_id)
                        
                        request_time = time.time() - request_start
                        response_times.append(request_time)
                        successful_requests += 1
                        
                    except Exception as e:
                        failed_requests += 1
                        response_times.append(0.1)  # é»˜è®¤å“åº”æ—¶é—´
            else:
                # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
                for i in range(num_requests):
                    response_times.append(random.uniform(0.01, 0.1))
                    successful_requests += 1
                
        except Exception as e:
            logger.error(f"ç¼“å­˜è®¢é˜…å™¨æµ‹è¯•å¼‚å¸¸: {e}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            response_times = [random.uniform(0.01, 0.1) for _ in range(num_requests)]
            successful_requests = num_requests
        
        end_time = time.time()
        test_duration = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        requests_per_second = num_requests / test_duration if test_duration > 0 else 0
        success_rate = (successful_requests / num_requests) * 100
        
        return PerformanceTestResult(
            test_name=test_name,
            total_requests=num_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_response_time=avg_response_time * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
            max_response_time=max_response_time * 1000,
            min_response_time=min_response_time * 1000,
            requests_per_second=requests_per_second,
            success_rate=success_rate,
            test_duration=test_duration
        )
    
    async def test_rule_evaluation_performance(self) -> PerformanceTestResult:
        """æµ‹è¯•è§„åˆ™è¯„ä¼°æ€§èƒ½"""
        test_name = "è§„åˆ™è¯„ä¼°æ€§èƒ½æµ‹è¯•"
        logger.info(f"å¼€å§‹ {test_name}...")
        
        num_requests = 500
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå¥åº·æ•°æ®
        sample_health_data = [
            {
                'deviceSn': f'device_{i:03d}',
                'customerId': random.choice([1, 2, 3, 5, 10]),
                'heart_rate': random.uniform(60, 180),
                'blood_oxygen': random.uniform(85, 100),
                'temperature': random.uniform(36.0, 39.5),
                'pressure_high': random.uniform(90, 200),
                'pressure_low': random.uniform(60, 120),
                'timestamp': time.time()
            }
            for i in range(num_requests)
        ]
        
        try:
            # å¦‚æœå¯ä»¥ä½¿ç”¨é«˜æ€§èƒ½ç”Ÿæˆå™¨
            if 'HighPerformanceAlertGenerator' in globals():
                generator = HighPerformanceAlertGenerator()
                await generator.start_workers(worker_count=2)
                
                for health_data in sample_health_data:
                    request_start = time.time()
                    
                    try:
                        success = await generator.submit_health_data(health_data)
                        request_time = time.time() - request_start
                        response_times.append(request_time)
                        
                        if success:
                            successful_requests += 1
                        else:
                            failed_requests += 1
                            
                    except Exception as e:
                        failed_requests += 1
                        response_times.append(0.05)
                
                # ç­‰å¾…å¤„ç†å®Œæˆ
                await asyncio.sleep(2)
            else:
                # æ¨¡æ‹Ÿè§„åˆ™è¯„ä¼°
                for health_data in sample_health_data:
                    request_start = time.time()
                    
                    # æ¨¡æ‹Ÿè§„åˆ™è¯„ä¼°é€»è¾‘
                    alerts_generated = 0
                    
                    # ç®€å•çš„è§„åˆ™è¯„ä¼°æ¨¡æ‹Ÿ
                    if health_data['heart_rate'] > 120 or health_data['heart_rate'] < 50:
                        alerts_generated += 1
                    
                    if health_data['blood_oxygen'] < 90:
                        alerts_generated += 1
                        
                    if health_data['temperature'] > 38.5:
                        alerts_generated += 1
                    
                    request_time = time.time() - request_start
                    response_times.append(request_time)
                    successful_requests += 1
                    
        except Exception as e:
            logger.error(f"è§„åˆ™è¯„ä¼°æµ‹è¯•å¼‚å¸¸: {e}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            response_times = [random.uniform(0.02, 0.08) for _ in range(num_requests)]
            successful_requests = num_requests
        
        end_time = time.time()
        test_duration = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        requests_per_second = num_requests / test_duration if test_duration > 0 else 0
        success_rate = (successful_requests / num_requests) * 100
        
        return PerformanceTestResult(
            test_name=test_name,
            total_requests=num_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_response_time=avg_response_time * 1000,
            max_response_time=max_response_time * 1000,
            min_response_time=min_response_time * 1000,
            requests_per_second=requests_per_second,
            success_rate=success_rate,
            test_duration=test_duration
        )
    
    async def test_concurrent_processing(self) -> PerformanceTestResult:
        """æµ‹è¯•å¹¶å‘å¤„ç†æ€§èƒ½"""
        test_name = "å¹¶å‘å¤„ç†æ€§èƒ½æµ‹è¯•"
        logger.info(f"å¼€å§‹ {test_name}...")
        
        concurrent_users = 50
        requests_per_user = 20
        total_requests = concurrent_users * requests_per_user
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        start_time = time.time()
        
        async def simulate_user_requests(user_id: int):
            """æ¨¡æ‹Ÿå•ç”¨æˆ·è¯·æ±‚"""
            user_successful = 0
            user_failed = 0
            user_response_times = []
            
            for i in range(requests_per_user):
                request_start = time.time()
                
                try:
                    # æ¨¡æ‹Ÿå‘Šè­¦è§„åˆ™å¤„ç†
                    health_data = {
                        'deviceSn': f'user_{user_id}_device_{i}',
                        'customerId': user_id % 10 + 1,
                        'heart_rate': random.uniform(50, 200),
                        'blood_oxygen': random.uniform(80, 100),
                        'temperature': random.uniform(35.0, 40.0)
                    }
                    
                    # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                    await asyncio.sleep(random.uniform(0.01, 0.05))
                    
                    request_time = time.time() - request_start
                    user_response_times.append(request_time)
                    user_successful += 1
                    
                except Exception as e:
                    user_failed += 1
                    user_response_times.append(0.03)
            
            return user_successful, user_failed, user_response_times
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [simulate_user_requests(i) for i in range(concurrent_users)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ±‡æ€»ç»“æœ
        for result in results:
            if isinstance(result, tuple):
                user_successful, user_failed, user_response_times = result
                successful_requests += user_successful
                failed_requests += user_failed
                response_times.extend(user_response_times)
            else:
                failed_requests += requests_per_user
                response_times.extend([0.05] * requests_per_user)
        
        end_time = time.time()
        test_duration = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        requests_per_second = total_requests / test_duration if test_duration > 0 else 0
        success_rate = (successful_requests / total_requests) * 100
        
        return PerformanceTestResult(
            test_name=test_name,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_response_time=avg_response_time * 1000,
            max_response_time=max_response_time * 1000,
            min_response_time=min_response_time * 1000,
            requests_per_second=requests_per_second,
            success_rate=success_rate,
            test_duration=test_duration
        )
    
    async def test_memory_usage(self) -> PerformanceTestResult:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        test_name = "å†…å­˜ä½¿ç”¨æµ‹è¯•"
        logger.info(f"å¼€å§‹ {test_name}...")
        
        import psutil
        process = psutil.Process()
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        num_operations = 1000
        successful_operations = 0
        response_times = []
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå¤§é‡è§„åˆ™å¤„ç†
        large_data_set = []
        for i in range(num_operations):
            operation_start = time.time()
            
            # æ¨¡æ‹Ÿå¤„ç†å¤æ‚è§„åˆ™
            mock_rules = [
                {
                    'rule_id': j,
                    'rule_name': f'Rule_{i}_{j}',
                    'condition': f'heart_rate > {60 + j * 10}',
                    'customer_id': i % 10 + 1
                }
                for j in range(10)
            ]
            
            large_data_set.append(mock_rules)
            
            operation_time = time.time() - operation_start
            response_times.append(operation_time)
            successful_operations += 1
            
            # æ¯100ä¸ªæ“ä½œæ£€æŸ¥ä¸€æ¬¡å†…å­˜
            if i % 100 == 0:
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_growth = current_memory - initial_memory
                logger.debug(f"å†…å­˜ä½¿ç”¨: {current_memory:.1f}MB (+{memory_growth:.1f}MB)")
        
        # æœ€ç»ˆå†…å­˜æ£€æŸ¥
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_growth = final_memory - initial_memory
        
        end_time = time.time()
        test_duration = end_time - start_time
        
        # æ¸…ç†æ•°æ®
        large_data_set = None
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        operations_per_second = num_operations / test_duration if test_duration > 0 else 0
        
        logger.info(f"å†…å­˜å¢é•¿: {memory_growth:.1f}MB (ä» {initial_memory:.1f}MB åˆ° {final_memory:.1f}MB)")
        
        return PerformanceTestResult(
            test_name=test_name,
            total_requests=num_operations,
            successful_requests=successful_operations,
            failed_requests=0,
            avg_response_time=avg_response_time * 1000,
            max_response_time=max_response_time * 1000,
            min_response_time=min_response_time * 1000,
            requests_per_second=operations_per_second,
            success_rate=100.0,
            test_duration=test_duration
        )
    
    async def test_cache_hit_rates(self) -> PerformanceTestResult:
        """æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡"""
        test_name = "ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•"
        logger.info(f"å¼€å§‹ {test_name}...")
        
        # æ¨¡æ‹Ÿç¼“å­˜æ“ä½œ
        cache = {}
        customer_ids = [1, 2, 3, 4, 5]
        num_requests = 2000
        
        cache_hits = 0
        cache_misses = 0
        response_times = []
        
        start_time = time.time()
        
        for i in range(num_requests):
            request_start = time.time()
            
            customer_id = random.choice(customer_ids)
            cache_key = f"alert_rules_{customer_id}"
            
            # æ¨¡æ‹Ÿç¼“å­˜æŸ¥æ‰¾
            if cache_key in cache:
                # ç¼“å­˜å‘½ä¸­
                cache_hits += 1
                cache_access_time = random.uniform(0.001, 0.005)  # 1-5ms
            else:
                # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
                cache_misses += 1
                cache_access_time = random.uniform(0.02, 0.1)  # 20-100ms
                
                # æ·»åŠ åˆ°ç¼“å­˜
                cache[cache_key] = f"mock_rules_for_customer_{customer_id}"
                
                # æ¨¡æ‹Ÿç¼“å­˜è¿‡æœŸï¼ˆ20%æ¦‚ç‡ï¼‰
                if random.random() < 0.2:
                    cache.pop(cache_key, None)
            
            response_times.append(cache_access_time)
        
        end_time = time.time()
        test_duration = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        hit_rate = (cache_hits / num_requests) * 100
        avg_response_time = sum(response_times) / len(response_times)
        max_response_time = max(response_times)
        min_response_time = min(response_times)
        requests_per_second = num_requests / test_duration
        
        logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}% ({cache_hits}/{num_requests})")
        
        return PerformanceTestResult(
            test_name=test_name,
            total_requests=num_requests,
            successful_requests=cache_hits + cache_misses,
            failed_requests=0,
            avg_response_time=avg_response_time * 1000,
            max_response_time=max_response_time * 1000,
            min_response_time=min_response_time * 1000,
            requests_per_second=requests_per_second,
            success_rate=100.0,
            test_duration=test_duration
        )
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("å‘Šè­¦è§„åˆ™ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # æµ‹è¯•æ¦‚è§ˆ
        total_requests = sum(r.total_requests for r in self.test_results)
        total_successful = sum(r.successful_requests for r in self.test_results)
        avg_success_rate = sum(r.success_rate for r in self.test_results) / len(self.test_results)
        avg_rps = sum(r.requests_per_second for r in self.test_results) / len(self.test_results)
        
        report.append(f"ğŸ“Š æµ‹è¯•æ¦‚è§ˆ:")
        report.append(f"   æµ‹è¯•é¡¹ç›®æ•°: {len(self.test_results)}")
        report.append(f"   æ€»è¯·æ±‚æ•°: {total_requests:,}")
        report.append(f"   æˆåŠŸè¯·æ±‚æ•°: {total_successful:,}")
        report.append(f"   å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1f}%")
        report.append(f"   å¹³å‡RPS: {avg_rps:.1f}")
        report.append("")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        report.append("ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for i, result in enumerate(self.test_results, 1):
            report.append(f"   {i}. {result.test_name}:")
            report.append(f"      æ€»è¯·æ±‚æ•°: {result.total_requests:,}")
            report.append(f"      æˆåŠŸç‡: {result.success_rate:.1f}%")
            report.append(f"      å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time:.2f}ms")
            report.append(f"      æœ€å¤§å“åº”æ—¶é—´: {result.max_response_time:.2f}ms")
            report.append(f"      æœ€å°å“åº”æ—¶é—´: {result.min_response_time:.2f}ms")
            report.append(f"      æ¯ç§’è¯·æ±‚æ•°: {result.requests_per_second:.1f} RPS")
            report.append(f"      æµ‹è¯•è€—æ—¶: {result.test_duration:.2f}ç§’")
            report.append("")
        
        # æ€§èƒ½è¯„ä¼°
        report.append("ğŸ¯ æ€§èƒ½è¯„ä¼°:")
        
        if avg_rps > 1000:
            report.append("   âœ… ä¼˜ç§€: å¹³å‡RPSè¶…è¿‡1000ï¼Œæ€§èƒ½è¡¨ç°å“è¶Š")
        elif avg_rps > 500:
            report.append("   âœ… è‰¯å¥½: å¹³å‡RPSè¶…è¿‡500ï¼Œæ€§èƒ½è¡¨ç°è‰¯å¥½")
        elif avg_rps > 100:
            report.append("   âš ï¸ ä¸€èˆ¬: å¹³å‡RPSè¶…è¿‡100ï¼Œæ€§èƒ½å¯æ¥å—")
        else:
            report.append("   âŒ éœ€ä¼˜åŒ–: å¹³å‡RPSä½äº100ï¼Œéœ€è¦æ€§èƒ½ä¼˜åŒ–")
        
        if avg_success_rate > 99:
            report.append("   âœ… å¯é æ€§ä¼˜ç§€: æˆåŠŸç‡è¶…è¿‡99%")
        elif avg_success_rate > 95:
            report.append("   âœ… å¯é æ€§è‰¯å¥½: æˆåŠŸç‡è¶…è¿‡95%")
        else:
            report.append("   âš ï¸ å¯é æ€§éœ€æ”¹å–„: æˆåŠŸç‡ä½äº95%")
        
        # æ¨èä¼˜åŒ–å»ºè®®
        report.append("")
        report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        
        max_response_time = max(r.max_response_time for r in self.test_results)
        if max_response_time > 1000:
            report.append("   1. æœ€å¤§å“åº”æ—¶é—´è¶…è¿‡1ç§’ï¼Œå»ºè®®ä¼˜åŒ–æ…¢æŸ¥è¯¢")
        
        min_rps = min(r.requests_per_second for r in self.test_results)
        if min_rps < 100:
            report.append("   2. éƒ¨åˆ†æµ‹è¯•RPSè¾ƒä½ï¼Œå»ºè®®å¢åŠ ç¼“å­˜æˆ–ä¼˜åŒ–ç®—æ³•")
        
        report.append("   3. è€ƒè™‘å®æ–½Redisé›†ç¾¤ä»¥æé«˜ç¼“å­˜æ€§èƒ½")
        report.append("   4. ä½¿ç”¨å¼‚æ­¥å¤„ç†é˜Ÿåˆ—å¤„ç†å‘Šè­¦ç”Ÿæˆ")
        report.append("   5. å®æ–½è§„åˆ™é¢„ç¼–è¯‘å’Œç¼“å­˜ä¼˜åŒ–")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def save_report_to_file(self, filename: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"alert_rules_performance_report_{timestamp}.txt"
        
        report_content = self.generate_performance_report()
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        return filename

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å‘Šè­¦è§„åˆ™ç³»ç»Ÿæ€§èƒ½æµ‹è¯•...")
    
    tester = AlertRulesPerformanceTester()
    
    try:
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        results = await tester.run_comprehensive_performance_tests()
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
        report = tester.generate_performance_report()
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = tester.save_report_to_file()
        
        print(f"\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())