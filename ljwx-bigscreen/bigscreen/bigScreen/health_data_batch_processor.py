#!/usr/bin/env python3
from flask import jsonify,request
from datetime import datetime,timedelta,date
from .models import db,UserHealthData,UserHealthDataDaily,UserHealthDataWeekly,HealthDataConfig
from .redis_helper import RedisHelper
from .alert import generate_alerts
import json,threading,queue,time
from sqlalchemy import text,and_,or_
from concurrent.futures import ThreadPoolExecutor
from config import MYSQL_HOST, MYSQL_PORT, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE
import pymysql

#å¯¼å…¥ä¸“ä¸šæ—¥å¿—ç³»ç»Ÿ
from logging_config import health_logger,db_logger,redis_logger,log_health_data_processing

redis=RedisHelper()
logger=health_logger#ä½¿ç”¨å¥åº·æ•°æ®ä¸“ç”¨è®°å½•å™¨

class HealthDataOptimizer:#å¥åº·æ•°æ®æ€§èƒ½ä¼˜åŒ–å™¨V4.0 - CPUè‡ªé€‚åº”ç‰ˆæœ¬
    def __init__(self):
        # CPUè‡ªé€‚åº”é…ç½®
        import psutil
        self.cpu_cores = psutil.cpu_count(logical=True)
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # åŠ¨æ€æ‰¹æ¬¡é…ç½®ï¼šCPUæ ¸å¿ƒæ•° Ã— 25
        self.batch_size = max(50, min(500, self.cpu_cores * 25))  # é™åˆ¶åœ¨50-500ä¹‹é—´
        self.batch_timeout=2#æ‰¹å¤„ç†è¶…æ—¶ç§’æ•°
        
        # åŠ¨æ€çº¿ç¨‹æ± é…ç½®ï¼šCPUæ ¸å¿ƒæ•° Ã— 2.5 (I/Oå¯†é›†å‹)
        max_workers = max(4, min(32, int(self.cpu_cores * 2.5)))
        
        self.batch_queue=queue.Queue(maxsize=5000)#æ‰¹å¤„ç†é˜Ÿåˆ—
        self.executor=ThreadPoolExecutor(max_workers=max_workers)#çº¿ç¨‹æ± 
        self.running=True#è¿è¡ŒçŠ¶æ€
        self.stats={'processed':0,'batches':0,'errors':0,'duplicates':0,'auto_adjustments':0}#ç»Ÿè®¡ä¿¡æ¯
        self.processed_keys=set()#å·²å¤„ç†è®°å½•é”®å€¼é›†åˆ
        
        # æ€§èƒ½ç›‘æ§
        self.performance_window = []
        self.last_adjustment_time = time.time()
        
        logger.info(f'ğŸš€ HealthDataOptimizer V4.0 åˆå§‹åŒ–:')
        logger.info(f'   CPUæ ¸å¿ƒ: {self.cpu_cores}, å†…å­˜: {self.memory_gb:.1f}GB')
        logger.info(f'   æ‰¹æ¬¡å¤§å°: {self.batch_size}, å·¥ä½œçº¿ç¨‹: {max_workers}')
        self.field_mapping={#æ•°æ®åº“å­—æ®µåˆ°APIå­—æ®µæ˜ å°„
            'heart_rate':'heart_rate','blood_oxygen':'blood_oxygen','temperature':'body_temperature',
            'pressure_high':'blood_pressure_systolic','pressure_low':'blood_pressure_diastolic','stress':'stress',
            'step':'step','distance':'distance','calorie':'calorie','latitude':'latitude',
            'longitude':'longitude','altitude':'altitude','sleep':'sleepData',
            'sleep_data':'sleepData','workout_data':'workoutData','exercise_daily_data':'exerciseDailyData',
            'exercise_week_data':'exerciseWeekData','scientific_sleep_data':'scientificSleepData'
        }
        self.app=None#åº”ç”¨å®ä¾‹
        self.processor_started=False#æ‰¹å¤„ç†å™¨å¯åŠ¨çŠ¶æ€
        
    def _ensure_processor_started(self):#ç¡®ä¿æ‰¹å¤„ç†å™¨å·²å¯åŠ¨
        if not self.processor_started:
            try:
                from flask import current_app
                self.app=current_app._get_current_object()#è·å–åº”ç”¨å®ä¾‹
                threading.Thread(target=self._batch_processor,daemon=True).start()
                self._schedule_cleanup()#å¯åŠ¨å®šæ—¶æ¸…ç†
                self.processor_started=True
                logger.info('æ‰¹å¤„ç†å™¨å’Œå®šæ—¶æ¸…ç†å·²å¯åŠ¨')
            except RuntimeError:
                logger.warning('åº”ç”¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œå»¶è¿Ÿå¯åŠ¨æ‰¹å¤„ç†å™¨')
        
    def _batch_processor(self):#æ‰¹å¤„ç†å™¨
        batch_data=[]
        last_flush=time.time()
        
        while self.running:
            try:
                timeout=max(0.1,self.batch_timeout-(time.time()-last_flush))
                item=self.batch_queue.get(timeout=timeout)
                
                # ç§»é™¤æ‰¹å¤„ç†å™¨ä¸­çš„é‡å¤æ£€æµ‹ï¼Œå› ä¸ºåœ¨add_dataä¸­å·²ç»é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è¿›è¡Œäº†å‡†ç¡®çš„é‡å¤æ£€æµ‹
                # key=f"{item['device_sn']}:{item['main_data']['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}"
                # if key in self.processed_keys:
                #     self.stats['duplicates']+=1
                #     logger.warning(f'è·³è¿‡é‡å¤è®°å½•: {key}')
                #     continue
                    
                batch_data.append(item)
                # self.processed_keys.add(key)  # ä¸å†ç»´æŠ¤å†…å­˜ä¸­çš„é‡å¤æ£€æµ‹é›†åˆ
                
                # æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ‰¹æ¬¡å¤„ç†æ—¶é—´
                if len(batch_data) == 1:
                    batch_start_time = time.time()
                
                if len(batch_data)>=self.batch_size or (time.time()-last_flush)>=self.batch_timeout:
                    if batch_data:
                        processing_start = time.time()
                        if self.app:
                            with self.app.app_context():#ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ
                                self._flush_batch(batch_data)
                        else:
                            self._flush_batch(batch_data)#ç›´æ¥æ‰§è¡Œ
                        
                        # è®°å½•æ€§èƒ½æ•°æ®å¹¶å°è¯•è‡ªåŠ¨è°ƒä¼˜
                        processing_time = time.time() - processing_start
                        self._record_performance(len(batch_data), processing_time)
                        
                        batch_data=[]
                        last_flush=time.time()
                        
            except queue.Empty:
                if batch_data and (time.time()-last_flush)>=self.batch_timeout:
                    if self.app:
                        with self.app.app_context():#ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ
                            self._flush_batch(batch_data)
                    else:
                        self._flush_batch(batch_data)#ç›´æ¥æ‰§è¡Œ
                    batch_data=[]
                    last_flush=time.time()
                    
    def _flush_batch(self,batch_data):#åˆ·æ–°æ‰¹æ¬¡åˆ°æ•°æ®åº“
        try:
            if not batch_data:return
            
            db_logger.info('æ‰¹å¤„ç†å¼€å§‹',extra={'batch_size':len(batch_data),'data_count':len(batch_data)})
            
            #åˆ†ç¦»ä¸åŒç±»å‹çš„æ•°æ®
            main_records=[]
            daily_records=[]
            weekly_records=[]
            
            for item in batch_data:
                main_records.append(item['main_data'])
                if item.get('daily_data'):
                    daily_records.append(item['daily_data'])
                    db_logger.debug('æ¯æ—¥æ•°æ®åˆ†ç¦»',extra={'device_sn':item['device_sn'],'data_count':1})
                if item.get('weekly_data'):
                    weekly_records.append(item['weekly_data'])
                    db_logger.debug('æ¯å‘¨æ•°æ®åˆ†ç¦»',extra={'device_sn':item['device_sn'],'data_count':1})
            
            db_logger.info('æ•°æ®åˆ†ç¦»å®Œæˆ',extra={'main_count':len(main_records),'daily_count':len(daily_records),'weekly_count':len(weekly_records)})
            
            # ä½¿ç”¨pymysqlç›´æ¥è¿æ¥
            conn = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE,
                autocommit=False
            )
            
            try:
                with conn.cursor() as cursor:
                    #æ‰¹é‡æ’å…¥ä¸»è¡¨
                    if main_records:
                        try:
                            insert_sql = """
                                INSERT INTO t_user_health_data 
                                (device_sn, user_id, org_id, heart_rate, blood_oxygen, temperature, 
                                 pressure_high, pressure_low, stress, step, distance, calorie, 
                                 latitude, longitude, altitude, sleep, timestamp, upload_method, create_time)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                            """
                            
                            for record in main_records:
                                cursor.execute(insert_sql, (
                                    record.get('device_sn'),
                                    record.get('user_id'),
                                    record.get('org_id'),
                                    record.get('heart_rate'),
                                    record.get('blood_oxygen'),
                                    record.get('temperature'),
                                    record.get('pressure_high'),
                                    record.get('pressure_low'),
                                    record.get('stress'),
                                    record.get('step'),
                                    record.get('distance'),
                                    record.get('calorie'),
                                    record.get('latitude'),
                                    record.get('longitude'),
                                    record.get('altitude'),
                                    record.get('sleep'),
                                    record.get('timestamp'),
                                    record.get('upload_method')
                                ))
                            
                            conn.commit()
                            db_logger.info('ä¸»è¡¨æ‰¹é‡æ’å…¥æˆåŠŸ',extra={'data_count':len(main_records)})
                        except Exception as e:
                            db_logger.error('ä¸»è¡¨æ’å…¥å¤±è´¥',extra={'error':str(e),'data_count':len(main_records)})
                            conn.rollback()
                    
                    #æ‰¹é‡å¤„ç†æ¯æ—¥è¡¨
                    if daily_records:
                        try:
                            for record in daily_records:
                                db_logger.debug('å¤„ç†æ¯æ—¥è®°å½•',extra={'device_sn':record['device_sn'],'date':record['date']})
                                
                                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                                cursor.execute("""
                                    SELECT id FROM t_user_health_data_daily 
                                    WHERE device_sn = %s AND date = %s
                                """, (record['device_sn'], record['date']))
                                
                                existing = cursor.fetchone()
                                
                                if existing:
                                    cursor.execute("""
                                        UPDATE t_user_health_data_daily 
                                        SET sleep_data = %s, exercise_daily_data = %s, workout_data = %s, update_time = NOW()
                                        WHERE id = %s
                                    """, (
                                        record.get('sleep_data'),
                                        record.get('exercise_daily_data'),
                                        record.get('workout_data'),
                                        existing[0]
                                    ))
                                else:
                                    cursor.execute("""
                                        INSERT INTO t_user_health_data_daily 
                                        (device_sn, user_id, org_id, date, sleep_data, exercise_daily_data, workout_data, create_time, update_time)
                                        VALUES (%s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                                    """, (
                                        record['device_sn'],
                                        record['user_id'],
                                        record['org_id'],
                                        record['date'],
                                        record.get('sleep_data'),
                                        record.get('exercise_daily_data'),
                                        record.get('workout_data')
                                    ))
                            
                            conn.commit()
                            db_logger.info('æ¯æ—¥è¡¨æ‰¹é‡å¤„ç†æˆåŠŸ',extra={'data_count':len(daily_records)})
                        except Exception as e:
                            db_logger.error('æ¯æ—¥è¡¨å¤„ç†å¤±è´¥',extra={'error':str(e),'data_count':len(daily_records)})
                            conn.rollback()
                    
                    #æ‰¹é‡å¤„ç†æ¯å‘¨è¡¨
                    if weekly_records:
                        try:
                            for record in weekly_records:
                                db_logger.debug('å¤„ç†æ¯å‘¨è®°å½•',extra={'device_sn':record['device_sn'],'week_start':record['week_start']})
                                
                                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                                cursor.execute("""
                                    SELECT id FROM t_user_health_data_weekly 
                                    WHERE device_sn = %s AND week_start = %s
                                """, (record['device_sn'], record['week_start']))
                                
                                existing = cursor.fetchone()
                                
                                if existing:
                                    cursor.execute("""
                                        UPDATE t_user_health_data_weekly 
                                        SET exercise_week_data = %s, update_time = NOW()
                                        WHERE id = %s
                                    """, (
                                        record.get('exercise_week_data'),
                                        existing[0]
                                    ))
                                else:
                                    cursor.execute("""
                                        INSERT INTO t_user_health_data_weekly 
                                        (device_sn, user_id, org_id, week_start, exercise_week_data, create_time, update_time)
                                        VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
                                    """, (
                                        record['device_sn'],
                                        record['user_id'],
                                        record['org_id'],
                                        record['week_start'],
                                        record.get('exercise_week_data')
                                    ))
                            
                            conn.commit()
                            db_logger.info('æ¯å‘¨è¡¨æ‰¹é‡å¤„ç†æˆåŠŸ',extra={'data_count':len(weekly_records)})
                        except Exception as e:
                            db_logger.error('æ¯å‘¨è¡¨å¤„ç†å¤±è´¥',extra={'error':str(e),'data_count':len(weekly_records)})
                            conn.rollback()
                            
            finally:
                conn.close()
            
            #å¼‚æ­¥å¤„ç†Rediså’Œå‘Šè­¦
            try:
                for item in batch_data:
                    if not self.executor._shutdown:  #æ£€æŸ¥çº¿ç¨‹æ± æ˜¯å¦å·²å…³é—­
                        self.executor.submit(self._async_process,item)
                    else:
                        #å¦‚æœçº¿ç¨‹æ± å·²å…³é—­ï¼Œç›´æ¥åŒæ­¥å¤„ç†
                        self._async_process(item)
            except RuntimeError as re:
                if 'cannot schedule new futures after shutdown' in str(re):
                    #çº¿ç¨‹æ± å·²å…³é—­ï¼Œæ”¹ä¸ºåŒæ­¥å¤„ç†
                    for item in batch_data:
                        self._async_process(item)
                else:
                    raise re
                
            self.stats['processed']+=len(batch_data)
            self.stats['batches']+=1
            health_logger.info('æ‰¹å¤„ç†å®Œæˆ',extra={'data_count':len(batch_data),'batch_id':self.stats['batches']})
            
        except Exception as e:
            self.stats['errors']+=1
            health_logger.error('æ‰¹å¤„ç†å¤±è´¥',extra={'error':str(e),'data_count':len(batch_data) if batch_data else 0},exc_info=True)
            
    def _insert_main_one_by_one(self,main_records):#å•æ¡æ’å…¥ä¸»è¡¨å¤„ç†é‡å¤
        success_count=0
        for record in main_records:
            try:
                existing=db.session.query(UserHealthData.id).filter(
                    and_(UserHealthData.device_sn==record['device_sn'],
                        UserHealthData.timestamp==record['timestamp'])
                ).first()
                
                if existing:
                    self.stats['duplicates']+=1
                    continue
                    
                health_record=UserHealthData(**record)
                db.session.add(health_record)
                db.session.commit()
                success_count+=1
                
            except Exception as e:
                try:
                    db.session.rollback()
                except:
                    pass#å¿½ç•¥å›æ»šé”™è¯¯
                if 'Duplicate entry' not in str(e):
                    logger.error(f'å•æ¡æ’å…¥å¤±è´¥: {e}')
                else:
                    self.stats['duplicates']+=1
                    
        logger.info(f'ä¸»è¡¨å•æ¡æ’å…¥å®Œæˆ: {success_count}æ¡æˆåŠŸ')
            
    def _async_process(self,item):#å¼‚æ­¥å¤„ç†Rediså’Œå‘Šè­¦
        try:
            device_sn=item['device_sn']
            redis_logger.info('Redisæ•°æ®æ›´æ–°å¼€å§‹',extra={'device_sn':device_sn})
            
            redis.hset_data(f"health_data:{device_sn}",mapping=item['redis_data'])
            redis.publish(f"health_data_channel:{device_sn}",device_sn)
            redis_logger.info('Redisæ•°æ®æ›´æ–°å®Œæˆ',extra={'device_sn':device_sn,'data_count':len(item['redis_data'])})
            
            if item.get('enable_alerts',True):
                from logging_config import alert_logger
                alert_logger.info('å‘Šè­¦æ£€æµ‹å¼€å§‹',extra={'device_sn':device_sn})
                
                if self.app:
                    with self.app.app_context():#ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨generate_alerts
                        generate_alerts(item['redis_data'],item.get('health_data_id'))
                else:
                    generate_alerts(item['redis_data'],item.get('health_data_id'))
                    
                alert_logger.info('å‘Šè­¦æ£€æµ‹å®Œæˆ',extra={'device_sn':device_sn})
                
        except Exception as e:
            health_logger.error('å¼‚æ­¥å¤„ç†å¤±è´¥',extra={'device_sn':item.get('device_sn','unknown'),'error':str(e)},exc_info=True)
            
    def get_health_config_fields(self,customer_id):#è·å–å¥åº·æ•°æ®é…ç½®å­—æ®µ
        """æ ¹æ®å®¢æˆ·IDè·å–é…ç½®çš„å¥åº·æ•°æ®å­—æ®µ"""
        try:
            configs=HealthDataConfig.query.filter_by(customer_id=customer_id,is_enabled=True).all()
            if not configs:
                return self._get_default_config()
                
            fields=[config.data_type for config in configs]
            weights={config.data_type:float(config.weight) if config.weight else 1.0 for config in configs}
            
            return {'fields':fields,'weights':weights,'config_source':'customer','customer_id':customer_id}
            
        except Exception as e:
            logger.error(f'è·å–å¥åº·é…ç½®å­—æ®µå¤±è´¥: {e}')
            return self._get_default_config()
    
    def _get_default_config(self):#é»˜è®¤é…ç½®
        """è¿”å›é»˜è®¤çš„å¥åº·æ•°æ®é…ç½®"""
        default_fields=list(self.field_mapping.keys())
        return {'fields':default_fields,'weights':{field:1.0 for field in default_fields},'config_source':'default','customer_id':None}

    def _get_week_start(self,date_obj):#è·å–å‘¨å¼€å§‹æ—¥æœŸ
        """è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å‘¨çš„å‘¨ä¸€æ—¥æœŸ"""
        if isinstance(date_obj,datetime):
            date_obj=date_obj.date()
        days_since_monday=date_obj.weekday()
        return date_obj-timedelta(days=days_since_monday)

    def _get_user_org_info(self,device_sn):#è·å–ç”¨æˆ·ç»„ç»‡ä¿¡æ¯
        """æ ¹æ®è®¾å¤‡SNè·å–ç”¨æˆ·å’Œç»„ç»‡ä¿¡æ¯"""
        print(f"ğŸ” å¼€å§‹æŸ¥è¯¢ç”¨æˆ·ç»„ç»‡ä¿¡æ¯: device_sn={device_sn}")
        try:
            # ç›´æ¥ä½¿ç”¨pymysqlè¿æ¥ï¼Œé¿å…SQLAlchemyçš„URLè§£æé—®é¢˜
            conn = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE
            )
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}")
            
            try:
                with conn.cursor() as cursor:
                    query_sql = """
                        SELECT u.id as user_id, uo.org_id
                        FROM sys_user u 
                        LEFT JOIN sys_user_org uo ON u.id = uo.user_id
                        WHERE u.device_sn = %s AND u.is_deleted = 0
                        LIMIT 1
                    """
                    print(f"ğŸ” æ‰§è¡Œç”¨æˆ·æŸ¥è¯¢SQL: {query_sql} with device_sn={device_sn}")
                    cursor.execute(query_sql, (device_sn,))
                    result = cursor.fetchone()
                    print(f"ğŸ” ç”¨æˆ·æŸ¥è¯¢ç»“æœ: {result}")
                    
                    if result:
                        user_id, org_id = result[0], result[1]
                        customer_id = None
                        print(f"âœ… æ‰¾åˆ°ç”¨æˆ·: user_id={user_id}, org_id={org_id}")
                        
                        # é€šè¿‡org_idæŸ¥æ‰¾sys_org_unitsçš„ancestorsè·å–ç§Ÿæˆ·ID
                        if org_id:
                            org_query_sql = """
                                SELECT ancestors FROM sys_org_units 
                                WHERE id = %s AND is_deleted = 0
                                LIMIT 1
                            """
                            print(f"ğŸ” æ‰§è¡Œç»„ç»‡æŸ¥è¯¢SQL: {org_query_sql} with org_id={org_id}")
                            cursor.execute(org_query_sql, (org_id,))
                            org_result = cursor.fetchone()
                            print(f"ğŸ” ç»„ç»‡æŸ¥è¯¢ç»“æœ: {org_result}")
                            
                            if org_result and org_result[0]:
                                ancestors = org_result[0]
                                print(f"ğŸ” ç»„ç»‡ancestors: {ancestors}")
                                # è§£æancestorsæ ¼å¼(0,X,Y...)ï¼Œè·å–ç¬¬äºŒä¸ªæ•°å­—Xä½œä¸ºç§Ÿæˆ·ID
                                parts = ancestors.split(',')
                                if len(parts) >= 2 and parts[0] == '0':
                                    try:
                                        customer_id = int(parts[1])
                                        print(f"âœ… è§£æå‡ºcustomer_id: {customer_id}")
                                    except ValueError:
                                        print(f"âŒ ancestorsæ ¼å¼å¼‚å¸¸: {ancestors}")
                                        logger.warning(f'ancestorsæ ¼å¼å¼‚å¸¸: {ancestors}')
                                else:
                                    print(f"âš ï¸ ancestorsæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ: {ancestors}")
                            else:
                                print(f"âš ï¸ æœªæ‰¾åˆ°ç»„ç»‡ä¿¡æ¯æˆ–ancestorsä¸ºç©º")
                        else:
                            print(f"âš ï¸ org_idä¸ºç©º")
                        
                        # åˆ›å»ºç±»ä¼¼SQLAlchemyç»“æœçš„å¯¹è±¡
                        class UserOrgInfo:
                            def __init__(self, user_id, org_id, customer_id):
                                self.user_id = user_id
                                self.org_id = org_id
                                self.customer_id = customer_id
                        
                        user_org_info = UserOrgInfo(user_id, org_id, customer_id)
                        print(f"âœ… ç”¨æˆ·ç»„ç»‡ä¿¡æ¯æ„å»ºå®Œæˆ: user_id={user_org_info.user_id}, org_id={user_org_info.org_id}, customer_id={user_org_info.customer_id}")
                        return user_org_info
                    else:
                        print(f"âŒ æœªæ‰¾åˆ°è®¾å¤‡å¯¹åº”çš„ç”¨æˆ·: {device_sn}")
                        return None
            finally:
                conn.close()
                print(f"âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
                
        except Exception as e:
            print(f"âŒ è·å–ç”¨æˆ·ç»„ç»‡ä¿¡æ¯å¼‚å¸¸: {e}")
            print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
            import traceback
            print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            logger.error(f'è·å–ç”¨æˆ·ç»„ç»‡ä¿¡æ¯å¤±è´¥: {e}')
            return None

    def add_data(self,raw_data,device_sn,enable_alerts=True):#æ·»åŠ æ•°æ®åˆ°é˜Ÿåˆ—
        """é…ç½®åŒ–å¤„ç†å¥åº·æ•°æ®ä¸Šä¼ """
        print(f"ğŸ”§ ä¼˜åŒ–å™¨æ·»åŠ æ•°æ®å¼€å§‹: device_sn={device_sn}, raw_data={json.dumps(raw_data, ensure_ascii=False)}")
        try:
            #ç¡®ä¿æ‰¹å¤„ç†å™¨å·²å¯åŠ¨
            self._ensure_processor_started()
            
            # ä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¼ é€’çš„å®¢æˆ·ä¿¡æ¯å‚æ•°
            user_id = raw_data.get("user_id")
            org_id = raw_data.get("org_id") 
            customer_id = raw_data.get("customer_id")
            
            print(f"ğŸ” ç›´æ¥ä¼ å…¥çš„å®¢æˆ·ä¿¡æ¯: user_id={user_id}, org_id={org_id}, customer_id={customer_id}")
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥ä¼ é€’å®¢æˆ·ä¿¡æ¯ï¼Œé€šè¿‡deviceSnæŸ¥è¯¢è·å–ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            if not user_id or not org_id or not customer_id:
                print(f"ğŸ” å®¢æˆ·ä¿¡æ¯ä¸å®Œæ•´ï¼Œé€šè¿‡deviceSnæŸ¥è¯¢è·å–")
                user_org_info=self._get_user_org_info(device_sn)
                if not user_org_info:
                    print(f"âŒ æœªæ‰¾åˆ°è®¾å¤‡å¯¹åº”ç”¨æˆ·: {device_sn}")
                    logger.warning(f'æœªæ‰¾åˆ°è®¾å¤‡å¯¹åº”ç”¨æˆ·: {device_sn}')
                    return {'success':False,'reason':'user_not_found','message':'è®¾å¤‡å¯¹åº”ç”¨æˆ·æœªæ‰¾åˆ°'}
                    
                user_id = user_id or user_org_info.user_id
                org_id = org_id or user_org_info.org_id
                customer_id = customer_id or user_org_info.customer_id
                print(f"âœ… è¡¥å……åçš„å®¢æˆ·ä¿¡æ¯: user_id={user_id}, org_id={org_id}, customer_id={customer_id}")
            else:
                print(f"âœ… ä½¿ç”¨ç›´æ¥ä¼ é€’çš„å®¢æˆ·ä¿¡æ¯")
            
            #è·å–é…ç½®å­—æ®µ
            config_info=self.get_health_config_fields(customer_id) if customer_id else self._get_default_config()
            config_fields=config_info['fields']
            print(f"ğŸ” å¥åº·æ•°æ®é…ç½®å­—æ®µ: {config_fields}")
            
            #æ—¶é—´æˆ³å¤„ç†
            timestamp=raw_data.get("timestamp") or raw_data.get("cjsj") or datetime.now()
            print(f"ğŸ” åŸå§‹æ—¶é—´æˆ³: {timestamp}")
            if isinstance(timestamp,str):
                try:
                    timestamp=datetime.strptime(timestamp,'%Y-%m-%d %H:%M:%S')
                except:
                    timestamp=datetime.now()
            print(f"ğŸ” å¤„ç†åæ—¶é—´æˆ³: {timestamp}")
            
            #æ­£ç¡®çš„é‡å¤æ£€æµ‹ï¼šåªç”¨device_sn+timestampï¼ŒæŸ¥è¯¢æ•°æ®åº“è€Œéå†…å­˜
            duplicate_key=f"{device_sn}:{timestamp.strftime('%Y-%m-%d %H:%M:%S')}"
            print(f"ğŸ” é‡å¤æ£€æµ‹é”®: {duplicate_key}")
            
            #ç›´æ¥æŸ¥è¯¢æ•°æ®åº“æ£€æŸ¥æ˜¯å¦é‡å¤ï¼Œè€Œä¸æ˜¯ä¾èµ–å†…å­˜ç¼“å­˜
            try:
                conn = pymysql.connect(host=MYSQL_HOST,port=MYSQL_PORT,user=MYSQL_USER,password=MYSQL_PASSWORD,database=MYSQL_DATABASE)
                try:
                    with conn.cursor() as cursor:
                        cursor.execute("SELECT id FROM t_user_health_data WHERE device_sn = %s AND timestamp = %s LIMIT 1",(device_sn,timestamp))
                        existing_record = cursor.fetchone()
                        if existing_record:
                            print(f"âš ï¸ æ•°æ®åº“ä¸­å·²å­˜åœ¨ç›¸åŒè®°å½•: {duplicate_key}, id={existing_record[0]}")
                            logger.info(f'è·³è¿‡é‡å¤æ•°æ®(æ•°æ®åº“å·²å­˜åœ¨): {duplicate_key}')
                            return {'success':True,'reason':'duplicate','message':'æ•°æ®åº“ä¸­å·²å­˜åœ¨ç›¸åŒæ—¶é—´æˆ³æ•°æ®'}
                        else:
                            print(f"âœ… æ•°æ®åº“é‡å¤æ£€æŸ¥é€šè¿‡: {duplicate_key}")
                finally:
                    conn.close()
            except Exception as e:
                print(f"âŒ é‡å¤æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†: {e}")
                logger.warning(f'é‡å¤æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†: {e}')
                #å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œç»§ç»­å¤„ç†æ•°æ®
            
            #åˆ†ç¦»å­—æ®µç±»å‹
            fast_fields=['heart_rate','blood_oxygen','temperature','pressure_high','pressure_low','stress','step','distance','calorie','latitude','longitude','altitude','sleep']
            slow_daily_fields=['sleep_data','exercise_daily_data','workout_data','scientific_sleep_data']
            slow_weekly_fields=['exercise_week_data']
            print(f"ğŸ” å­—æ®µåˆ†ç¦»: fast_fields={fast_fields}")
            
            #æ„å»ºä¸»è¡¨æ•°æ®(åªåŒ…å«é…ç½®æ”¯æŒçš„å¿«æ›´æ–°å­—æ®µ)
            main_data={'device_sn':device_sn,'user_id':user_id,'org_id':org_id,'customer_id':customer_id,'timestamp':timestamp,'upload_method':raw_data.get("upload_method","wifi")}
            print(f"ğŸ” åˆå§‹ä¸»è¡¨æ•°æ®: {main_data}")
            
            for field in fast_fields:
                if field in config_fields:
                    if field == 'sleep':
                        # ä¸“é—¨å¤„ç†ç¡çœ æ•°æ®è§£æ
                        sleep_data_raw = raw_data.get('sleepData') or raw_data.get('smData')
                        value = parse_sleep_data(sleep_data_raw)
                        print(f"ğŸ” ç¡çœ æ•°æ®è§£æ: raw={sleep_data_raw} -> value={value}")
                    else:
                        value=raw_data.get(self.field_mapping.get(field,field))
                        print(f"ğŸ” å­—æ®µæ˜ å°„: {field} -> {self.field_mapping.get(field,field)} = {value}")
                    if value is not None:
                        main_data[field]=value
            print(f"âœ… å®Œæ•´ä¸»è¡¨æ•°æ®: {main_data}")
            
            #æ„å»ºæ¯æ—¥æ•°æ®(åªåŒ…å«é…ç½®æ”¯æŒçš„æ¯æ—¥å­—æ®µ)
            daily_data=None
            daily_fields_in_config=[f for f in slow_daily_fields if f in config_fields]
            if daily_fields_in_config:
                daily_data={'device_sn':device_sn,'user_id':user_id,'org_id':org_id,'customer_id':customer_id,'date':timestamp.date()}
                for field in daily_fields_in_config:
                    value=raw_data.get(self.field_mapping.get(field,field))
                    if value is not None:
                        daily_data[field]=value
                print(f"âœ… æ¯æ—¥æ•°æ®: {daily_data}")
            else:
                print(f"âš ï¸ æ— æ¯æ—¥æ•°æ®å­—æ®µ")
                        
            #æ„å»ºæ¯å‘¨æ•°æ®(åªåŒ…å«é…ç½®æ”¯æŒçš„æ¯å‘¨å­—æ®µ)
            weekly_data=None
            weekly_fields_in_config=[f for f in slow_weekly_fields if f in config_fields]
            if weekly_fields_in_config:
                week_start=self._get_week_start(timestamp)
                weekly_data={'device_sn':device_sn,'user_id':user_id,'org_id':org_id,'customer_id':customer_id,'week_start':week_start}
                for field in weekly_fields_in_config:
                    value=raw_data.get(self.field_mapping.get(field,field))
                    if value is not None:
                        weekly_data[field]=value
                print(f"âœ… æ¯å‘¨æ•°æ®: {weekly_data}")
            else:
                print(f"âš ï¸ æ— æ¯å‘¨æ•°æ®å­—æ®µ")
            
            #æ„å»ºRedisæ•°æ®(åªåŒ…å«é…ç½®å­—æ®µ)
            redis_data={}
            for field in config_fields:
                if field in self.field_mapping:
                    api_field=self.field_mapping[field]
                    value=raw_data.get(api_field)
                    if value is not None:
                        redis_data[api_field]=str(value)
            redis_data['deviceSn']=device_sn
            print(f"âœ… Redisæ•°æ®: {redis_data}")
                
            item={'device_sn':device_sn,'main_data':main_data,'daily_data':daily_data,'weekly_data':weekly_data,'redis_data':redis_data,'enable_alerts':enable_alerts,'config_info':config_info}
            print(f"ğŸ”§ å‡†å¤‡åŠ å…¥é˜Ÿåˆ—çš„æ•°æ®é¡¹: {json.dumps(item, ensure_ascii=False, default=str)}")
            self.batch_queue.put(item,timeout=1)
            print(f"âœ… æ•°æ®å·²æˆåŠŸåŠ å…¥å¤„ç†é˜Ÿåˆ—: {device_sn}")
            # ä¸å†ç»´æŠ¤å†…å­˜ä¸­çš„processed_keysï¼Œå› ä¸ºé‡å¤æ£€æµ‹å·²åœ¨æ•°æ®åº“å±‚é¢å®Œæˆ
            # self.processed_keys.add(duplicate_key)
            return {'success':True,'reason':'queued','message':'æ•°æ®å·²åŠ å…¥å¤„ç†é˜Ÿåˆ—'}
            
        except queue.Full:
            print(f"âŒ æ‰¹å¤„ç†é˜Ÿåˆ—å·²æ»¡")
            logger.warning('æ‰¹å¤„ç†é˜Ÿåˆ—å·²æ»¡')
            return {'success':False,'reason':'queue_full','message':'å¤„ç†é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åé‡è¯•'}
        except Exception as e:
            print(f"âŒ æ·»åŠ æ•°æ®å¼‚å¸¸: {e}")
            print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
            import traceback
            print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            logger.error(f'æ·»åŠ æ•°æ®å¤±è´¥: {e}')
            return {'success':False,'reason':'error','message':f'æ•°æ®å¤„ç†å¤±è´¥: {str(e)}'}
            
    def _record_performance(self, batch_size, processing_time):
        """è®°å½•æ€§èƒ½æ•°æ®å¹¶å°è¯•è‡ªåŠ¨è°ƒä¼˜"""
        throughput = batch_size / processing_time if processing_time > 0 else 0
        
        self.performance_window.append({
            'batch_size': batch_size,
            'processing_time': processing_time,
            'throughput': throughput,
            'timestamp': time.time()
        })
        
        # ä¿æŒæ€§èƒ½çª—å£å¤§å°
        if len(self.performance_window) > 50:
            self.performance_window.pop(0)
        
        # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦è°ƒä¼˜
        current_time = time.time()
        if current_time - self.last_adjustment_time > 30 and len(self.performance_window) >= 10:
            self._auto_adjust_batch_size()
            self.last_adjustment_time = current_time
    
    def _auto_adjust_batch_size(self):
        """è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
        import psutil
        
        # è®¡ç®—æœ€è¿‘æ€§èƒ½æŒ‡æ ‡
        recent_performance = self.performance_window[-10:]
        avg_throughput = sum(p['throughput'] for p in recent_performance) / len(recent_performance)
        avg_processing_time = sum(p['processing_time'] for p in recent_performance) / len(recent_performance)
        
        # ç³»ç»Ÿèµ„æºæ£€æŸ¥
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_percent = psutil.virtual_memory().percent
        queue_size = self.batch_queue.qsize()
        
        old_batch_size = self.batch_size
        
        # è°ƒä¼˜é€»è¾‘
        if cpu_percent < 50 and avg_throughput < 100:
            # CPUåˆ©ç”¨ç‡ä½ï¼Œååé‡ä½ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°
            self.batch_size = min(500, int(self.batch_size * 1.2))
        elif cpu_percent > 90 or memory_percent > 85:
            # èµ„æºå‹åŠ›å¤§ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°
            self.batch_size = max(50, int(self.batch_size * 0.8))
        elif queue_size > 2000:
            # é˜Ÿåˆ—å †ç§¯ä¸¥é‡ï¼Œå¢åŠ å¤„ç†èƒ½åŠ›
            self.batch_size = min(500, int(self.batch_size * 1.1))
        
        # è®°å½•è°ƒæ•´
        if old_batch_size != self.batch_size:
            self.stats['auto_adjustments'] += 1
            logger.info(f"ğŸ“Š HealthDataæ‰¹æ¬¡å¤§å°è‡ªåŠ¨è°ƒæ•´: {old_batch_size} â†’ {self.batch_size} "
                       f"(CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_percent:.1f}%, "
                       f"é˜Ÿåˆ—: {queue_size}, ååé‡: {avg_throughput:.1f}/ç§’)")
    
    def get_stats(self):#è·å–ç»Ÿè®¡ä¿¡æ¯
        stats=self.stats.copy()
        stats['cpu_cores'] = getattr(self, 'cpu_cores', 'N/A')
        stats['batch_size'] = self.batch_size
        stats['max_workers'] = self.executor._max_workers
        stats['queue_size']=self.batch_queue.qsize()
        stats['performance_window_size'] = len(getattr(self, 'performance_window', []))
        # ä¸å†ç»Ÿè®¡processed_keys_countï¼Œå› ä¸ºå·²ç§»é™¤å†…å­˜é‡å¤æ£€æµ‹
        # stats['processed_keys_count']=len(self.processed_keys)
        return stats
        
    def clear_processed_keys(self):#æ¸…ç†å·²å¤„ç†é”®å€¼
        """æ™ºèƒ½æ¸…ç†è¿‡æœŸçš„å¤„ç†é”®å€¼"""
        if len(self.processed_keys)>10000:
            # åªä¿ç•™æœ€è¿‘1å°æ—¶çš„è®°å½•
            import time
            current_time=time.time()
            new_keys=set()
            
            for key in self.processed_keys:
                try:
                    # ä»é”®å€¼ä¸­æå–æ—¶é—´æˆ³ï¼Œæ ¼å¼ï¼šdevice_sn:YYYY-MM-DD HH:MM:SS
                    parts=key.split(':',1)  # åªåˆ†å‰²ä¸€æ¬¡ï¼Œé˜²æ­¢æ—¶é—´æˆ³ä¸­çš„å†’å·è¢«è¯¯åˆ†
                    if len(parts)==2:
                        device_sn,timestamp_str=parts
                        key_timestamp=datetime.strptime(timestamp_str,'%Y-%m-%d %H:%M:%S').timestamp()
                        # ä¿ç•™1å°æ—¶å†…çš„è®°å½•
                        if current_time-key_timestamp<3600:
                            new_keys.add(key)
                except:
                    continue
                    
            self.processed_keys=new_keys
            logger.info(f'æ™ºèƒ½æ¸…ç†processed_keysç¼“å­˜ï¼Œä¿ç•™{len(new_keys)}æ¡è®°å½•')
        
    def _schedule_cleanup(self):#å®šæ—¶æ¸…ç†ä»»åŠ¡
        """å®šæ—¶æ¸…ç†è¿‡æœŸé”®å€¼"""
        import threading
        def cleanup_worker():
            while self.running:
                time.sleep(1800)#æ¯30åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                self.clear_processed_keys()
        
        threading.Thread(target=cleanup_worker,daemon=True).start()

#å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
optimizer=HealthDataOptimizer()

def optimized_upload_health_data(health_data):#ä¼˜åŒ–çš„å¥åº·æ•°æ®ä¸Šä¼ V3.1
    """é…ç½®åŒ–å¥åº·æ•°æ®ä¸Šä¼ å¤„ç†"""
    print(f"ğŸ¥ å¥åº·æ•°æ®ä¸Šä¼ å¼€å§‹ - åŸå§‹æ•°æ®: {json.dumps(health_data, ensure_ascii=False, indent=2)}")
    try:
        # åœ¨Flaskè·¯ç”±ä¸Šä¸‹æ–‡ä¸­è·å–åº”ç”¨å®ä¾‹å¹¶ä¼ é€’ç»™ä¼˜åŒ–å™¨
        try:
            from flask import current_app
            if current_app and not optimizer.app:
                optimizer.app = current_app._get_current_object()
        except RuntimeError:
            pass  # å¿½ç•¥åº”ç”¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨çš„é”™è¯¯
            
        data=health_data.get("data",{})
        
        # æå–é¡¶çº§çš„å®¢æˆ·ä¿¡æ¯å‚æ•°
        customer_id = health_data.get("customer_id")
        org_id = health_data.get("org_id") 
        user_id = health_data.get("user_id")
        print(f"ğŸ” æå–å®¢æˆ·ä¿¡æ¯: customer_id={customer_id}, org_id={org_id}, user_id={user_id}")
        print(f"ğŸ” è§£ædataå­—æ®µ: {json.dumps(data, ensure_ascii=False, indent=2)}")
        
        if isinstance(data,list):
            print(f"ğŸ” æ£€æµ‹åˆ°æ‰¹é‡æ•°æ®ï¼Œæ•°é‡: {len(data)}")
            if len(data)>10:#å¤§æ‰¹é‡ä½¿ç”¨é˜Ÿåˆ—
                print(f"ğŸ¥ å¤§æ‰¹é‡å¤„ç†æ¨¡å¼: {len(data)}æ¡æ•°æ®")
                results=[]
                success_count=0
                duplicate_count=0
                error_count=0
                
                for i, item in enumerate(data):
                    # ä¼˜å…ˆä»ç›´æ¥å­—æ®µè·å–è®¾å¤‡SN
                    device_sn = item.get("deviceSn") or item.get("id")
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—çš„dataå­—æ®µ
                    if not device_sn and 'data' in item:
                        nested_data = item['data']
                        if isinstance(nested_data, dict):
                            device_sn = nested_data.get('deviceSn') or nested_data.get('id')
                        elif isinstance(nested_data, list) and len(nested_data) > 0:
                            device_sn = nested_data[0].get('deviceSn') or nested_data[0].get('id')
                    
                    # å°†å®¢æˆ·ä¿¡æ¯æ·»åŠ åˆ°æ¯ä¸ªæ•°æ®é¡¹ä¸­ï¼Œä¼˜å…ˆä½¿ç”¨é¡¶çº§å‚æ•°
                    if customer_id is not None:
                        item['customer_id'] = customer_id
                    if org_id is not None:
                        item['org_id'] = org_id
                    if user_id is not None:
                        item['user_id'] = user_id
                    
                    print(f"ğŸ” å¤„ç†ç¬¬{i+1}æ¡æ•°æ®: device_sn={device_sn}, æ•°æ®={json.dumps(item, ensure_ascii=False)}")
                    if device_sn:
                        result=optimizer.add_data(item,device_sn)
                        results.append(result)
                        print(f"ğŸ” å¤„ç†ç»“æœ: {result}")
                        
                        if result.get('success'):
                            if result.get('reason')=='duplicate':
                                duplicate_count+=1
                            else:
                                success_count+=1
                        else:
                            error_count+=1
                    else:
                        print(f"âŒ ç¬¬{i+1}æ¡æ•°æ®ç¼ºå°‘è®¾å¤‡SN")
                        error_count+=1
                
                response_msg=f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ{success_count}æ¡"
                if duplicate_count>0:
                    response_msg+=f"ï¼Œé‡å¤{duplicate_count}æ¡"
                if error_count>0:
                    response_msg+=f"ï¼Œå¤±è´¥{error_count}æ¡"
                
                print(f"âœ… å¤§æ‰¹é‡å¤„ç†å®Œæˆ: {response_msg}")
                return jsonify({"status":"success","message":response_msg,"details":{"success":success_count,"duplicate":duplicate_count,"error":error_count}})
            else:#å°æ‰¹é‡ç›´æ¥å¤„ç†
                print(f"ğŸ¥ å°æ‰¹é‡å¤„ç†æ¨¡å¼: {len(data)}æ¡æ•°æ®")
                return _process_batch_direct(data)
        else:
            # ä¼˜å…ˆä»ç›´æ¥å­—æ®µè·å–è®¾å¤‡SN
            device_sn = data.get("deviceSn") or data.get("id")
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—çš„dataå­—æ®µï¼ˆé’ˆå¯¹data.data.idçš„æƒ…å†µï¼‰
            if not device_sn and 'data' in data:
                nested_data = data['data']
                if isinstance(nested_data, dict):
                    device_sn = nested_data.get('deviceSn') or nested_data.get('id')
                    print(f"ğŸ” ä»åµŒå¥—dataå¯¹è±¡æå–device_sn: {device_sn}")
                elif isinstance(nested_data, list) and len(nested_data) > 0:
                    device_sn = nested_data[0].get('deviceSn') or nested_data[0].get('id')
                    print(f"ğŸ” ä»åµŒå¥—dataæ•°ç»„æå–device_sn: {device_sn}")
            
            print(f"ğŸ” å•æ¡æ•°æ®å¤„ç†: device_sn={device_sn}")
            if not device_sn:
                print(f"âŒ è®¾å¤‡IDä¸ºç©º")
                return jsonify({"status":"error","message":"è®¾å¤‡IDä¸èƒ½ä¸ºç©º"})
            
            # å°†å®¢æˆ·ä¿¡æ¯æ·»åŠ åˆ°æ•°æ®é¡¹ä¸­ï¼Œä¼˜å…ˆä½¿ç”¨é¡¶çº§å‚æ•°
            if customer_id is not None:
                data['customer_id'] = customer_id
            if org_id is not None:
                data['org_id'] = org_id
            if user_id is not None:
                data['user_id'] = user_id
            
            print(f"ğŸ” å•æ¡æ•°æ®è¯¦æƒ…: {json.dumps(data, ensure_ascii=False, indent=2)}")
            #å•æ¡æ•°æ®ä½¿ç”¨é˜Ÿåˆ—å¤„ç†
            result=optimizer.add_data(data,device_sn)
            print(f"ğŸ” å•æ¡å¤„ç†ç»“æœ: {result}")
            if result.get('success'):
                if result.get('reason')=='duplicate':
                    print(f"âš ï¸ æ•°æ®é‡å¤: {device_sn}")
                    return jsonify({"status":"success","message":"æ•°æ®é‡å¤ï¼Œå·²è·³è¿‡å¤„ç†","reason":"duplicate"})
                else:
                    print(f"âœ… å•æ¡æ•°æ®å¤„ç†æˆåŠŸ: {device_sn}")
                    return jsonify({"status":"success","message":result.get('message','æ•°æ®å¤„ç†æˆåŠŸ')})
            else:
                print(f"âŒ å•æ¡æ•°æ®å¤„ç†å¤±è´¥: {result}")
                # æ ¹æ®å¤±è´¥åŸå› è¿”å›ä¸åŒçš„çŠ¶æ€ç 
                if result.get('reason')=='user_not_found':
                    return jsonify({"status":"error","message":result.get('message')}),404
                elif result.get('reason')=='queue_full':
                    return jsonify({"status":"error","message":result.get('message')}),503
                else:
                    return jsonify({"status":"error","message":result.get('message')}),500
            
    except Exception as e:
        print(f"âŒ å¥åº·æ•°æ®ä¸Šä¼ å¼‚å¸¸: {e}")
        print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
        import traceback
        print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        logger.error(f'æ•°æ®ä¸Šä¼ å¤±è´¥: {e}')
        return jsonify({"status":"error","message":str(e)}),500

def _process_batch_direct(data_list):#å°æ‰¹é‡ç›´æ¥å¤„ç†
    """å°æ‰¹é‡æ•°æ®ç›´æ¥åŒæ­¥å¤„ç†"""
    print(f"ğŸ¥ å°æ‰¹é‡ç›´æ¥å¤„ç†å¼€å§‹ï¼Œæ•°æ®é‡: {len(data_list)}")
    try:
        success_count=0
        duplicate_count=0
        error_count=0
        
        for i, data in enumerate(data_list):
            # ä¼˜å…ˆä»ç›´æ¥å­—æ®µè·å–è®¾å¤‡SN
            device_sn = data.get("deviceSn") or data.get("id")
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—çš„dataå­—æ®µ
            if not device_sn and 'data' in data:
                nested_data = data['data']
                if isinstance(nested_data, dict):
                    device_sn = nested_data.get('deviceSn') or nested_data.get('id')
                elif isinstance(nested_data, list) and len(nested_data) > 0:
                    device_sn = nested_data[0].get('deviceSn') or nested_data[0].get('id')
            
            print(f"ğŸ” å°æ‰¹é‡å¤„ç†ç¬¬{i+1}æ¡: device_sn={device_sn}, æ•°æ®={json.dumps(data, ensure_ascii=False)}")
            if device_sn:
                result=optimizer.add_data(data,device_sn)
                print(f"ğŸ” å°æ‰¹é‡å¤„ç†ç»“æœ: {result}")
                if result.get('success'):
                    if result.get('reason')=='duplicate':
                        duplicate_count+=1
                    else:
                        success_count+=1
                else:
                    error_count+=1
            else:
                print(f"âŒ ç¬¬{i+1}æ¡æ•°æ®ç¼ºå°‘è®¾å¤‡SN")
                error_count+=1
        
        response_msg=f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ{success_count}æ¡"
        if duplicate_count>0:
            response_msg+=f"ï¼Œé‡å¤{duplicate_count}æ¡"
        if error_count>0:
            response_msg+=f"ï¼Œå¤±è´¥{error_count}æ¡"
        
        print(f"âœ… å°æ‰¹é‡ç›´æ¥å¤„ç†å®Œæˆ: {response_msg}")
        return jsonify({"status":"success","message":response_msg,"details":{"success":success_count,"duplicate":duplicate_count,"error":error_count}})
        
    except Exception as e:
        print(f"âŒ å°æ‰¹é‡ç›´æ¥å¤„ç†å¼‚å¸¸: {e}")
        print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
        import traceback
        print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        logger.error(f'å°æ‰¹é‡ç›´æ¥å¤„ç†å¤±è´¥: {e}')
        return jsonify({"status":"error","message":f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"}),500

def get_optimizer_stats():#è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡
    """è·å–ä¼˜åŒ–å™¨è¿è¡Œç»Ÿè®¡ä¿¡æ¯"""
    return jsonify(optimizer.get_stats())

def save_health_data_fast(*args,**kwargs):#å¿«é€Ÿä¿å­˜(å…¼å®¹åŸæ¥å£)
    """å…¼å®¹åŸæœ‰æ¥å£çš„å¿«é€Ÿä¿å­˜æ–¹æ³•"""
    try:
        #æ„å»ºæ•°æ®å­—å…¸
        data={
            'heartRate':args[0] if len(args)>0 else kwargs.get('heartRate'),
            'pressureHigh':args[1] if len(args)>1 else kwargs.get('pressureHigh'),
            'pressureLow':args[2] if len(args)>2 else kwargs.get('pressureLow'),
            'bloodOxygen':args[3] if len(args)>3 else kwargs.get('bloodOxygen'),
            'temperature':args[4] if len(args)>4 else kwargs.get('temperature'),
            'stress':args[5] if len(args)>5 else kwargs.get('stress'),
            'step':args[6] if len(args)>6 else kwargs.get('step'),
            'timestamp':args[7] if len(args)>7 else kwargs.get('timestamp',datetime.now()),
            'deviceSn':args[8] if len(args)>8 else kwargs.get('deviceSn'),
            'distance':args[9] if len(args)>9 else kwargs.get('distance'),
            'calorie':args[10] if len(args)>10 else kwargs.get('calorie'),
            'latitude':args[11] if len(args)>11 else kwargs.get('latitude'),
            'longitude':args[12] if len(args)>12 else kwargs.get('longitude'),
            'altitude':args[13] if len(args)>13 else kwargs.get('altitude'),
            'sleepData':args[14] if len(args)>14 else kwargs.get('sleepData'),
            'exerciseDailyData':args[15] if len(args)>15 else kwargs.get('exerciseDailyData'),
            'exerciseWeekData':args[16] if len(args)>16 else kwargs.get('exerciseWeekData'),
            'scientificSleepData':args[17] if len(args)>17 else kwargs.get('scientificSleepData'),
            'workoutData':args[18] if len(args)>18 else kwargs.get('workoutData'),
            'uploadMethod':args[19] if len(args)>19 else kwargs.get('uploadMethod','wifi')
        }
        
        device_sn=data.get('deviceSn')
        if not device_sn:
            logger.error('å¿«é€Ÿä¿å­˜ç¼ºå°‘è®¾å¤‡SN')
            return None
            
        #ä½¿ç”¨ä¼˜åŒ–å™¨å¤„ç†
        result=optimizer.add_data(data,device_sn)
        if result.get('success'):
            return True
        else:
            logger.warning(f'å¿«é€Ÿä¿å­˜å¤±è´¥: {result.get("message","æœªçŸ¥é”™è¯¯")}')
            return None
        
    except Exception as e:
        logger.error(f'å¿«é€Ÿä¿å­˜å¤±è´¥: {e}')
        return None 
def parse_sleep_data(sleep_data_json):
    """
    è§£æsleepData JSONï¼Œè®¡ç®—ç¡çœ æ—¶é•¿(å°æ—¶)
    æ”¯æŒçš„æ ¼å¼ï¼š
    1. '{"code":0,"data":[{"endTimeStamp":1747440420000,"startTimeStamp":1747418280000,"type":2}],"name":"sleep","type":"history"}'
    2. null, "null", '{"code":-1,"data":[],"name":"sleep","type":"history"}'
    3. '{"code":"0","data":[],"name":"sleep","type":"history"}'
    
    è¿”å›: ç¡çœ æ—¶é•¿(å°æ—¶)ï¼Œå‡ºé”™æ—¶è¿”å›None
    """
    if not sleep_data_json or sleep_data_json in ['null', None]:
        return None
    
    try:
        if isinstance(sleep_data_json, str):
            # å¤„ç†JSONå­—ç¬¦ä¸²æ ¼å¼é”™è¯¯çš„æƒ…å†µï¼Œå¦‚'{"code":"0"data":[]}'
            sleep_data_json = sleep_data_json.replace('"0"data"', '"0","data"')
            sleep_data = json.loads(sleep_data_json)
        elif isinstance(sleep_data_json, dict):
            sleep_data = sleep_data_json
        else:
            return None
            
        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if not isinstance(sleep_data, dict):
            return None
            
        code = sleep_data.get('code')
        if code == -1 or code == '-1' or str(code) == '-1':
            return None
            
        data_list = sleep_data.get('data', [])
        if not isinstance(data_list, list) or len(data_list) == 0:
            return None
            
        total_sleep_seconds = 0
        
        # éå†æ‰€æœ‰ç¡çœ æ—¶é—´æ®µï¼Œä¸ç®¡typeï¼ŒæŒ‰ç…§endTimeStamp-startTimeStampè®¡ç®—
        for sleep_period in data_list:
            if not isinstance(sleep_period, dict):
                continue
                
            start_time = sleep_period.get('startTimeStamp')
            end_time = sleep_period.get('endTimeStamp')
            
            if start_time is None or end_time is None:
                continue
                
            try:
                # æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’
                start_seconds = int(start_time) / 1000 if int(start_time) > 9999999999 else int(start_time)
                end_seconds = int(end_time) / 1000 if int(end_time) > 9999999999 else int(end_time)
                
                # è®¡ç®—æ—¶é—´å·®(ç§’)
                if end_seconds > start_seconds:
                    total_sleep_seconds += (end_seconds - start_seconds)
                    
            except (ValueError, TypeError):
                continue
                
        # è½¬æ¢ä¸ºå°æ—¶ï¼Œä¿ç•™2ä½å°æ•°
        if total_sleep_seconds > 0:
            total_sleep_hours = round(total_sleep_seconds / 3600, 2)
            return total_sleep_hours
        else:
            return None
            
    except (json.JSONDecodeError, Exception) as e:
        print(f"è§£æsleepDataæ—¶å‡ºé”™: {e}, åŸå§‹æ•°æ®: {sleep_data_json}")
        return None