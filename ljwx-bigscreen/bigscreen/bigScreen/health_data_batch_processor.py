#!/usr/bin/env python3
from flask import jsonify,request
from datetime import datetime,timedelta,date
from .models import db,UserHealthData,UserHealthDataDaily,UserHealthDataWeekly,HealthDataConfig
from .redis_helper import RedisHelper
from .alert import generate_alerts
import json,threading,queue,time,asyncio
from sqlalchemy import text,and_,or_
from concurrent.futures import ThreadPoolExecutor
from config import MYSQL_HOST, MYSQL_PORT, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE
import pymysql
import psutil
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import uuid

#å¯¼å…¥ä¸“ä¸šæ—¥å¿—ç³»ç»Ÿ
from logging_config import health_logger,db_logger,redis_logger,log_health_data_processing

redis=RedisHelper()
logger=health_logger#ä½¿ç”¨å¥åº·æ•°æ®ä¸“ç”¨è®°å½•å™¨

class HealthDataOptimizer:#å¥åº·æ•°æ®æ€§èƒ½ä¼˜åŒ–å™¨V5.0 - å¤šé˜Ÿåˆ—åˆ†ç‰‡ç‰ˆæœ¬
    def __init__(self):
        # CPUè‡ªé€‚åº”é…ç½®
        import psutil
        self.cpu_cores = psutil.cpu_count(logical=True)
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # åˆå§‹åŒ–åˆ†ç‰‡æ‰¹å¤„ç†å™¨
        from .sharded_batch_processor import ShardedBatchProcessor
        self.sharded_processor = ShardedBatchProcessor()
        # è®¾ç½®æ‰¹å¤„ç†å›è°ƒï¼Œè®©åˆ†ç‰‡å¤„ç†å™¨ä½¿ç”¨ç°æœ‰çš„æ•°æ®åº“å¤„ç†é€»è¾‘
        self.sharded_processor.set_batch_callback(self._flush_batch)
        
        # ä¿æŒå…¼å®¹æ€§çš„ç»Ÿè®¡ä¿¡æ¯æ¥å£
        self._legacy_stats = {'processed':0,'batches':0,'errors':0,'duplicates':0,'auto_adjustments':0}
        
        # æ€§èƒ½ç›‘æ§
        self.performance_window = []
        self.last_adjustment_time = time.time()
        
        logger.info(f'ğŸš€ HealthDataOptimizer V5.0 åˆå§‹åŒ– - å¤šé˜Ÿåˆ—åˆ†ç‰‡ç‰ˆæœ¬:')
        logger.info(f'   CPUæ ¸å¿ƒ: {self.cpu_cores}, å†…å­˜: {self.memory_gb:.1f}GB')
        logger.info(f'   åˆ†ç‰‡æ•°é‡: {self.sharded_processor.shard_count}, æ‰¹æ¬¡å¤§å°: {self.sharded_processor.batch_size}')
        self.field_mapping={#æ•°æ®åº“å­—æ®µåˆ°APIå­—æ®µæ˜ å°„
            'heart_rate':'heart_rate','blood_oxygen':'blood_oxygen','temperature':'body_temperature',
            'pressure_high':'blood_pressure_systolic','pressure_low':'blood_pressure_diastolic','stress':'stress',
            'step':'step','distance':'distance','calorie':'calorie','latitude':'latitude',
            'longitude':'longitude','altitude':'altitude','sleep':'sleepData',
            'sleep_data':'sleepData','workout_data':'workoutData','exercise_daily_data':'exerciseDailyData',
            'exercise_week_data':'exerciseWeekData','scientific_sleep_data':'scientificSleepData'
        }
        self.app=None#åº”ç”¨å®ä¾‹
        self.processor_started=False#æ‰¹å¤„ç†å™¨å¯åŠ¨çŠ¶æ€
        
        # å¼‚æ­¥å¤„ç†çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=max(4, self.cpu_cores), thread_name_prefix='health-async')
        self.running = True
        
        # å…¼å®¹æ€§å±æ€§ï¼Œç”¨äºç°æœ‰ç»Ÿè®¡å’Œè°ƒä¼˜ä»£ç 
        self.batch_size = 200  # é»˜è®¤æ‰¹æ¬¡å¤§å°
        self.batch_queue = queue.Queue(maxsize=5000)  # è™šæ‹Ÿé˜Ÿåˆ—ï¼Œç”¨äºå…¼å®¹æ€§
        self.processed_keys = set()  # å…¼å®¹æ€§å±æ€§ï¼Œç”¨äºé—ç•™æ¸…ç†ä»£ç 
    
    @property
    def stats(self):
        """ç»Ÿè®¡ä¿¡æ¯æ¥å£ï¼Œå…¼å®¹ç°æœ‰ä»£ç """
        if hasattr(self.sharded_processor, 'get_overall_stats'):
            sharded_stats = self.sharded_processor.get_overall_stats()
            return {
                'processed': sharded_stats.get('total_processed', 0),
                'batches': sharded_stats.get('total_batches', 0), 
                'errors': sharded_stats.get('total_errors', 0),
                'duplicates': self._legacy_stats['duplicates'],
                'auto_adjustments': self._legacy_stats['auto_adjustments']
            }
        return self._legacy_stats
        
    def _ensure_processor_started(self):#ç¡®ä¿åˆ†ç‰‡æ‰¹å¤„ç†å™¨å·²å¯åŠ¨
        if not self.processor_started:
            try:
                from flask import current_app
                self.app=current_app._get_current_object()#è·å–åº”ç”¨å®ä¾‹
                self.sharded_processor.start()  # å¯åŠ¨åˆ†ç‰‡æ‰¹å¤„ç†å™¨
                self._schedule_cleanup()#å¯åŠ¨å®šæ—¶æ¸…ç†
                self.processor_started=True
                print(f"âœ… åˆ†ç‰‡æ‰¹å¤„ç†å™¨å’Œå®šæ—¶æ¸…ç†å·²å¯åŠ¨ï¼Œåˆ†ç‰‡æ•°: {self.sharded_processor.shard_count}")
                logger.info('åˆ†ç‰‡æ‰¹å¤„ç†å™¨å’Œå®šæ—¶æ¸…ç†å·²å¯åŠ¨')
            except RuntimeError as e:
                print(f"âŒ åº”ç”¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œå»¶è¿Ÿå¯åŠ¨æ‰¹å¤„ç†å™¨: {e}")
                logger.warning('åº”ç”¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œå»¶è¿Ÿå¯åŠ¨æ‰¹å¤„ç†å™¨')
        
    # æ‰¹å¤„ç†å™¨åŠŸèƒ½å·²è¿ç§»åˆ° ShardedBatchProcessor
        
    def _flush_batch(self,batch_data):#åˆ·æ–°æ‰¹æ¬¡åˆ°æ•°æ®åº“
        try:
            if not batch_data:return
            
            db_logger.info('æ‰¹å¤„ç†å¼€å§‹',extra={'batch_size':len(batch_data),'data_count':len(batch_data)})
            
            #åˆ†ç¦»ä¸åŒç±»å‹çš„æ•°æ®
            main_records=[]
            daily_records=[]
            weekly_records=[]
            
            for item in batch_data:
                main_records.append(item['main_data'])
                if item.get('daily_data'):
                    daily_records.append(item['daily_data'])
                    db_logger.debug('æ¯æ—¥æ•°æ®åˆ†ç¦»',extra={'device_sn':item['device_sn'],'data_count':1})
                if item.get('weekly_data'):
                    weekly_records.append(item['weekly_data'])
                    db_logger.debug('æ¯å‘¨æ•°æ®åˆ†ç¦»',extra={'device_sn':item['device_sn'],'data_count':1})
            
            db_logger.info('æ•°æ®åˆ†ç¦»å®Œæˆ',extra={'main_count':len(main_records),'daily_count':len(daily_records),'weekly_count':len(weekly_records)})
            
            # ä½¿ç”¨pymysqlç›´æ¥è¿æ¥
            conn = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE,
                autocommit=False
            )
            
            try:
                with conn.cursor() as cursor:
                    #æ‰¹é‡æ’å…¥ä¸»è¡¨
                    if main_records:
                        try:
                            insert_sql = """
                                INSERT INTO t_user_health_data 
                                (device_sn, user_id, org_id, customer_id, heart_rate, blood_oxygen, temperature, 
                                 pressure_high, pressure_low, stress, step, distance, calorie, 
                                 latitude, longitude, altitude, sleep, timestamp, upload_method, create_time)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                            """
                            
                            for record in main_records:
                                cursor.execute(insert_sql, (
                                    record.get('device_sn'),
                                    record.get('user_id'),
                                    record.get('org_id'),
                                    record.get('customer_id'),
                                    record.get('heart_rate'),
                                    record.get('blood_oxygen'),
                                    record.get('temperature'),
                                    record.get('pressure_high'),
                                    record.get('pressure_low'),
                                    record.get('stress'),
                                    record.get('step'),
                                    record.get('distance'),
                                    record.get('calorie'),
                                    record.get('latitude'),
                                    record.get('longitude'),
                                    record.get('altitude'),
                                    record.get('sleep'),
                                    record.get('timestamp'),
                                    record.get('upload_method')
                                ))
                            
                            conn.commit()
                            db_logger.info('ä¸»è¡¨æ‰¹é‡æ’å…¥æˆåŠŸ',extra={'data_count':len(main_records)})
                        except Exception as e:
                            print(f"âŒ ä¸»è¡¨æ‰¹é‡æ’å…¥å¤±è´¥è¯¦ç»†é”™è¯¯: {str(e)}")
                            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
                            import traceback
                            print(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                            db_logger.error('ä¸»è¡¨æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•å•æ¡æ’å…¥å¤„ç†é‡å¤',extra={'error':str(e),'data_count':len(main_records)})
                            conn.rollback()
                            # æ‰¹é‡æ’å…¥å¤±è´¥æ—¶ï¼Œä½¿ç”¨å•æ¡æ’å…¥å¤„ç†é‡å¤è®°å½•
                            try:
                                for record in main_records:
                                    cursor.execute("""
                                        SELECT id FROM t_user_health_data 
                                        WHERE device_sn = %s AND timestamp = %s
                                    """, (record.get('device_sn'), record.get('timestamp')))
                                    
                                    existing = cursor.fetchone()
                                    if not existing:
                                        cursor.execute(insert_sql, (
                                            record.get('device_sn'),
                                            record.get('user_id'),
                                            record.get('org_id'),
                                            record.get('customer_id'),
                                            record.get('heart_rate'),
                                            record.get('blood_oxygen'),
                                            record.get('temperature'),
                                            record.get('pressure_high'),
                                            record.get('pressure_low'),
                                            record.get('stress'),
                                            record.get('step'),
                                            record.get('distance'),
                                            record.get('calorie'),
                                            record.get('latitude'),
                                            record.get('longitude'),
                                            record.get('altitude'),
                                            record.get('sleep'),
                                            record.get('timestamp'),
                                            record.get('upload_method')
                                        ))
                                    else:
                                        self.stats['duplicates'] += 1
                                        db_logger.debug('è·³è¿‡é‡å¤è®°å½•',extra={'device_sn':record.get('device_sn'),'timestamp':record.get('timestamp')})
                                
                                conn.commit()
                                db_logger.info('ä¸»è¡¨å•æ¡æ’å…¥å®Œæˆ',extra={'data_count':len(main_records)})
                            except Exception as fallback_error:
                                print(f"âŒ ä¸»è¡¨å•æ¡æ’å…¥å¤±è´¥è¯¦ç»†é”™è¯¯: {str(fallback_error)}")
                                print(f"âŒ å•æ¡æ’å…¥é”™è¯¯ç±»å‹: {type(fallback_error).__name__}")
                                import traceback
                                print(f"âŒ å•æ¡æ’å…¥é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                                db_logger.error('ä¸»è¡¨å•æ¡æ’å…¥ä¹Ÿå¤±è´¥',extra={'error':str(fallback_error),'data_count':len(main_records)})
                                conn.rollback()
                    
                    #æ‰¹é‡å¤„ç†æ¯æ—¥è¡¨
                    if daily_records:
                        try:
                            for record in daily_records:
                                db_logger.debug('å¤„ç†æ¯æ—¥è®°å½•',extra={'device_sn':record['device_sn'],'date':record['date']})
                                
                                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                                cursor.execute("""
                                    SELECT id FROM t_user_health_data_daily 
                                    WHERE device_sn = %s AND date = %s
                                """, (record['device_sn'], record['date']))
                                
                                existing = cursor.fetchone()
                                
                                if existing:
                                    cursor.execute("""
                                        UPDATE t_user_health_data_daily 
                                        SET sleep_data = %s, exercise_daily_data = %s, workout_data = %s, update_time = NOW()
                                        WHERE id = %s
                                    """, (
                                        record.get('sleep_data'),
                                        record.get('exercise_daily_data'),
                                        record.get('workout_data'),
                                        existing[0]
                                    ))
                                else:
                                    cursor.execute("""
                                        INSERT INTO t_user_health_data_daily 
                                        (device_sn, user_id, org_id, customer_id, date, sleep_data, exercise_daily_data, workout_data, create_time, update_time)
                                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                                    """, (
                                        record['device_sn'],
                                        record['user_id'],
                                        record['org_id'],
                                        record.get('customer_id'),
                                        record['date'],
                                        record.get('sleep_data'),
                                        record.get('exercise_daily_data'),
                                        record.get('workout_data')
                                    ))
                            
                            conn.commit()
                            db_logger.info('æ¯æ—¥è¡¨æ‰¹é‡å¤„ç†æˆåŠŸ',extra={'data_count':len(daily_records)})
                        except Exception as e:
                            db_logger.error('æ¯æ—¥è¡¨å¤„ç†å¤±è´¥',extra={'error':str(e),'data_count':len(daily_records)})
                            conn.rollback()
                    
                    #æ‰¹é‡å¤„ç†æ¯å‘¨è¡¨
                    if weekly_records:
                        try:
                            for record in weekly_records:
                                db_logger.debug('å¤„ç†æ¯å‘¨è®°å½•',extra={'device_sn':record['device_sn'],'week_start':record['week_start']})
                                
                                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                                cursor.execute("""
                                    SELECT id FROM t_user_health_data_weekly 
                                    WHERE device_sn = %s AND week_start = %s
                                """, (record['device_sn'], record['week_start']))
                                
                                existing = cursor.fetchone()
                                
                                if existing:
                                    cursor.execute("""
                                        UPDATE t_user_health_data_weekly 
                                        SET exercise_week_data = %s, update_time = NOW()
                                        WHERE id = %s
                                    """, (
                                        record.get('exercise_week_data'),
                                        existing[0]
                                    ))
                                else:
                                    cursor.execute("""
                                        INSERT INTO t_user_health_data_weekly 
                                        (device_sn, user_id, org_id, customer_id, week_start, exercise_week_data, create_time, update_time)
                                        VALUES (%s, %s, %s, %s, %s, %s, NOW(), NOW())
                                    """, (
                                        record['device_sn'],
                                        record['user_id'],
                                        record['org_id'],
                                        record.get('customer_id'),
                                        record['week_start'],
                                        record.get('exercise_week_data')
                                    ))
                            
                            conn.commit()
                            db_logger.info('æ¯å‘¨è¡¨æ‰¹é‡å¤„ç†æˆåŠŸ',extra={'data_count':len(weekly_records)})
                        except Exception as e:
                            db_logger.error('æ¯å‘¨è¡¨å¤„ç†å¤±è´¥',extra={'error':str(e),'data_count':len(weekly_records)})
                            conn.rollback()
                            
            finally:
                conn.close()
            
            #å¼‚æ­¥å¤„ç†Rediså’Œå‘Šè­¦
            try:
                for item in batch_data:
                    if not self.executor._shutdown:  #æ£€æŸ¥çº¿ç¨‹æ± æ˜¯å¦å·²å…³é—­
                        self.executor.submit(self._async_process,item)
                    else:
                        #å¦‚æœçº¿ç¨‹æ± å·²å…³é—­ï¼Œç›´æ¥åŒæ­¥å¤„ç†
                        self._async_process(item)
            except RuntimeError as re:
                if 'cannot schedule new futures after shutdown' in str(re):
                    #çº¿ç¨‹æ± å·²å…³é—­ï¼Œæ”¹ä¸ºåŒæ­¥å¤„ç†
                    for item in batch_data:
                        self._async_process(item)
                else:
                    raise re
                
            self.stats['processed']+=len(batch_data)
            self.stats['batches']+=1
            health_logger.info('æ‰¹å¤„ç†å®Œæˆ',extra={'data_count':len(batch_data),'batch_id':self.stats['batches']})
            
        except Exception as e:
            self.stats['errors']+=1
            health_logger.error('æ‰¹å¤„ç†å¤±è´¥',extra={'error':str(e),'data_count':len(batch_data) if batch_data else 0},exc_info=True)
            
    def _insert_main_one_by_one(self,main_records):#å•æ¡æ’å…¥ä¸»è¡¨å¤„ç†é‡å¤
        success_count=0
        for record in main_records:
            try:
                existing=db.session.query(UserHealthData.id).filter(
                    and_(UserHealthData.device_sn==record['device_sn'],
                        UserHealthData.timestamp==record['timestamp'])
                ).first()
                
                if existing:
                    self.stats['duplicates']+=1
                    continue
                    
                health_record=UserHealthData(**record)
                db.session.add(health_record)
                db.session.commit()
                success_count+=1
                
            except Exception as e:
                try:
                    db.session.rollback()
                except:
                    pass#å¿½ç•¥å›æ»šé”™è¯¯
                if 'Duplicate entry' not in str(e):
                    logger.error(f'å•æ¡æ’å…¥å¤±è´¥: {e}')
                else:
                    self.stats['duplicates']+=1
                    
        logger.info(f'ä¸»è¡¨å•æ¡æ’å…¥å®Œæˆ: {success_count}æ¡æˆåŠŸ')
            
    def _async_process(self,item):#å¼‚æ­¥å¤„ç†Rediså’Œå‘Šè­¦
        try:
            device_sn=item['device_sn']
            redis_logger.info('Redisæ•°æ®æ›´æ–°å¼€å§‹',extra={'device_sn':device_sn})
            
            redis.hset_data(f"health_data:{device_sn}",mapping=item['redis_data'])
            redis.publish(f"health_data_channel:{device_sn}",device_sn)
            redis_logger.info('Redisæ•°æ®æ›´æ–°å®Œæˆ',extra={'device_sn':device_sn,'data_count':len(item['redis_data'])})
            
            if item.get('enable_alerts',True):
                from logging_config import alert_logger
                alert_logger.info('å‘Šè­¦æ£€æµ‹å¼€å§‹',extra={'device_sn':device_sn})
                
                if self.app:
                    with self.app.app_context():#ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨generate_alerts
                        generate_alerts(item['redis_data'],item.get('health_data_id'))
                else:
                    generate_alerts(item['redis_data'],item.get('health_data_id'))
                    
                alert_logger.info('å‘Šè­¦æ£€æµ‹å®Œæˆ',extra={'device_sn':device_sn})
                
        except Exception as e:
            health_logger.error('å¼‚æ­¥å¤„ç†å¤±è´¥',extra={'device_sn':item.get('device_sn','unknown'),'error':str(e)},exc_info=True)
            
    def get_health_config_fields(self,customer_id):#è·å–å¥åº·æ•°æ®é…ç½®å­—æ®µ
        """æ ¹æ®å®¢æˆ·IDè·å–é…ç½®çš„å¥åº·æ•°æ®å­—æ®µ"""
        try:
            configs=HealthDataConfig.query.filter_by(customer_id=customer_id,is_enabled=True).all()
            if not configs:
                return self._get_default_config()
                
            fields=[config.data_type for config in configs]
            weights={config.data_type:float(config.weight) if config.weight else 1.0 for config in configs}
            
            # ç¡®ä¿æ ¸å¿ƒå­—æ®µåŒ…å«åœ¨é…ç½®ä¸­ï¼Œå³ä½¿æ•°æ®åº“é…ç½®ä¸­æ²¡æœ‰
            # heart_rate æ˜¯åŸºç¡€å­—æ®µï¼Œpressure_high å’Œ pressure_low æ ¹æ® heart_rate æ¨¡æ‹Ÿç”Ÿæˆ
            # latitude, longitude, altitude æ˜¯ä½ç½®ä¿¡æ¯ï¼Œä¹Ÿæ˜¯åŸºç¡€å­—æ®µ
            essential_fields = ['heart_rate', 'pressure_high', 'pressure_low', 'latitude', 'longitude', 'altitude']
            for field in essential_fields:
                if field not in fields:
                    fields.append(field)
                    weights[field] = 1.0
            
            return {'fields':fields,'weights':weights,'config_source':'customer','customer_id':customer_id}
            
        except Exception as e:
            logger.error(f'è·å–å¥åº·é…ç½®å­—æ®µå¤±è´¥: {e}')
            return self._get_default_config()
    
    def _get_default_config(self):#é»˜è®¤é…ç½®
        """è¿”å›é»˜è®¤çš„å¥åº·æ•°æ®é…ç½®"""
        default_fields=list(self.field_mapping.keys())
        return {'fields':default_fields,'weights':{field:1.0 for field in default_fields},'config_source':'default','customer_id':None}

    def _get_week_start(self,date_obj):#è·å–å‘¨å¼€å§‹æ—¥æœŸ
        """è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å‘¨çš„å‘¨ä¸€æ—¥æœŸ"""
        if isinstance(date_obj,datetime):
            date_obj=date_obj.date()
        days_since_monday=date_obj.weekday()
        return date_obj-timedelta(days=days_since_monday)

    def _get_user_org_info(self,device_sn):#è·å–ç”¨æˆ·ç»„ç»‡ä¿¡æ¯
        """æ ¹æ®è®¾å¤‡SNè·å–ç”¨æˆ·å’Œç»„ç»‡ä¿¡æ¯"""
        print(f"ğŸ” å¼€å§‹æŸ¥è¯¢ç”¨æˆ·ç»„ç»‡ä¿¡æ¯: device_sn={device_sn}")
        try:
            # ç›´æ¥ä½¿ç”¨pymysqlè¿æ¥ï¼Œé¿å…SQLAlchemyçš„URLè§£æé—®é¢˜
            conn = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE
            )
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}")
            
            try:
                with conn.cursor() as cursor:
                    query_sql = """
                        SELECT u.id as user_id, uo.org_id
                        FROM sys_user u 
                        LEFT JOIN sys_user_org uo ON u.id = uo.user_id
                        WHERE u.device_sn = %s AND u.is_deleted = 0
                        LIMIT 1
                    """
                    print(f"ğŸ” æ‰§è¡Œç”¨æˆ·æŸ¥è¯¢SQL: {query_sql} with device_sn={device_sn}")
                    cursor.execute(query_sql, (device_sn,))
                    result = cursor.fetchone()
                    print(f"ğŸ” ç”¨æˆ·æŸ¥è¯¢ç»“æœ: {result}")
                    
                    if result:
                        user_id, org_id = result[0], result[1]
                        customer_id = None
                        print(f"âœ… æ‰¾åˆ°ç”¨æˆ·: user_id={user_id}, org_id={org_id}")
                        
                        # é€šè¿‡org_idæŸ¥æ‰¾sys_org_unitsçš„ancestorsè·å–ç§Ÿæˆ·ID
                        if org_id:
                            org_query_sql = """
                                SELECT ancestors FROM sys_org_units 
                                WHERE id = %s AND is_deleted = 0
                                LIMIT 1
                            """
                            print(f"ğŸ” æ‰§è¡Œç»„ç»‡æŸ¥è¯¢SQL: {org_query_sql} with org_id={org_id}")
                            cursor.execute(org_query_sql, (org_id,))
                            org_result = cursor.fetchone()
                            print(f"ğŸ” ç»„ç»‡æŸ¥è¯¢ç»“æœ: {org_result}")
                            
                            if org_result and org_result[0]:
                                ancestors = org_result[0]
                                print(f"ğŸ” ç»„ç»‡ancestors: {ancestors}")
                                # è§£æancestorsæ ¼å¼(0,X,Y...)ï¼Œè·å–ç¬¬äºŒä¸ªæ•°å­—Xä½œä¸ºç§Ÿæˆ·ID
                                parts = ancestors.split(',')
                                if len(parts) >= 2 and parts[0] == '0':
                                    try:
                                        customer_id = int(parts[1])
                                        print(f"âœ… è§£æå‡ºcustomer_id: {customer_id}")
                                    except ValueError:
                                        print(f"âŒ ancestorsæ ¼å¼å¼‚å¸¸: {ancestors}")
                                        logger.warning(f'ancestorsæ ¼å¼å¼‚å¸¸: {ancestors}')
                                else:
                                    print(f"âš ï¸ ancestorsæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ: {ancestors}")
                            else:
                                print(f"âš ï¸ æœªæ‰¾åˆ°ç»„ç»‡ä¿¡æ¯æˆ–ancestorsä¸ºç©º")
                        else:
                            print(f"âš ï¸ org_idä¸ºç©º")
                        
                        # åˆ›å»ºç±»ä¼¼SQLAlchemyç»“æœçš„å¯¹è±¡
                        class UserOrgInfo:
                            def __init__(self, user_id, org_id, customer_id):
                                self.user_id = user_id
                                self.org_id = org_id
                                self.customer_id = customer_id
                        
                        user_org_info = UserOrgInfo(user_id, org_id, customer_id)
                        print(f"âœ… ç”¨æˆ·ç»„ç»‡ä¿¡æ¯æ„å»ºå®Œæˆ: user_id={user_org_info.user_id}, org_id={user_org_info.org_id}, customer_id={user_org_info.customer_id}")
                        return user_org_info
                    else:
                        print(f"âŒ æœªæ‰¾åˆ°è®¾å¤‡å¯¹åº”çš„ç”¨æˆ·: {device_sn}")
                        return None
            finally:
                conn.close()
                print(f"âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
                
        except Exception as e:
            print(f"âŒ è·å–ç”¨æˆ·ç»„ç»‡ä¿¡æ¯å¼‚å¸¸: {e}")
            print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
            import traceback
            print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            logger.error(f'è·å–ç”¨æˆ·ç»„ç»‡ä¿¡æ¯å¤±è´¥: {e}')
            return None

    def add_data(self,raw_data,device_sn,enable_alerts=True):#æ·»åŠ æ•°æ®åˆ°é˜Ÿåˆ—
        """é…ç½®åŒ–å¤„ç†å¥åº·æ•°æ®ä¸Šä¼ """
        print(f"ğŸ”§ ä¼˜åŒ–å™¨æ·»åŠ æ•°æ®å¼€å§‹: device_sn={device_sn}, raw_data={json.dumps(raw_data, ensure_ascii=False)}")
        try:
            #ç¡®ä¿æ‰¹å¤„ç†å™¨å·²å¯åŠ¨
            self._ensure_processor_started()
            
            # ä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¼ é€’çš„å®¢æˆ·ä¿¡æ¯å‚æ•° - æ”¯æŒä¸¤ç§å­—æ®µåæ ¼å¼
            user_id = raw_data.get("user_id") or raw_data.get("userId")
            org_id = raw_data.get("org_id") or raw_data.get("orgId")
            customer_id = raw_data.get("customer_id") or raw_data.get("customerId")
            
            print(f"ğŸ” ç›´æ¥ä¼ å…¥çš„å®¢æˆ·ä¿¡æ¯: user_id={user_id}, org_id={org_id}, customer_id={customer_id}")
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥ä¼ é€’å®¢æˆ·ä¿¡æ¯ï¼Œé€šè¿‡deviceSnæŸ¥è¯¢è·å–ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            if not user_id or not org_id or not customer_id:
                print(f"ğŸ” å®¢æˆ·ä¿¡æ¯ä¸å®Œæ•´ï¼Œé€šè¿‡deviceSnæŸ¥è¯¢è·å–")
                user_org_info=self._get_user_org_info(device_sn)
                if not user_org_info:
                    print(f"âŒ æœªæ‰¾åˆ°è®¾å¤‡å¯¹åº”ç”¨æˆ·: {device_sn}")
                    logger.warning(f'æœªæ‰¾åˆ°è®¾å¤‡å¯¹åº”ç”¨æˆ·: {device_sn}')
                    return {'success':False,'reason':'user_not_found','message':'è®¾å¤‡å¯¹åº”ç”¨æˆ·æœªæ‰¾åˆ°'}
                    
                user_id = user_id or user_org_info.user_id
                org_id = org_id or user_org_info.org_id
                customer_id = customer_id or user_org_info.customer_id
                print(f"âœ… è¡¥å……åçš„å®¢æˆ·ä¿¡æ¯: user_id={user_id}, org_id={org_id}, customer_id={customer_id}")
            else:
                print(f"âœ… ä½¿ç”¨ç›´æ¥ä¼ é€’çš„å®¢æˆ·ä¿¡æ¯")
            
            #è·å–é…ç½®å­—æ®µ
            config_info=self.get_health_config_fields(customer_id) if customer_id else self._get_default_config()
            config_fields=config_info['fields']
            print(f"ğŸ” å¥åº·æ•°æ®é…ç½®å­—æ®µ: {config_fields}")
            
            #æ—¶é—´æˆ³å¤„ç†
            timestamp=raw_data.get("timestamp") or raw_data.get("cjsj") or datetime.now()
            print(f"ğŸ” åŸå§‹æ—¶é—´æˆ³: {timestamp}")
            if isinstance(timestamp,str):
                try:
                    timestamp=datetime.strptime(timestamp,'%Y-%m-%d %H:%M:%S')
                except:
                    timestamp=datetime.now()
            print(f"ğŸ” å¤„ç†åæ—¶é—´æˆ³: {timestamp}")
            
            #æ­£ç¡®çš„é‡å¤æ£€æµ‹ï¼šåªç”¨device_sn+timestampï¼ŒæŸ¥è¯¢æ•°æ®åº“è€Œéå†…å­˜
            duplicate_key=f"{device_sn}:{timestamp.strftime('%Y-%m-%d %H:%M:%S')}"
            print(f"ğŸ” é‡å¤æ£€æµ‹é”®: {duplicate_key}")
            
            #ç›´æ¥æŸ¥è¯¢æ•°æ®åº“æ£€æŸ¥æ˜¯å¦é‡å¤ï¼Œè€Œä¸æ˜¯ä¾èµ–å†…å­˜ç¼“å­˜
            try:
                conn = pymysql.connect(host=MYSQL_HOST,port=MYSQL_PORT,user=MYSQL_USER,password=MYSQL_PASSWORD,database=MYSQL_DATABASE)
                try:
                    with conn.cursor() as cursor:
                        cursor.execute("SELECT id FROM t_user_health_data WHERE device_sn = %s AND timestamp = %s LIMIT 1",(device_sn,timestamp))
                        existing_record = cursor.fetchone()
                        if existing_record:
                            print(f"âš ï¸ æ•°æ®åº“ä¸­å·²å­˜åœ¨ç›¸åŒè®°å½•: {duplicate_key}, id={existing_record[0]}")
                            logger.info(f'è·³è¿‡é‡å¤æ•°æ®(æ•°æ®åº“å·²å­˜åœ¨): {duplicate_key}')
                            return {'success':True,'reason':'duplicate','message':'æ•°æ®åº“ä¸­å·²å­˜åœ¨ç›¸åŒæ—¶é—´æˆ³æ•°æ®'}
                        else:
                            print(f"âœ… æ•°æ®åº“é‡å¤æ£€æŸ¥é€šè¿‡: {duplicate_key}")
                finally:
                    conn.close()
            except Exception as e:
                print(f"âŒ é‡å¤æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†: {e}")
                logger.warning(f'é‡å¤æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†: {e}')
                #å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œç»§ç»­å¤„ç†æ•°æ®
            
            #åˆ†ç¦»å­—æ®µç±»å‹
            fast_fields=['heart_rate','blood_oxygen','temperature','pressure_high','pressure_low','stress','step','distance','calorie','latitude','longitude','altitude','sleep']
            slow_daily_fields=['sleep_data','exercise_daily_data','workout_data','scientific_sleep_data']
            slow_weekly_fields=['exercise_week_data']
            print(f"ğŸ” å­—æ®µåˆ†ç¦»: fast_fields={fast_fields}")
            
            #æ„å»ºä¸»è¡¨æ•°æ®(åªåŒ…å«é…ç½®æ”¯æŒçš„å¿«æ›´æ–°å­—æ®µ)
            # å¤„ç†upload_methodå­—æ®µï¼Œå°†4gæ˜ å°„ä¸ºesim
            upload_method = raw_data.get("upload_method", "wifi")
            if upload_method == "4g":
                upload_method = "esim"
            main_data={'device_sn':device_sn,'user_id':user_id,'org_id':org_id,'customer_id':customer_id,'timestamp':timestamp,'upload_method':upload_method}
            print(f"ğŸ” åˆå§‹ä¸»è¡¨æ•°æ®: {main_data}")
            
            for field in fast_fields:
                if field in config_fields:
                    if field == 'sleep':
                        # ä¸“é—¨å¤„ç†ç¡çœ æ•°æ®è§£æ
                        sleep_data_raw = raw_data.get('sleepData') or raw_data.get('smData')
                        value = parse_sleep_data(sleep_data_raw)
                        print(f"ğŸ” ç¡çœ æ•°æ®è§£æ: raw={sleep_data_raw} -> value={value}")
                    else:
                        value=raw_data.get(self.field_mapping.get(field,field))
                        print(f"ğŸ” å­—æ®µæ˜ å°„: {field} -> {self.field_mapping.get(field,field)} = {value}")
                    if value is not None:
                        main_data[field]=value
                else:
                    print(f"âš ï¸ å­—æ®µ {field} ä¸åœ¨é…ç½®å­—æ®µä¸­ï¼Œè·³è¿‡å¤„ç†")
            print(f"âœ… å®Œæ•´ä¸»è¡¨æ•°æ®: {main_data}")
            
            #æ„å»ºæ¯æ—¥æ•°æ®(åªåŒ…å«é…ç½®æ”¯æŒçš„æ¯æ—¥å­—æ®µ)
            daily_data=None
            daily_fields_in_config=[f for f in slow_daily_fields if f in config_fields]
            if daily_fields_in_config:
                daily_data={'device_sn':device_sn,'user_id':user_id,'org_id':org_id,'customer_id':customer_id,'date':timestamp.date()}
                for field in daily_fields_in_config:
                    value=raw_data.get(self.field_mapping.get(field,field))
                    if value is not None:
                        daily_data[field]=value
                print(f"âœ… æ¯æ—¥æ•°æ®: {daily_data}")
            else:
                print(f"âš ï¸ æ— æ¯æ—¥æ•°æ®å­—æ®µ")
                        
            #æ„å»ºæ¯å‘¨æ•°æ®(åªåŒ…å«é…ç½®æ”¯æŒçš„æ¯å‘¨å­—æ®µ)
            weekly_data=None
            weekly_fields_in_config=[f for f in slow_weekly_fields if f in config_fields]
            if weekly_fields_in_config:
                week_start=self._get_week_start(timestamp)
                weekly_data={'device_sn':device_sn,'user_id':user_id,'org_id':org_id,'customer_id':customer_id,'week_start':week_start}
                for field in weekly_fields_in_config:
                    value=raw_data.get(self.field_mapping.get(field,field))
                    if value is not None:
                        weekly_data[field]=value
                print(f"âœ… æ¯å‘¨æ•°æ®: {weekly_data}")
            else:
                print(f"âš ï¸ æ— æ¯å‘¨æ•°æ®å­—æ®µ")
            
            #æ„å»ºRedisæ•°æ®(åŒ…å«é…ç½®å­—æ®µå’Œå®¢æˆ·ä¿¡æ¯)
            redis_data={}
            for field in config_fields:
                if field in self.field_mapping:
                    api_field=self.field_mapping[field]
                    value=raw_data.get(api_field)
                    if value is not None:
                        redis_data[api_field]=str(value)
            redis_data['deviceSn']=device_sn
            # æ·»åŠ å®¢æˆ·ä¿¡æ¯ç”¨äºå‘Šè­¦è§„åˆ™ç¼“å­˜
            redis_data['customer_id']=customer_id
            redis_data['customerId']=customer_id  # å…¼å®¹æ€§å­—æ®µå
            redis_data['org_id']=org_id
            redis_data['user_id']=user_id
            print(f"âœ… Redisæ•°æ®(å«å®¢æˆ·ä¿¡æ¯): {redis_data}")
                
            item={'device_sn':device_sn,'main_data':main_data,'daily_data':daily_data,'weekly_data':weekly_data,'redis_data':redis_data,'enable_alerts':enable_alerts,'config_info':config_info}
            print(f"ğŸ”§ å‡†å¤‡åŠ å…¥åˆ†ç‰‡é˜Ÿåˆ—çš„æ•°æ®é¡¹: {json.dumps(item, ensure_ascii=False, default=str)}")
            success = self.sharded_processor.add_data(item, device_sn)
            if success:
                print(f"âœ… æ•°æ®å·²æˆåŠŸåŠ å…¥åˆ†ç‰‡å¤„ç†é˜Ÿåˆ—: {device_sn}")
            else:
                print(f"âŒ æ•°æ®åŠ å…¥åˆ†ç‰‡å¤„ç†é˜Ÿåˆ—å¤±è´¥: {device_sn}")
                return {'success': False, 'reason': 'queue_full', 'message': 'åˆ†ç‰‡å¤„ç†é˜Ÿåˆ—å·²æ»¡'}
            # ä¸å†ç»´æŠ¤å†…å­˜ä¸­çš„processed_keysï¼Œå› ä¸ºé‡å¤æ£€æµ‹å·²åœ¨æ•°æ®åº“å±‚é¢å®Œæˆ
            # self.processed_keys.add(duplicate_key)
            return {'success':True,'reason':'queued','message':'æ•°æ®å·²åŠ å…¥å¤„ç†é˜Ÿåˆ—'}
            
        except queue.Full:
            print(f"âŒ æ‰¹å¤„ç†é˜Ÿåˆ—å·²æ»¡")
            logger.warning('æ‰¹å¤„ç†é˜Ÿåˆ—å·²æ»¡')
            return {'success':False,'reason':'queue_full','message':'å¤„ç†é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åé‡è¯•'}
        except Exception as e:
            print(f"âŒ æ·»åŠ æ•°æ®å¼‚å¸¸: {e}")
            print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
            import traceback
            print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            logger.error(f'æ·»åŠ æ•°æ®å¤±è´¥: {e}')
            return {'success':False,'reason':'error','message':f'æ•°æ®å¤„ç†å¤±è´¥: {str(e)}'}
            
    def _record_performance(self, batch_size, processing_time):
        """è®°å½•æ€§èƒ½æ•°æ®å¹¶å°è¯•è‡ªåŠ¨è°ƒä¼˜"""
        throughput = batch_size / processing_time if processing_time > 0 else 0
        
        self.performance_window.append({
            'batch_size': batch_size,
            'processing_time': processing_time,
            'throughput': throughput,
            'timestamp': time.time()
        })
        
        # ä¿æŒæ€§èƒ½çª—å£å¤§å°
        if len(self.performance_window) > 50:
            self.performance_window.pop(0)
        
        # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦è°ƒä¼˜
        current_time = time.time()
        if current_time - self.last_adjustment_time > 30 and len(self.performance_window) >= 10:
            self._auto_adjust_batch_size()
            self.last_adjustment_time = current_time
    
    def _auto_adjust_batch_size(self):
        """è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
        import psutil
        
        # è®¡ç®—æœ€è¿‘æ€§èƒ½æŒ‡æ ‡
        recent_performance = self.performance_window[-10:]
        avg_throughput = sum(p['throughput'] for p in recent_performance) / len(recent_performance)
        avg_processing_time = sum(p['processing_time'] for p in recent_performance) / len(recent_performance)
        
        # ç³»ç»Ÿèµ„æºæ£€æŸ¥
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_percent = psutil.virtual_memory().percent
        queue_size = self.sharded_processor.get_queue_size()
        
        old_batch_size = self.batch_size
        
        # è°ƒä¼˜é€»è¾‘
        if cpu_percent < 50 and avg_throughput < 100:
            # CPUåˆ©ç”¨ç‡ä½ï¼Œååé‡ä½ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°
            self.batch_size = min(500, int(self.batch_size * 1.2))
        elif cpu_percent > 90 or memory_percent > 85:
            # èµ„æºå‹åŠ›å¤§ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°
            self.batch_size = max(50, int(self.batch_size * 0.8))
        elif queue_size > 2000:
            # é˜Ÿåˆ—å †ç§¯ä¸¥é‡ï¼Œå¢åŠ å¤„ç†èƒ½åŠ›
            self.batch_size = min(500, int(self.batch_size * 1.1))
        
        # è®°å½•è°ƒæ•´
        if old_batch_size != self.batch_size:
            self.stats['auto_adjustments'] += 1
            logger.info(f"ğŸ“Š HealthDataæ‰¹æ¬¡å¤§å°è‡ªåŠ¨è°ƒæ•´: {old_batch_size} â†’ {self.batch_size} "
                       f"(CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_percent:.1f}%, "
                       f"é˜Ÿåˆ—: {queue_size}, ååé‡: {avg_throughput:.1f}/ç§’)")
    
    def get_stats(self):#è·å–ç»Ÿè®¡ä¿¡æ¯
        stats=self.stats.copy()
        stats['cpu_cores'] = getattr(self, 'cpu_cores', 'N/A')
        stats['batch_size'] = self.batch_size
        stats['max_workers'] = self.executor._max_workers
        stats['queue_size']=self.sharded_processor.get_queue_size()
        stats['performance_window_size'] = len(getattr(self, 'performance_window', []))
        # ä¸å†ç»Ÿè®¡processed_keys_countï¼Œå› ä¸ºå·²ç§»é™¤å†…å­˜é‡å¤æ£€æµ‹
        # stats['processed_keys_count']=len(self.processed_keys)
        return stats
        
    def clear_processed_keys(self):#æ¸…ç†å·²å¤„ç†é”®å€¼
        """æ™ºèƒ½æ¸…ç†è¿‡æœŸçš„å¤„ç†é”®å€¼"""
        if len(self.processed_keys)>10000:
            # åªä¿ç•™æœ€è¿‘1å°æ—¶çš„è®°å½•
            import time
            current_time=time.time()
            new_keys=set()
            
            for key in self.processed_keys:
                try:
                    # ä»é”®å€¼ä¸­æå–æ—¶é—´æˆ³ï¼Œæ ¼å¼ï¼šdevice_sn:YYYY-MM-DD HH:MM:SS
                    parts=key.split(':',1)  # åªåˆ†å‰²ä¸€æ¬¡ï¼Œé˜²æ­¢æ—¶é—´æˆ³ä¸­çš„å†’å·è¢«è¯¯åˆ†
                    if len(parts)==2:
                        device_sn,timestamp_str=parts
                        key_timestamp=datetime.strptime(timestamp_str,'%Y-%m-%d %H:%M:%S').timestamp()
                        # ä¿ç•™1å°æ—¶å†…çš„è®°å½•
                        if current_time-key_timestamp<3600:
                            new_keys.add(key)
                except:
                    continue
                    
            self.processed_keys=new_keys
            logger.info(f'æ™ºèƒ½æ¸…ç†processed_keysç¼“å­˜ï¼Œä¿ç•™{len(new_keys)}æ¡è®°å½•')
        
    def _schedule_cleanup(self):#å®šæ—¶æ¸…ç†ä»»åŠ¡
        """å®šæ—¶æ¸…ç†è¿‡æœŸé”®å€¼"""
        import threading
        def cleanup_worker():
            while self.running:
                time.sleep(1800)#æ¯30åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                self.clear_processed_keys()
        
        threading.Thread(target=cleanup_worker,daemon=True).start()

#å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
optimizer=HealthDataOptimizer()

def optimized_upload_health_data(health_data):#ä¼˜åŒ–çš„å¥åº·æ•°æ®ä¸Šä¼ V3.1
    """é…ç½®åŒ–å¥åº·æ•°æ®ä¸Šä¼ å¤„ç†"""
    print(f"ğŸ¥ å¥åº·æ•°æ®ä¸Šä¼ å¼€å§‹ - åŸå§‹æ•°æ®: {json.dumps(health_data, ensure_ascii=False, indent=2)}")
    try:
        # åœ¨Flaskè·¯ç”±ä¸Šä¸‹æ–‡ä¸­è·å–åº”ç”¨å®ä¾‹å¹¶ä¼ é€’ç»™ä¼˜åŒ–å™¨
        try:
            from flask import current_app
            if current_app and not optimizer.app:
                optimizer.app = current_app._get_current_object()
        except RuntimeError:
            pass  # å¿½ç•¥åº”ç”¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨çš„é”™è¯¯
            
        data=health_data.get("data",{})
        
        # æå–é¡¶çº§çš„å®¢æˆ·ä¿¡æ¯å‚æ•° - æ”¯æŒä¸¤ç§å­—æ®µåæ ¼å¼
        customer_id = health_data.get("customer_id") or health_data.get("customerId")
        org_id = health_data.get("org_id") or health_data.get("orgId")
        user_id = health_data.get("user_id") or health_data.get("userId")
        
        # å¦‚æœé¡¶çº§æ²¡æœ‰ï¼Œä»dataå­—æ®µä¸­æå–
        if not customer_id and isinstance(data, dict):
            customer_id = data.get("customer_id") or data.get("customerId")
        if not org_id and isinstance(data, dict):
            org_id = data.get("org_id") or data.get("orgId")
        if not user_id and isinstance(data, dict):
            user_id = data.get("user_id") or data.get("userId")
        
        print(f"ğŸ” æå–å®¢æˆ·ä¿¡æ¯: customer_id={customer_id}, org_id={org_id}, user_id={user_id}")
        print(f"ğŸ” è§£ædataå­—æ®µ: {json.dumps(data, ensure_ascii=False, indent=2)}")
        
        if isinstance(data,list):
            print(f"ğŸ” æ£€æµ‹åˆ°æ‰¹é‡æ•°æ®ï¼Œæ•°é‡: {len(data)}")
            if len(data)>10:#å¤§æ‰¹é‡ä½¿ç”¨é˜Ÿåˆ—
                print(f"ğŸ¥ å¤§æ‰¹é‡å¤„ç†æ¨¡å¼: {len(data)}æ¡æ•°æ®")
                results=[]
                success_count=0
                duplicate_count=0
                error_count=0
                
                for i, item in enumerate(data):
                    # ä¼˜å…ˆä»ç›´æ¥å­—æ®µè·å–è®¾å¤‡SN
                    device_sn = item.get("deviceSn") or item.get("id")
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—çš„dataå­—æ®µ
                    if not device_sn and 'data' in item:
                        nested_data = item['data']
                        if isinstance(nested_data, dict):
                            device_sn = nested_data.get('deviceSn') or nested_data.get('id')
                        elif isinstance(nested_data, list) and len(nested_data) > 0:
                            device_sn = nested_data[0].get('deviceSn') or nested_data[0].get('id')
                    
                    # å°†å®¢æˆ·ä¿¡æ¯æ·»åŠ åˆ°æ¯ä¸ªæ•°æ®é¡¹ä¸­ï¼Œä¼˜å…ˆä½¿ç”¨é¡¶çº§å‚æ•°
                    if customer_id is not None:
                        item['customer_id'] = customer_id
                    if org_id is not None:
                        item['org_id'] = org_id
                    if user_id is not None:
                        item['user_id'] = user_id
                    
                    print(f"ğŸ” å¤„ç†ç¬¬{i+1}æ¡æ•°æ®: device_sn={device_sn}, æ•°æ®={json.dumps(item, ensure_ascii=False)}")
                    if device_sn:
                        result=optimizer.add_data(item,device_sn)
                        results.append(result)
                        print(f"ğŸ” å¤„ç†ç»“æœ: {result}")
                        
                        if result.get('success'):
                            if result.get('reason')=='duplicate':
                                duplicate_count+=1
                            else:
                                success_count+=1
                        else:
                            error_count+=1
                    else:
                        print(f"âŒ ç¬¬{i+1}æ¡æ•°æ®ç¼ºå°‘è®¾å¤‡SN")
                        error_count+=1
                
                response_msg=f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ{success_count}æ¡"
                if duplicate_count>0:
                    response_msg+=f"ï¼Œé‡å¤{duplicate_count}æ¡"
                if error_count>0:
                    response_msg+=f"ï¼Œå¤±è´¥{error_count}æ¡"
                
                print(f"âœ… å¤§æ‰¹é‡å¤„ç†å®Œæˆ: {response_msg}")
                return jsonify({"status":"success","message":response_msg,"details":{"success":success_count,"duplicate":duplicate_count,"error":error_count}})
            else:#å°æ‰¹é‡ç›´æ¥å¤„ç†
                print(f"ğŸ¥ å°æ‰¹é‡å¤„ç†æ¨¡å¼: {len(data)}æ¡æ•°æ®")
                return _process_batch_direct(data)
        else:
            # ä¼˜å…ˆä»ç›´æ¥å­—æ®µè·å–è®¾å¤‡SN
            device_sn = data.get("deviceSn") or data.get("id")
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—çš„dataå­—æ®µï¼ˆé’ˆå¯¹data.data.idçš„æƒ…å†µï¼‰
            if not device_sn and 'data' in data:
                nested_data = data['data']
                if isinstance(nested_data, dict):
                    device_sn = nested_data.get('deviceSn') or nested_data.get('id')
                    print(f"ğŸ” ä»åµŒå¥—dataå¯¹è±¡æå–device_sn: {device_sn}")
                elif isinstance(nested_data, list) and len(nested_data) > 0:
                    device_sn = nested_data[0].get('deviceSn') or nested_data[0].get('id')
                    print(f"ğŸ” ä»åµŒå¥—dataæ•°ç»„æå–device_sn: {device_sn}")
            
            print(f"ğŸ” å•æ¡æ•°æ®å¤„ç†: device_sn={device_sn}")
            if not device_sn:
                print(f"âŒ è®¾å¤‡IDä¸ºç©º")
                return jsonify({"status":"error","message":"è®¾å¤‡IDä¸èƒ½ä¸ºç©º"})
            
            # å°†å®¢æˆ·ä¿¡æ¯æ·»åŠ åˆ°æ•°æ®é¡¹ä¸­ï¼Œä¼˜å…ˆä½¿ç”¨é¡¶çº§å‚æ•°
            if customer_id is not None:
                data['customer_id'] = customer_id
            if org_id is not None:
                data['org_id'] = org_id
            if user_id is not None:
                data['user_id'] = user_id
            
            print(f"ğŸ” å•æ¡æ•°æ®è¯¦æƒ…: {json.dumps(data, ensure_ascii=False, indent=2)}")
            #å•æ¡æ•°æ®ä½¿ç”¨é˜Ÿåˆ—å¤„ç†
            result=optimizer.add_data(data,device_sn)
            print(f"ğŸ” å•æ¡å¤„ç†ç»“æœ: {result}")
            if result.get('success'):
                if result.get('reason')=='duplicate':
                    print(f"âš ï¸ æ•°æ®é‡å¤: {device_sn}")
                    return jsonify({"status":"success","message":"æ•°æ®é‡å¤ï¼Œå·²è·³è¿‡å¤„ç†","reason":"duplicate"})
                else:
                    print(f"âœ… å•æ¡æ•°æ®å¤„ç†æˆåŠŸ: {device_sn}")
                    return jsonify({"status":"success","message":result.get('message','æ•°æ®å¤„ç†æˆåŠŸ')})
            else:
                print(f"âŒ å•æ¡æ•°æ®å¤„ç†å¤±è´¥: {result}")
                # æ ¹æ®å¤±è´¥åŸå› è¿”å›ä¸åŒçš„çŠ¶æ€ç 
                if result.get('reason')=='user_not_found':
                    return jsonify({"status":"error","message":result.get('message')}),404
                elif result.get('reason')=='queue_full':
                    return jsonify({"status":"error","message":result.get('message')}),503
                else:
                    return jsonify({"status":"error","message":result.get('message')}),500
            
    except Exception as e:
        print(f"âŒ å¥åº·æ•°æ®ä¸Šä¼ å¼‚å¸¸: {e}")
        print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
        import traceback
        print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        logger.error(f'æ•°æ®ä¸Šä¼ å¤±è´¥: {e}')
        return jsonify({"status":"error","message":str(e)}),500

def _process_batch_direct(data_list):#å°æ‰¹é‡ç›´æ¥å¤„ç†
    """å°æ‰¹é‡æ•°æ®ç›´æ¥åŒæ­¥å¤„ç†"""
    print(f"ğŸ¥ å°æ‰¹é‡ç›´æ¥å¤„ç†å¼€å§‹ï¼Œæ•°æ®é‡: {len(data_list)}")
    try:
        success_count=0
        duplicate_count=0
        error_count=0
        
        for i, data in enumerate(data_list):
            # ä¼˜å…ˆä»ç›´æ¥å­—æ®µè·å–è®¾å¤‡SN
            device_sn = data.get("deviceSn") or data.get("id")
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åµŒå¥—çš„dataå­—æ®µ
            if not device_sn and 'data' in data:
                nested_data = data['data']
                if isinstance(nested_data, dict):
                    device_sn = nested_data.get('deviceSn') or nested_data.get('id')
                elif isinstance(nested_data, list) and len(nested_data) > 0:
                    device_sn = nested_data[0].get('deviceSn') or nested_data[0].get('id')
            
            print(f"ğŸ” å°æ‰¹é‡å¤„ç†ç¬¬{i+1}æ¡: device_sn={device_sn}, æ•°æ®={json.dumps(data, ensure_ascii=False)}")
            if device_sn:
                result=optimizer.add_data(data,device_sn)
                print(f"ğŸ” å°æ‰¹é‡å¤„ç†ç»“æœ: {result}")
                if result.get('success'):
                    if result.get('reason')=='duplicate':
                        duplicate_count+=1
                    else:
                        success_count+=1
                else:
                    error_count+=1
            else:
                print(f"âŒ ç¬¬{i+1}æ¡æ•°æ®ç¼ºå°‘è®¾å¤‡SN")
                error_count+=1
        
        response_msg=f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ{success_count}æ¡"
        if duplicate_count>0:
            response_msg+=f"ï¼Œé‡å¤{duplicate_count}æ¡"
        if error_count>0:
            response_msg+=f"ï¼Œå¤±è´¥{error_count}æ¡"
        
        print(f"âœ… å°æ‰¹é‡ç›´æ¥å¤„ç†å®Œæˆ: {response_msg}")
        return jsonify({"status":"success","message":response_msg,"details":{"success":success_count,"duplicate":duplicate_count,"error":error_count}})
        
    except Exception as e:
        print(f"âŒ å°æ‰¹é‡ç›´æ¥å¤„ç†å¼‚å¸¸: {e}")
        print(f"âŒ å¼‚å¸¸è¯¦æƒ…: {type(e).__name__} - {str(e)}")
        import traceback
        print(f"âŒ å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        logger.error(f'å°æ‰¹é‡ç›´æ¥å¤„ç†å¤±è´¥: {e}')
        return jsonify({"status":"error","message":f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"}),500

# ================================ å¼‚æ­¥å¤„ç†æ¡†æ¶ ================================

@dataclass
class AsyncTaskConfig:
    """å¼‚æ­¥ä»»åŠ¡é…ç½®"""
    parsing_queue_size: int = 1000
    alert_queue_size: int = 500
    aggregation_queue_size: int = 300
    notification_queue_size: int = 200
    parsing_workers: int = 8
    alert_workers: int = 12
    aggregation_workers: int = 6
    notification_workers: int = 4
    batch_timeout: float = 2.0
    max_retry_attempts: int = 3
    notification_timeout: float = 10.0

class AsyncHealthDataProcessor:
    """å¼‚æ­¥å¥åº·æ•°æ®å¤„ç†å™¨ - å¤šé˜¶æ®µæµæ°´çº¿å¤„ç†æ¶æ„"""
    
    def __init__(self, config: Optional[AsyncTaskConfig] = None):
        self.config = config or AsyncTaskConfig()
        
        # CPUè‡ªé€‚åº”é…ç½®
        self.cpu_cores = psutil.cpu_count(logical=True)
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # åŠ¨æ€è°ƒæ•´å·¥ä½œè€…æ•°é‡
        self._adjust_workers_by_resources()
        
        # å¤šé˜¶æ®µå¼‚æ­¥é˜Ÿåˆ—
        self.parsing_queue = asyncio.Queue(maxsize=self.config.parsing_queue_size)
        self.alert_queue = asyncio.Queue(maxsize=self.config.alert_queue_size)
        self.aggregation_queue = asyncio.Queue(maxsize=self.config.aggregation_queue_size)
        self.notification_queue = asyncio.Queue(maxsize=self.config.notification_queue_size)
        
        # ä¸“ç”¨å¼‚æ­¥å·¥ä½œæ± 
        self.parsing_workers = []
        self.alert_workers = []
        self.aggregation_workers = []
        self.notification_workers = []
        
        # çŠ¶æ€ç®¡ç†
        self.running = False
        self.stats = {
            'processed_total': 0,
            'parsing_processed': 0,
            'alerts_generated': 0,
            'aggregations_processed': 0,
            'notifications_sent': 0,
            'errors': {'parsing': 0, 'alerts': 0, 'aggregation': 0, 'notification': 0},
            'performance': {'avg_parsing_time': 0, 'avg_alert_time': 0, 'avg_aggregation_time': 0}
        }
        
        logger.info(f"ğŸš€ AsyncHealthDataProcessor åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   CPUæ ¸å¿ƒ: {self.cpu_cores}, å†…å­˜: {self.memory_gb:.1f}GB")
        logger.info(f"   å·¥ä½œè€…é…ç½® - è§£æ: {self.config.parsing_workers}, "
                   f"å‘Šè­¦: {self.config.alert_workers}, "
                   f"èšåˆ: {self.config.aggregation_workers}, "
                   f"é€šçŸ¥: {self.config.notification_workers}")
    
    def _adjust_workers_by_resources(self):
        """æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´å·¥ä½œè€…æ•°é‡"""
        # è§£æå·¥ä½œè€…: CPUæ ¸å¿ƒæ•° Ã— 1ï¼Œå¤„ç†CPUå¯†é›†å‹çš„JSONè§£æ
        self.config.parsing_workers = max(4, min(16, self.cpu_cores))
        
        # å‘Šè­¦å·¥ä½œè€…: CPUæ ¸å¿ƒæ•° Ã— 1.5ï¼Œå¹³è¡¡è®¡ç®—å’ŒI/O
        self.config.alert_workers = max(6, min(20, int(self.cpu_cores * 1.5)))
        
        # èšåˆå·¥ä½œè€…: CPUæ ¸å¿ƒæ•° Ã— 0.75ï¼Œæ•°æ®åº“å¯†é›†å‹
        self.config.aggregation_workers = max(3, min(12, int(self.cpu_cores * 0.75)))
        
        # é€šçŸ¥å·¥ä½œè€…: å›ºå®šæ•°é‡ï¼Œé¿å…è¿‡å¤šç½‘ç»œè¿æ¥
        self.config.notification_workers = max(2, min(8, int(self.cpu_cores * 0.5)))
    
    async def start(self):
        """å¯åŠ¨å¼‚æ­¥å¤„ç†å™¨"""
        if self.running:
            return
            
        self.running = True
        logger.info("ğŸ”„ å¯åŠ¨å¼‚æ­¥å¥åº·æ•°æ®å¤„ç†å™¨...")
        
        # å¯åŠ¨è§£æå·¥ä½œè€…
        for i in range(self.config.parsing_workers):
            worker = asyncio.create_task(self._parsing_worker(f"parser-{i}"))
            self.parsing_workers.append(worker)
        
        # å¯åŠ¨å‘Šè­¦å·¥ä½œè€…
        for i in range(self.config.alert_workers):
            worker = asyncio.create_task(self._alert_worker(f"alert-{i}"))
            self.alert_workers.append(worker)
        
        # å¯åŠ¨èšåˆå·¥ä½œè€…
        for i in range(self.config.aggregation_workers):
            worker = asyncio.create_task(self._aggregation_worker(f"aggregation-{i}"))
            self.aggregation_workers.append(worker)
        
        # å¯åŠ¨é€šçŸ¥å·¥ä½œè€…
        for i in range(self.config.notification_workers):
            worker = asyncio.create_task(self._notification_worker(f"notification-{i}"))
            self.notification_workers.append(worker)
        
        logger.info(f"âœ… å¼‚æ­¥å¤„ç†å™¨å¯åŠ¨å®Œæˆï¼Œæ€»è®¡ {len(self.parsing_workers + self.alert_workers + self.aggregation_workers + self.notification_workers)} ä¸ªå·¥ä½œè€…")
    
    async def stop(self):
        """åœæ­¢å¼‚æ­¥å¤„ç†å™¨"""
        if not self.running:
            return
            
        self.running = False
        logger.info("ğŸ›‘ åœæ­¢å¼‚æ­¥å¥åº·æ•°æ®å¤„ç†å™¨...")
        
        # å–æ¶ˆæ‰€æœ‰å·¥ä½œè€…
        all_workers = self.parsing_workers + self.alert_workers + self.aggregation_workers + self.notification_workers
        for worker in all_workers:
            worker.cancel()
        
        # ç­‰å¾…å·¥ä½œè€…å®Œæˆ
        await asyncio.gather(*all_workers, return_exceptions=True)
        
        # æ¸…ç©ºå·¥ä½œè€…åˆ—è¡¨
        self.parsing_workers.clear()
        self.alert_workers.clear()
        self.aggregation_workers.clear()
        self.notification_workers.clear()
        
        logger.info("âœ… å¼‚æ­¥å¤„ç†å™¨å·²åœæ­¢")
    
    async def process_health_data(self, health_data: dict) -> dict:
        """å¼‚æ­¥æµæ°´çº¿å…¥å£"""
        task_id = str(uuid.uuid4())
        
        # å¿«é€Ÿå“åº”ï¼Œç«‹å³å°†æ•°æ®æ”¾å…¥è§£æé˜Ÿåˆ—
        await self.parsing_queue.put({
            'task_id': task_id,
            'health_data': health_data,
            'timestamp': datetime.now()
        })
        
        self.stats['processed_total'] += 1
        
        return {
            "success": True,
            "task_id": task_id,
            "status": "processing",
            "message": "æ•°æ®å·²è¿›å…¥å¼‚æ­¥å¤„ç†ç®¡é“"
        }
    
    async def _parsing_worker(self, worker_name: str):
        """æ•°æ®è§£æå·¥ä½œè€…"""
        logger.info(f"ğŸ” {worker_name} å¯åŠ¨")
        
        while self.running:
            try:
                # ä»è§£æé˜Ÿåˆ—è·å–ä»»åŠ¡
                task = await asyncio.wait_for(
                    self.parsing_queue.get(),
                    timeout=self.config.batch_timeout
                )
                
                start_time = time.time()
                
                # è§£æå¥åº·æ•°æ®
                parsed_data = await self._parse_health_data_async(task)
                
                if parsed_data:
                    # å°†è§£æç»“æœæ”¾å…¥å‘Šè­¦é˜Ÿåˆ—
                    await self.alert_queue.put(parsed_data)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    processing_time = time.time() - start_time
                    self.stats['parsing_processed'] += 1
                    self._update_avg_time('avg_parsing_time', processing_time)
                
            except asyncio.TimeoutError:
                continue  # è¶…æ—¶ç»§ç»­ç­‰å¾…
            except Exception as e:
                self.stats['errors']['parsing'] += 1
                logger.error(f"âŒ {worker_name} å¤„ç†å¤±è´¥: {e}")
        
        logger.info(f"ğŸ” {worker_name} å·²åœæ­¢")
    
    async def _alert_worker(self, worker_name: str):
        """å‘Šè­¦æ£€æµ‹å·¥ä½œè€…"""
        logger.info(f"âš ï¸ {worker_name} å¯åŠ¨")
        
        while self.running:
            try:
                # ä»å‘Šè­¦é˜Ÿåˆ—è·å–ä»»åŠ¡
                parsed_data = await asyncio.wait_for(
                    self.alert_queue.get(),
                    timeout=self.config.batch_timeout
                )
                
                start_time = time.time()
                
                # å¹¶è¡Œæ£€æµ‹å‘Šè­¦
                alert_results = await self._generate_alerts_async(parsed_data)
                
                # å°†ç»“æœæ”¾å…¥èšåˆé˜Ÿåˆ—
                aggregation_data = {**parsed_data, 'alert_results': alert_results}
                await self.aggregation_queue.put(aggregation_data)
                
                # å¦‚æœæœ‰å‘Šè­¦ï¼Œæ”¾å…¥é€šçŸ¥é˜Ÿåˆ—
                if alert_results:
                    await self.notification_queue.put({
                        'alerts': alert_results,
                        'device_sn': parsed_data.get('device_sn'),
                        'task_id': parsed_data.get('task_id')
                    })
                
                # æ›´æ–°ç»Ÿè®¡
                processing_time = time.time() - start_time
                self.stats['alerts_generated'] += len(alert_results) if alert_results else 0
                self._update_avg_time('avg_alert_time', processing_time)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                self.stats['errors']['alerts'] += 1
                logger.error(f"âŒ {worker_name} å¤„ç†å¤±è´¥: {e}")
        
        logger.info(f"âš ï¸ {worker_name} å·²åœæ­¢")
    
    async def _aggregation_worker(self, worker_name: str):
        """èšåˆè®¡ç®—å·¥ä½œè€…"""
        logger.info(f"ğŸ“Š {worker_name} å¯åŠ¨")
        
        while self.running:
            try:
                # ä»èšåˆé˜Ÿåˆ—è·å–ä»»åŠ¡
                aggregation_data = await asyncio.wait_for(
                    self.aggregation_queue.get(),
                    timeout=self.config.batch_timeout
                )
                
                start_time = time.time()
                
                # å¼‚æ­¥å¤„ç†èšåˆæ•°æ®
                await self._save_daily_weekly_data_async(aggregation_data)
                
                # æ›´æ–°ç»Ÿè®¡
                processing_time = time.time() - start_time
                self.stats['aggregations_processed'] += 1
                self._update_avg_time('avg_aggregation_time', processing_time)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                self.stats['errors']['aggregation'] += 1
                logger.error(f"âŒ {worker_name} å¤„ç†å¤±è´¥: {e}")
        
        logger.info(f"ğŸ“Š {worker_name} å·²åœæ­¢")
    
    async def _notification_worker(self, worker_name: str):
        """é€šçŸ¥å‘é€å·¥ä½œè€…"""
        logger.info(f"ğŸ“± {worker_name} å¯åŠ¨")
        
        while self.running:
            try:
                # ä»é€šçŸ¥é˜Ÿåˆ—è·å–ä»»åŠ¡
                notification_data = await asyncio.wait_for(
                    self.notification_queue.get(),
                    timeout=self.config.batch_timeout
                )
                
                # å¹¶è¡Œå‘é€é€šçŸ¥
                await self._send_notifications_async(notification_data)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['notifications_sent'] += len(notification_data.get('alerts', []))
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                self.stats['errors']['notification'] += 1
                logger.error(f"âŒ {worker_name} å¤„ç†å¤±è´¥: {e}")
        
        logger.info(f"ğŸ“± {worker_name} å·²åœæ­¢")
    
    async def _parse_health_data_async(self, task: dict) -> Optional[dict]:
        """å¼‚æ­¥è§£æå¥åº·æ•°æ®"""
        try:
            loop = asyncio.get_event_loop()
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†CPUå¯†é›†å‹çš„JSONè§£æ
            with ThreadPoolExecutor(max_workers=4) as parsing_pool:
                parsed_data = await loop.run_in_executor(
                    parsing_pool,
                    self._parse_health_data_sync,
                    task
                )
            
            return parsed_data
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥è§£æå¥åº·æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _parse_health_data_sync(self, task: dict) -> Optional[dict]:
        """åŒæ­¥è§£æå¥åº·æ•°æ®ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        try:
            health_data = task['health_data']
            task_id = task['task_id']
            timestamp = task['timestamp']
            
            # æå–è®¾å¤‡ä¿¡æ¯
            data = health_data.get("data", {})
            if isinstance(data, list) and len(data) > 0:
                data = data[0]  # å–ç¬¬ä¸€ä¸ªæ•°æ®é¡¹
            
            device_sn = data.get("deviceSn") or data.get("id")
            if not device_sn:
                return None
            
            # è§£æç¡çœ æ•°æ®ï¼ˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰- ä½¿ç”¨å¢å¼ºç‰ˆè§£æå™¨
            sleep_data = data.get('sleepData')
            parsed_sleep_duration = parse_sleep_data_enhanced(sleep_data) if sleep_data else None
            
            # æ„å»ºè§£æç»“æœ
            parsed_result = {
                'task_id': task_id,
                'device_sn': device_sn,
                'timestamp': timestamp,
                'original_data': health_data,
                'parsed_data': data,
                'sleep_duration': parsed_sleep_duration,
                'user_id': data.get('user_id'),
                'org_id': data.get('org_id'),
                'customer_id': data.get('customer_id')
            }
            
            return parsed_result
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥è§£æå¥åº·æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _generate_alerts_async(self, parsed_data: dict) -> List[dict]:
        """å¼‚æ­¥å¹¶è¡Œå‘Šè­¦æ£€æµ‹ - ä½¿ç”¨ä¸“ä¸šå‘Šè­¦å¤„ç†å™¨"""
        try:
            health_data = parsed_data.get('parsed_data', {})
            health_data_id = parsed_data.get('health_data_id')
            
            # ä½¿ç”¨å…¨å±€å‘Šè­¦å¤„ç†å™¨è¿›è¡Œå¹¶è¡Œå‘Šè­¦æ£€æµ‹
            active_alerts = await alert_processor.generate_alerts_async(health_data, health_data_id)
            
            return active_alerts
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥å‘Šè­¦ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    
    async def _save_daily_weekly_data_async(self, aggregation_data: dict):
        """å¼‚æ­¥ä¿å­˜æ¯æ—¥å’Œæ¯å‘¨èšåˆæ•°æ® - ä½¿ç”¨ä¸“ä¸šèšåˆå¤„ç†å™¨"""
        try:
            # æ„å»ºå¥åº·è®°å½•åˆ—è¡¨æ ¼å¼
            health_records = [{
                'device_sn': aggregation_data.get('device_sn'),
                'timestamp': aggregation_data.get('timestamp', datetime.now()),
                'user_id': aggregation_data.get('parsed_data', {}).get('user_id'),
                'org_id': aggregation_data.get('parsed_data', {}).get('org_id'),
                **aggregation_data.get('parsed_data', {})
            }]
            
            # ä½¿ç”¨å…¨å±€èšåˆå¤„ç†å™¨è¿›è¡Œå¹¶è¡Œèšåˆ
            result = await aggregation_processor.save_daily_weekly_data_async(health_records)
            
            logger.info(f"ğŸ“Š èšåˆæ•°æ®å¤„ç†å®Œæˆ: {result}")
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥ä¿å­˜èšåˆæ•°æ®å¤±è´¥: {e}")
    
    async def _send_notifications_async(self, notification_data: dict):
        """å¼‚æ­¥å‘é€é€šçŸ¥"""
        try:
            alerts = notification_data.get('alerts', [])
            device_sn = notification_data.get('device_sn')
            
            if not alerts:
                return
            
            # å¹¶è¡Œå‘é€ä¸åŒç±»å‹çš„é€šçŸ¥
            notification_tasks = []
            
            for alert in alerts:
                # å¾®ä¿¡é€šçŸ¥
                if alert.get('level') in ['warning', 'critical']:
                    task = self._send_wechat_notification(alert, device_sn)
                    notification_tasks.append(task)
                
                # é‚®ä»¶é€šçŸ¥ï¼ˆcriticalçº§åˆ«ï¼‰
                if alert.get('level') == 'critical':
                    task = self._send_email_notification(alert, device_sn)
                    notification_tasks.append(task)
            
            # å¹¶è¡Œå‘é€æ‰€æœ‰é€šçŸ¥
            if notification_tasks:
                await asyncio.gather(*notification_tasks, return_exceptions=True)
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥å‘é€é€šçŸ¥å¤±è´¥: {e}")
    
    async def _send_wechat_notification(self, alert: dict, device_sn: str):
        """å‘é€å¾®ä¿¡é€šçŸ¥"""
        try:
            # æ¨¡æ‹Ÿå¾®ä¿¡é€šçŸ¥å‘é€
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
            message = f"[å¥åº·å‘Šè­¦] è®¾å¤‡{device_sn}: {alert.get('message', '')}"
            logger.info(f"ğŸ“± å¾®ä¿¡é€šçŸ¥å·²å‘é€: {message}")
            
        except Exception as e:
            logger.error(f"âŒ å¾®ä¿¡é€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    async def _send_email_notification(self, alert: dict, device_sn: str):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        try:
            # æ¨¡æ‹Ÿé‚®ä»¶é€šçŸ¥å‘é€
            await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
            message = f"[ç´§æ€¥å¥åº·å‘Šè­¦] è®¾å¤‡{device_sn}: {alert.get('message', '')}"
            logger.info(f"ğŸ“§ é‚®ä»¶é€šçŸ¥å·²å‘é€: {message}")
            
        except Exception as e:
            logger.error(f"âŒ é‚®ä»¶é€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    def _update_avg_time(self, metric_key: str, new_time: float):
        """æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´"""
        current_avg = self.stats['performance'].get(metric_key, 0)
        # ç®€å•çš„ç§»åŠ¨å¹³å‡ç®—æ³•
        self.stats['performance'][metric_key] = (current_avg * 0.9) + (new_time * 0.1)
    
    def get_async_stats(self) -> dict:
        """è·å–å¼‚æ­¥å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'config': {
                'cpu_cores': self.cpu_cores,
                'memory_gb': round(self.memory_gb, 1),
                'workers': {
                    'parsing': self.config.parsing_workers,
                    'alert': self.config.alert_workers,
                    'aggregation': self.config.aggregation_workers,
                    'notification': self.config.notification_workers
                }
            },
            'runtime': {
                'running': self.running,
                'total_workers': len(self.parsing_workers + self.alert_workers + 
                                   self.aggregation_workers + self.notification_workers)
            },
            'queues': {
                'parsing_queue_size': self.parsing_queue.qsize(),
                'alert_queue_size': self.alert_queue.qsize(),
                'aggregation_queue_size': self.aggregation_queue.qsize(),
                'notification_queue_size': self.notification_queue.qsize()
            },
            'statistics': self.stats
        }

# å…¨å±€å¼‚æ­¥å¤„ç†å™¨å®ä¾‹
async_processor = None

async def get_async_processor() -> AsyncHealthDataProcessor:
    """è·å–å…¨å±€å¼‚æ­¥å¤„ç†å™¨å®ä¾‹"""
    global async_processor
    if async_processor is None:
        async_processor = AsyncHealthDataProcessor()
        await async_processor.start()
    return async_processor

# ================================ å¼‚æ­¥ç¡çœ æ•°æ®è§£æå™¨ ================================

class AsyncSleepDataParser:
    """å¼‚æ­¥ç¡çœ æ•°æ®è§£æå™¨ - CPUå¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–"""
    
    def __init__(self, max_workers: int = 8):
        self.max_workers = max_workers
        self.parser_pool = ThreadPoolExecutor(max_workers=max_workers)
        self.stats = {
            'parsed_total': 0,
            'parse_errors': 0,
            'avg_parse_time': 0.0,
            'batch_processed': 0
        }
        
        logger.info(f"ğŸ›ï¸ AsyncSleepDataParser åˆå§‹åŒ– - å·¥ä½œçº¿ç¨‹: {max_workers}")
    
    async def parse_sleep_data_async(self, sleep_data_json) -> Optional[float]:
        """å¼‚æ­¥è§£æå•ä¸ªç¡çœ æ•°æ®"""
        try:
            loop = asyncio.get_event_loop()
            start_time = time.time()
            
            # CPUå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨çº¿ç¨‹æ± 
            result = await loop.run_in_executor(
                self.parser_pool, 
                self._parse_sleep_data_sync, 
                sleep_data_json
            )
            
            # æ›´æ–°ç»Ÿè®¡
            parse_time = time.time() - start_time
            self.stats['parsed_total'] += 1
            self._update_avg_parse_time(parse_time)
            
            return result
            
        except Exception as e:
            self.stats['parse_errors'] += 1
            logger.error(f"âŒ å¼‚æ­¥ç¡çœ æ•°æ®è§£æå¤±è´¥: {e}")
            return None
    
    def _parse_sleep_data_sync(self, sleep_data_json) -> Optional[float]:
        """åŒæ­¥è§£æé€»è¾‘ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        # å¤ç”¨ç°æœ‰çš„parse_sleep_dataå‡½æ•°é€»è¾‘ï¼Œä½†ä¼˜åŒ–æ€§èƒ½
        return parse_sleep_data(sleep_data_json)
    
    async def batch_parse_sleep_data(self, batch_data: List[dict]) -> List[Optional[float]]:
        """æ‰¹é‡å¼‚æ­¥è§£æç¡çœ æ•°æ®"""
        try:
            start_time = time.time()
            
            # åˆ›å»ºå¹¶è¡Œè§£æä»»åŠ¡
            parse_tasks = []
            for item in batch_data:
                sleep_data = item.get('sleepData') or item.get('sleep_data')
                if sleep_data:
                    task = self.parse_sleep_data_async(sleep_data)
                    parse_tasks.append(task)
                else:
                    # å¯¹äºæ²¡æœ‰ç¡çœ æ•°æ®çš„é¡¹ï¼Œç›´æ¥è¿”å›None
                    parse_tasks.append(asyncio.coroutine(lambda: None)())
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è§£æä»»åŠ¡
            results = await asyncio.gather(*parse_tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"âŒ æ‰¹é‡ç¡çœ æ•°æ®è§£æå¼‚å¸¸: {result}")
                    processed_results.append(None)
                else:
                    processed_results.append(result)
            
            # æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡
            batch_time = time.time() - start_time
            self.stats['batch_processed'] += 1
            
            logger.info(f"ğŸ›ï¸ æ‰¹é‡ç¡çœ æ•°æ®è§£æå®Œæˆ: {len(batch_data)}æ¡, è€—æ—¶: {batch_time:.2f}ç§’")
            
            return processed_results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç¡çœ æ•°æ®è§£æå¤±è´¥: {e}")
            return [None] * len(batch_data)
    
    def _update_avg_parse_time(self, new_time: float):
        """æ›´æ–°å¹³å‡è§£ææ—¶é—´"""
        current_avg = self.stats['avg_parse_time']
        # ç§»åŠ¨å¹³å‡ç®—æ³•
        self.stats['avg_parse_time'] = (current_avg * 0.9) + (new_time * 0.1)
    
    def get_parser_stats(self) -> dict:
        """è·å–è§£æå™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'config': {
                'max_workers': self.max_workers,
                'pool_active': not self.parser_pool._shutdown
            },
            'performance': {
                'parsed_total': self.stats['parsed_total'],
                'parse_errors': self.stats['parse_errors'],
                'avg_parse_time': round(self.stats['avg_parse_time'], 4),
                'batch_processed': self.stats['batch_processed'],
                'success_rate': round((self.stats['parsed_total'] - self.stats['parse_errors']) / 
                                    max(1, self.stats['parsed_total']) * 100, 2)
            }
        }
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'parser_pool') and not self.parser_pool._shutdown:
            self.parser_pool.shutdown(wait=True)

# å…¨å±€ç¡çœ æ•°æ®è§£æå™¨å®ä¾‹
sleep_parser = AsyncSleepDataParser()

# ================================ å¼‚æ­¥å‘Šè­¦å¤„ç†å™¨ ================================

class AsyncAlertProcessor:
    """å¼‚æ­¥å‘Šè­¦å¤„ç†å™¨ - å¹¶è¡Œå‘Šè­¦æ£€æµ‹ä¸é€šçŸ¥å‘é€"""
    
    def __init__(self, max_workers: int = 12, notification_workers: int = 4):
        self.alert_executor = ThreadPoolExecutor(max_workers=max_workers)
        self.notification_executor = ThreadPoolExecutor(max_workers=notification_workers)
        
        self.stats = {
            'alerts_processed': 0,
            'alerts_generated': 0,
            'notifications_sent': 0,
            'processing_errors': 0,
            'notification_errors': 0,
            'avg_alert_time': 0.0,
            'avg_notification_time': 0.0
        }
        
        logger.info(f"âš ï¸ AsyncAlertProcessor åˆå§‹åŒ– - å‘Šè­¦çº¿ç¨‹: {max_workers}, é€šçŸ¥çº¿ç¨‹: {notification_workers}")
    
    async def generate_alerts_async(self, health_data: dict, health_data_id: Optional[int] = None) -> List[dict]:
        """å¼‚æ­¥å‘Šè­¦æ£€æµ‹ä¸»å…¥å£"""
        try:
            start_time = time.time()
            
            # ğŸš€ è§„åˆ™æ£€æµ‹å¹¶è¡ŒåŒ–
            detection_tasks = [
                self._detect_heart_rate_alert(health_data),
                self._detect_blood_oxygen_alert(health_data),
                self._detect_temperature_alert(health_data),
                self._detect_blood_pressure_alert(health_data),
                self._detect_step_alert(health_data),
                self._detect_stress_alert(health_data),
                self._detect_sleep_alert(health_data),
                self._detect_calorie_alert(health_data),
                self._detect_distance_alert(health_data)
            ]
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å‘Šè­¦æ£€æµ‹
            alert_results = await asyncio.gather(*detection_tasks, return_exceptions=True)
            
            # è¿‡æ»¤æœ‰æ•ˆå‘Šè­¦
            active_alerts = []
            for result in alert_results:
                if isinstance(result, dict) and result:
                    active_alerts.append(result)
                elif isinstance(result, Exception):
                    logger.error(f"âŒ å‘Šè­¦æ£€æµ‹å¼‚å¸¸: {result}")
                    self.stats['processing_errors'] += 1
            
            if active_alerts:
                # ğŸ¯ æ•°æ®åº“å†™å…¥ä¸é€šçŸ¥å‘é€å¹¶è¡Œ
                db_task = self._save_alerts_to_db_async(active_alerts, health_data_id)
                notify_task = self._send_notifications_async(active_alerts, health_data.get('deviceSn', 'unknown'))
                
                # å¹¶è¡Œæ‰§è¡Œæ•°æ®åº“ä¿å­˜å’Œé€šçŸ¥å‘é€
                await asyncio.gather(db_task, notify_task, return_exceptions=True)
            
            # æ›´æ–°ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.stats['alerts_processed'] += 1
            self.stats['alerts_generated'] += len(active_alerts)
            self._update_avg_alert_time(processing_time)
            
            return active_alerts
            
        except Exception as e:
            self.stats['processing_errors'] += 1
            logger.error(f"âŒ å¼‚æ­¥å‘Šè­¦ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    async def _detect_heart_rate_alert(self, health_data: dict) -> Optional[dict]:
        """å¿ƒç‡å‘Šè­¦æ£€æµ‹"""
        try:
            heart_rate = self._extract_numeric_value(health_data, ['heart_rate', 'heartRate'])
            if not heart_rate:
                return None
            
            if heart_rate > 120:
                return self._create_alert('heart_rate_critical', heart_rate, 120, 
                                        f'å¿ƒç‡å±é™©: {heart_rate} bpm', 'critical')
            elif heart_rate > 100:
                return self._create_alert('heart_rate_high', heart_rate, 100, 
                                        f'å¿ƒç‡åé«˜: {heart_rate} bpm', 'warning')
            elif heart_rate < 50:
                return self._create_alert('heart_rate_low', heart_rate, 50, 
                                        f'å¿ƒç‡è¿‡ä½: {heart_rate} bpm', 'warning')
            
            return None
        except Exception as e:
            logger.error(f"å¿ƒç‡å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_blood_oxygen_alert(self, health_data: dict) -> Optional[dict]:
        """è¡€æ°§å‘Šè­¦æ£€æµ‹"""
        try:
            blood_oxygen = self._extract_numeric_value(health_data, ['blood_oxygen', 'bloodOxygen'])
            if not blood_oxygen:
                return None
            
            if blood_oxygen < 90:
                return self._create_alert('blood_oxygen_critical', blood_oxygen, 90,
                                        f'è¡€æ°§å±é™©: {blood_oxygen}%', 'critical')
            elif blood_oxygen < 95:
                return self._create_alert('blood_oxygen_low', blood_oxygen, 95,
                                        f'è¡€æ°§åä½: {blood_oxygen}%', 'warning')
            
            return None
        except Exception as e:
            logger.error(f"è¡€æ°§å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_temperature_alert(self, health_data: dict) -> Optional[dict]:
        """ä½“æ¸©å‘Šè­¦æ£€æµ‹"""
        try:
            temperature = self._extract_numeric_value(health_data, ['temperature', 'body_temperature'])
            if not temperature:
                return None
            
            if temperature > 38.5:
                return self._create_alert('temperature_critical', temperature, 38.5,
                                        f'ä½“æ¸©å±é™©: {temperature}Â°C', 'critical')
            elif temperature > 37.5:
                return self._create_alert('temperature_high', temperature, 37.5,
                                        f'ä½“æ¸©åé«˜: {temperature}Â°C', 'warning')
            elif temperature < 35.5:
                return self._create_alert('temperature_low', temperature, 35.5,
                                        f'ä½“æ¸©è¿‡ä½: {temperature}Â°C', 'warning')
            
            return None
        except Exception as e:
            logger.error(f"ä½“æ¸©å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_blood_pressure_alert(self, health_data: dict) -> Optional[dict]:
        """è¡€å‹å‘Šè­¦æ£€æµ‹"""
        try:
            systolic = self._extract_numeric_value(health_data, ['pressure_high', 'blood_pressure_systolic'])
            diastolic = self._extract_numeric_value(health_data, ['pressure_low', 'blood_pressure_diastolic'])
            
            # æ”¶ç¼©å‹æ£€æµ‹
            if systolic:
                if systolic > 160:
                    return self._create_alert('blood_pressure_critical', systolic, 160,
                                            f'æ”¶ç¼©å‹å±é™©: {systolic} mmHg', 'critical')
                elif systolic > 140:
                    return self._create_alert('blood_pressure_high', systolic, 140,
                                            f'æ”¶ç¼©å‹åé«˜: {systolic} mmHg', 'warning')
            
            # èˆ’å¼ å‹æ£€æµ‹
            if diastolic:
                if diastolic > 100:
                    return self._create_alert('blood_pressure_critical', diastolic, 100,
                                            f'èˆ’å¼ å‹å±é™©: {diastolic} mmHg', 'critical')
                elif diastolic > 90:
                    return self._create_alert('blood_pressure_high', diastolic, 90,
                                            f'èˆ’å¼ å‹åé«˜: {diastolic} mmHg', 'warning')
            
            return None
        except Exception as e:
            logger.error(f"è¡€å‹å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_step_alert(self, health_data: dict) -> Optional[dict]:
        """æ­¥æ•°å‘Šè­¦æ£€æµ‹"""
        try:
            steps = self._extract_numeric_value(health_data, ['step', 'steps'])
            if not steps:
                return None
            
            # æ ¹æ®æ—¶é—´åˆ¤æ–­æ­¥æ•°ç›®æ ‡ï¼ˆæ¯æ—¥ç›®æ ‡ï¼‰
            current_hour = datetime.now().hour
            expected_steps = max(500, int(6000 * current_hour / 24))  # åŠ¨æ€ç›®æ ‡
            
            if steps < expected_steps and current_hour > 18:  # æ™šä¸Š6ç‚¹åæ£€æµ‹
                return self._create_alert('steps_low', steps, expected_steps,
                                        f'ä»Šæ—¥æ­¥æ•°ä¸è¶³: {steps}æ­¥ï¼Œå»ºè®®å¢åŠ è¿åŠ¨', 'info')
            
            return None
        except Exception as e:
            logger.error(f"æ­¥æ•°å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_stress_alert(self, health_data: dict) -> Optional[dict]:
        """å‹åŠ›å‘Šè­¦æ£€æµ‹"""
        try:
            stress = self._extract_numeric_value(health_data, ['stress'])
            if not stress:
                return None
            
            if stress > 80:
                return self._create_alert('stress_critical', stress, 80,
                                        f'å‹åŠ›è¿‡å¤§: {stress}%ï¼Œè¯·ç«‹å³ä¼‘æ¯', 'critical')
            elif stress > 70:
                return self._create_alert('stress_high', stress, 70,
                                        f'å‹åŠ›åé«˜: {stress}%ï¼Œå»ºè®®æ”¾æ¾', 'warning')
            
            return None
        except Exception as e:
            logger.error(f"å‹åŠ›å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_sleep_alert(self, health_data: dict) -> Optional[dict]:
        """ç¡çœ å‘Šè­¦æ£€æµ‹"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¡çœ æ•°æ®
            sleep_data = health_data.get('sleepData') or health_data.get('sleep_data')
            if not sleep_data:
                return None
            
            # ä½¿ç”¨å¼‚æ­¥ç¡çœ è§£æå™¨
            sleep_duration = await sleep_parser.parse_sleep_data_async(sleep_data)
            if not sleep_duration:
                return None
            
            if sleep_duration < 5:
                return self._create_alert('sleep_insufficient', sleep_duration, 7,
                                        f'ç¡çœ ä¸è¶³: {sleep_duration}å°æ—¶', 'warning')
            elif sleep_duration > 10:
                return self._create_alert('sleep_excessive', sleep_duration, 10,
                                        f'ç¡çœ è¿‡å¤š: {sleep_duration}å°æ—¶', 'info')
            
            return None
        except Exception as e:
            logger.error(f"ç¡çœ å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_calorie_alert(self, health_data: dict) -> Optional[dict]:
        """å¡è·¯é‡Œå‘Šè­¦æ£€æµ‹"""
        try:
            calorie = self._extract_numeric_value(health_data, ['calorie', 'calories'])
            if not calorie:
                return None
            
            # æ ¹æ®æ—¶é—´åˆ¤æ–­å¡è·¯é‡Œç›®æ ‡
            current_hour = datetime.now().hour
            expected_calorie = max(200, int(1800 * current_hour / 24))
            
            if calorie < expected_calorie and current_hour > 18:
                return self._create_alert('calorie_low', calorie, expected_calorie,
                                        f'ä»Šæ—¥æ¶ˆè€—ä¸è¶³: {calorie}å¡è·¯é‡Œ', 'info')
            
            return None
        except Exception as e:
            logger.error(f"å¡è·¯é‡Œå‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _detect_distance_alert(self, health_data: dict) -> Optional[dict]:
        """è·ç¦»å‘Šè­¦æ£€æµ‹"""
        try:
            distance = self._extract_numeric_value(health_data, ['distance'])
            if not distance:
                return None
            
            # è·ç¦»ç›®æ ‡æ£€æµ‹ï¼ˆå…¬é‡Œï¼‰
            if distance < 2 and datetime.now().hour > 18:
                return self._create_alert('distance_low', distance, 5,
                                        f'ä»Šæ—¥è¿åŠ¨è·ç¦»ä¸è¶³: {distance}å…¬é‡Œ', 'info')
            
            return None
        except Exception as e:
            logger.error(f"è·ç¦»å‘Šè­¦æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    def _extract_numeric_value(self, health_data: dict, field_names: List[str]) -> Optional[float]:
        """ä»å¥åº·æ•°æ®ä¸­æå–æ•°å€¼"""
        for field in field_names:
            value = health_data.get(field)
            if value is not None:
                try:
                    return float(value)
                except (ValueError, TypeError):
                    continue
        return None
    
    def _create_alert(self, alert_type: str, value: float, threshold: float, message: str, level: str) -> dict:
        """åˆ›å»ºå‘Šè­¦å¯¹è±¡"""
        return {
            'type': alert_type,
            'value': value,
            'threshold': threshold,
            'message': message,
            'level': level,
            'timestamp': datetime.now().isoformat(),
            'device_sn': None  # å°†åœ¨ä¸Šå±‚è®¾ç½®
        }
    
    async def _save_alerts_to_db_async(self, alerts: List[dict], health_data_id: Optional[int]):
        """å¼‚æ­¥ä¿å­˜å‘Šè­¦åˆ°æ•°æ®åº“"""
        try:
            if not alerts:
                return
            
            loop = asyncio.get_event_loop()
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ•°æ®åº“æ“ä½œ
            await loop.run_in_executor(
                self.alert_executor,
                self._save_alerts_to_db_sync,
                alerts, health_data_id
            )
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥ä¿å­˜å‘Šè­¦å¤±è´¥: {e}")
    
    def _save_alerts_to_db_sync(self, alerts: List[dict], health_data_id: Optional[int]):
        """åŒæ­¥ä¿å­˜å‘Šè­¦åˆ°æ•°æ®åº“"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ç°æœ‰çš„generate_alertså‡½æ•°æˆ–ç›´æ¥æ“ä½œæ•°æ®åº“
            # ä¸ºäº†é›†æˆç°æœ‰ç³»ç»Ÿï¼Œå¯ä»¥è°ƒç”¨åŸæœ‰çš„å‘Šè­¦ç”Ÿæˆé€»è¾‘
            from .alert import generate_alerts
            
            # æ„é€ å…¼å®¹çš„æ•°æ®æ ¼å¼
            for alert in alerts:
                # è°ƒç”¨ç°æœ‰çš„å‘Šè­¦ç”Ÿæˆå‡½æ•°
                generate_alerts({
                    'deviceSn': alert.get('device_sn', 'unknown'),
                    'alert_type': alert['type'],
                    'alert_level': alert['level'],
                    'alert_message': alert['message'],
                    'alert_value': alert['value'],
                    'alert_threshold': alert['threshold']
                }, health_data_id)
            
            logger.info(f"ğŸ“ å‘Šè­¦æ•°æ®åº“ä¿å­˜å®Œæˆ: {len(alerts)}æ¡å‘Šè­¦")
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ä¿å­˜å‘Šè­¦å¤±è´¥: {e}")
    
    async def _send_notifications_async(self, alerts: List[dict], device_sn: str):
        """å¼‚æ­¥é€šçŸ¥å‘é€ï¼ˆå¾®ä¿¡ã€é‚®ä»¶ç­‰ï¼‰"""
        try:
            if not alerts:
                return
            
            start_time = time.time()
            notification_tasks = []
            
            for alert in alerts:
                alert['device_sn'] = device_sn  # è®¾ç½®è®¾å¤‡SN
                
                # æ ¹æ®å‘Šè­¦çº§åˆ«å†³å®šé€šçŸ¥æ–¹å¼
                if alert.get('level') in ['warning', 'critical']:
                    # å¾®ä¿¡é€šçŸ¥
                    task = self._send_wechat_notification_async(alert)
                    notification_tasks.append(task)
                
                # ç´§æ€¥å‘Šè­¦å‘é€é‚®ä»¶
                if alert.get('level') == 'critical':
                    task = self._send_email_notification_async(alert)
                    notification_tasks.append(task)
                
                # çŸ­ä¿¡é€šçŸ¥ï¼ˆcriticalçº§åˆ«ï¼‰
                if alert.get('level') == 'critical':
                    task = self._send_sms_notification_async(alert)
                    notification_tasks.append(task)
            
            # ğŸš€ å¹¶è¡Œå‘é€æ‰€æœ‰é€šçŸ¥
            if notification_tasks:
                results = await asyncio.gather(*notification_tasks, return_exceptions=True)
                
                # ç»Ÿè®¡å‘é€ç»“æœ
                success_count = sum(1 for r in results if not isinstance(r, Exception))
                error_count = len(results) - success_count
                
                self.stats['notifications_sent'] += success_count
                self.stats['notification_errors'] += error_count
            
            # æ›´æ–°é€šçŸ¥æ—¶é—´ç»Ÿè®¡
            notification_time = time.time() - start_time
            self._update_avg_notification_time(notification_time)
            
        except Exception as e:
            self.stats['notification_errors'] += 1
            logger.error(f"âŒ å¼‚æ­¥é€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    async def _send_wechat_notification_async(self, alert: dict):
        """å¼‚æ­¥å‘é€å¾®ä¿¡é€šçŸ¥"""
        try:
            # æ¨¡æ‹Ÿç½‘ç»œI/Oå»¶è¿Ÿ
            await asyncio.sleep(0.1)
            
            message = f"[å¥åº·å‘Šè­¦] è®¾å¤‡{alert.get('device_sn')}: {alert.get('message', '')}"
            logger.info(f"ğŸ“± å¾®ä¿¡é€šçŸ¥: {message}")
            
            return {"status": "success", "type": "wechat"}
            
        except Exception as e:
            logger.error(f"âŒ å¾®ä¿¡é€šçŸ¥å‘é€å¤±è´¥: {e}")
            raise e
    
    async def _send_email_notification_async(self, alert: dict):
        """å¼‚æ­¥å‘é€é‚®ä»¶é€šçŸ¥"""
        try:
            # æ¨¡æ‹Ÿç½‘ç»œI/Oå»¶è¿Ÿ
            await asyncio.sleep(0.2)
            
            message = f"[ç´§æ€¥å¥åº·å‘Šè­¦] è®¾å¤‡{alert.get('device_sn')}: {alert.get('message', '')}"
            logger.info(f"ğŸ“§ é‚®ä»¶é€šçŸ¥: {message}")
            
            return {"status": "success", "type": "email"}
            
        except Exception as e:
            logger.error(f"âŒ é‚®ä»¶é€šçŸ¥å‘é€å¤±è´¥: {e}")
            raise e
    
    async def _send_sms_notification_async(self, alert: dict):
        """å¼‚æ­¥å‘é€çŸ­ä¿¡é€šçŸ¥"""
        try:
            # æ¨¡æ‹Ÿç½‘ç»œI/Oå»¶è¿Ÿ
            await asyncio.sleep(0.15)
            
            message = f"[å¥åº·ç´§æ€¥å‘Šè­¦] {alert.get('device_sn')}: {alert.get('message', '')}"
            logger.info(f"ğŸ“² çŸ­ä¿¡é€šçŸ¥: {message}")
            
            return {"status": "success", "type": "sms"}
            
        except Exception as e:
            logger.error(f"âŒ çŸ­ä¿¡é€šçŸ¥å‘é€å¤±è´¥: {e}")
            raise e
    
    def _update_avg_alert_time(self, new_time: float):
        """æ›´æ–°å¹³å‡å‘Šè­¦å¤„ç†æ—¶é—´"""
        current_avg = self.stats['avg_alert_time']
        self.stats['avg_alert_time'] = (current_avg * 0.9) + (new_time * 0.1)
    
    def _update_avg_notification_time(self, new_time: float):
        """æ›´æ–°å¹³å‡é€šçŸ¥å‘é€æ—¶é—´"""
        current_avg = self.stats['avg_notification_time']
        self.stats['avg_notification_time'] = (current_avg * 0.9) + (new_time * 0.1)
    
    def get_alert_stats(self) -> dict:
        """è·å–å‘Šè­¦å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'config': {
                'alert_workers': self.alert_executor._max_workers,
                'notification_workers': self.notification_executor._max_workers,
                'alert_pool_active': not self.alert_executor._shutdown,
                'notification_pool_active': not self.notification_executor._shutdown
            },
            'performance': {
                'alerts_processed': self.stats['alerts_processed'],
                'alerts_generated': self.stats['alerts_generated'],
                'notifications_sent': self.stats['notifications_sent'],
                'processing_errors': self.stats['processing_errors'],
                'notification_errors': self.stats['notification_errors'],
                'avg_alert_time': round(self.stats['avg_alert_time'], 4),
                'avg_notification_time': round(self.stats['avg_notification_time'], 4),
                'alert_success_rate': round((self.stats['alerts_processed'] - self.stats['processing_errors']) / 
                                          max(1, self.stats['alerts_processed']) * 100, 2),
                'notification_success_rate': round((self.stats['notifications_sent']) / 
                                                  max(1, self.stats['notifications_sent'] + self.stats['notification_errors']) * 100, 2)
            }
        }
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'alert_executor') and not self.alert_executor._shutdown:
            self.alert_executor.shutdown(wait=True)
        if hasattr(self, 'notification_executor') and not self.notification_executor._shutdown:
            self.notification_executor.shutdown(wait=True)

# å…¨å±€å‘Šè­¦å¤„ç†å™¨å®ä¾‹
alert_processor = AsyncAlertProcessor()

# ================================ å¼‚æ­¥èšåˆè®¡ç®—å¤„ç†å™¨ ================================

class AsyncAggregationProcessor:
    """å¼‚æ­¥èšåˆè®¡ç®—å¤„ç†å™¨ - å¹¶è¡Œå¤„ç†æ¯æ—¥/æ¯å‘¨æ•°æ®èšåˆ"""
    
    def __init__(self, max_workers: int = 6):
        self.aggregation_executor = ThreadPoolExecutor(max_workers=max_workers)
        
        self.stats = {
            'daily_processed': 0,
            'weekly_processed': 0,
            'aggregation_errors': 0,
            'avg_daily_time': 0.0,
            'avg_weekly_time': 0.0,
            'device_groups_processed': 0
        }
        
        logger.info(f"ğŸ“Š AsyncAggregationProcessor åˆå§‹åŒ– - èšåˆçº¿ç¨‹: {max_workers}")
    
    async def save_daily_weekly_data_async(self, health_records: List[dict]) -> dict:
        """å¼‚æ­¥èšåˆæ•°æ®è®¡ç®—ä¸»å…¥å£"""
        try:
            start_time = time.time()
            
            # ğŸš€ æŒ‰è®¾å¤‡åˆ†ç»„ï¼Œå¹¶è¡Œå¤„ç†
            device_groups = self._group_by_device(health_records)
            
            if not device_groups:
                return {'status': 'success', 'processed': 0, 'message': 'æ²¡æœ‰éœ€è¦èšåˆçš„æ•°æ®'}
            
            # åˆ›å»ºå¹¶è¡Œèšåˆä»»åŠ¡
            aggregation_tasks = []
            for device_sn, records in device_groups.items():
                task = self._process_device_aggregation_async(device_sn, records)
                aggregation_tasks.append(task)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è®¾å¤‡çš„èšåˆè®¡ç®—
            results = await asyncio.gather(*aggregation_tasks, return_exceptions=True)
            
            # ç»Ÿè®¡å¤„ç†ç»“æœ
            success_count = 0
            error_count = 0
            daily_count = 0
            weekly_count = 0
            
            for result in results:
                if isinstance(result, dict) and result.get('success'):
                    success_count += 1
                    daily_count += result.get('daily_processed', 0)
                    weekly_count += result.get('weekly_processed', 0)
                elif isinstance(result, Exception):
                    error_count += 1
                    logger.error(f"âŒ è®¾å¤‡èšåˆè®¡ç®—å¼‚å¸¸: {result}")
                else:
                    error_count += 1
            
            # æ›´æ–°ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.stats['device_groups_processed'] += len(device_groups)
            self.stats['daily_processed'] += daily_count
            self.stats['weekly_processed'] += weekly_count
            self.stats['aggregation_errors'] += error_count
            
            logger.info(f"ğŸ“Š èšåˆè®¡ç®—å®Œæˆ: {len(device_groups)}ä¸ªè®¾å¤‡ç»„, "
                       f"æ¯æ—¥è®°å½•: {daily_count}, æ¯å‘¨è®°å½•: {weekly_count}, "
                       f"è€—æ—¶: {processing_time:.2f}ç§’")
            
            return {
                'status': 'success',
                'device_groups': len(device_groups),
                'daily_processed': daily_count,
                'weekly_processed': weekly_count,
                'errors': error_count,
                'processing_time': processing_time
            }
            
        except Exception as e:
            self.stats['aggregation_errors'] += 1
            logger.error(f"âŒ å¼‚æ­¥èšåˆè®¡ç®—å¤±è´¥: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def _group_by_device(self, health_records: List[dict]) -> Dict[str, List[dict]]:
        """æŒ‰è®¾å¤‡SNåˆ†ç»„å¥åº·è®°å½•"""
        device_groups = {}
        
        for record in health_records:
            device_sn = record.get('device_sn')
            if device_sn:
                if device_sn not in device_groups:
                    device_groups[device_sn] = []
                device_groups[device_sn].append(record)
        
        return device_groups
    
    async def _process_device_aggregation_async(self, device_sn: str, records: List[dict]) -> dict:
        """å¤„ç†å•è®¾å¤‡èšåˆè®¡ç®—"""
        try:
            loop = asyncio.get_event_loop()
            
            # å¹¶è¡Œè®¡ç®—æ¯æ—¥å’Œæ¯å‘¨ç»Ÿè®¡
            daily_task = loop.run_in_executor(
                self.aggregation_executor,
                self._calculate_daily_statistics,
                device_sn, records
            )
            
            weekly_task = loop.run_in_executor(
                self.aggregation_executor,
                self._calculate_weekly_statistics,
                device_sn, records
            )
            
            # ç­‰å¾…è®¡ç®—å®Œæˆ
            daily_stats, weekly_stats = await asyncio.gather(daily_task, weekly_task)
            
            # ğŸ¯ å¹¶è¡Œä¿å­˜åˆ°æ•°æ®åº“
            if daily_stats or weekly_stats:
                save_tasks = []
                
                if daily_stats:
                    save_tasks.append(
                        loop.run_in_executor(
                            self.aggregation_executor,
                            self._save_daily_data_sync,
                            device_sn, daily_stats
                        )
                    )
                
                if weekly_stats:
                    save_tasks.append(
                        loop.run_in_executor(
                            self.aggregation_executor,
                            self._save_weekly_data_sync,
                            device_sn, weekly_stats
                        )
                    )
                
                # å¹¶è¡Œä¿å­˜
                if save_tasks:
                    await asyncio.gather(*save_tasks, return_exceptions=True)
            
            return {
                'success': True,
                'device_sn': device_sn,
                'daily_processed': len(daily_stats) if daily_stats else 0,
                'weekly_processed': len(weekly_stats) if weekly_stats else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ è®¾å¤‡{device_sn}èšåˆè®¡ç®—å¤±è´¥: {e}")
            return {'success': False, 'device_sn': device_sn, 'error': str(e)}
    
    def _calculate_daily_statistics(self, device_sn: str, records: List[dict]) -> List[dict]:
        """è®¡ç®—æ¯æ—¥ç»Ÿè®¡æ•°æ®"""
        try:
            daily_stats = []
            
            # æŒ‰æ—¥æœŸåˆ†ç»„è®°å½•
            date_groups = {}
            for record in records:
                timestamp = record.get('timestamp')
                if timestamp:
                    if isinstance(timestamp, str):
                        date_obj = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).date()
                    else:
                        date_obj = timestamp.date() if hasattr(timestamp, 'date') else timestamp
                    
                    date_key = date_obj.strftime('%Y-%m-%d')
                    if date_key not in date_groups:
                        date_groups[date_key] = []
                    date_groups[date_key].append(record)
            
            # è®¡ç®—æ¯æ—¥èšåˆæ•°æ®
            for date_str, date_records in date_groups.items():
                daily_data = self._aggregate_daily_data(device_sn, date_str, date_records)
                if daily_data:
                    daily_stats.append(daily_data)
            
            return daily_stats
            
        except Exception as e:
            logger.error(f"âŒ è®¾å¤‡{device_sn}æ¯æ—¥ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            return []
    
    def _calculate_weekly_statistics(self, device_sn: str, records: List[dict]) -> List[dict]:
        """è®¡ç®—æ¯å‘¨ç»Ÿè®¡æ•°æ®"""
        try:
            weekly_stats = []
            
            # æŒ‰å‘¨åˆ†ç»„è®°å½•
            week_groups = {}
            for record in records:
                timestamp = record.get('timestamp')
                if timestamp:
                    if isinstance(timestamp, str):
                        date_obj = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).date()
                    else:
                        date_obj = timestamp.date() if hasattr(timestamp, 'date') else timestamp
                    
                    # è·å–å‘¨å¼€å§‹æ—¥æœŸï¼ˆå‘¨ä¸€ï¼‰
                    week_start = date_obj - timedelta(days=date_obj.weekday())
                    week_key = week_start.strftime('%Y-%m-%d')
                    
                    if week_key not in week_groups:
                        week_groups[week_key] = []
                    week_groups[week_key].append(record)
            
            # è®¡ç®—æ¯å‘¨èšåˆæ•°æ®
            for week_start_str, week_records in week_groups.items():
                weekly_data = self._aggregate_weekly_data(device_sn, week_start_str, week_records)
                if weekly_data:
                    weekly_stats.append(weekly_data)
            
            return weekly_stats
            
        except Exception as e:
            logger.error(f"âŒ è®¾å¤‡{device_sn}æ¯å‘¨ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            return []
    
    def _aggregate_daily_data(self, device_sn: str, date_str: str, records: List[dict]) -> Optional[dict]:
        """èšåˆæ¯æ—¥æ•°æ®"""
        try:
            if not records:
                return None
            
            # æå–èšåˆå­—æ®µ
            sleep_data_list = []
            exercise_data_list = []
            workout_data_list = []
            
            # åŸºç¡€å¥åº·æŒ‡æ ‡èšåˆ
            heart_rates = []
            blood_oxygens = []
            temperatures = []
            steps = []
            calories = []
            distances = []
            
            for record in records:
                # æå–å¤åˆæ•°æ®
                if record.get('sleepData'):
                    sleep_data_list.append(record['sleepData'])
                if record.get('exerciseDailyData'):
                    exercise_data_list.append(record['exerciseDailyData'])
                if record.get('workoutData'):
                    workout_data_list.append(record['workoutData'])
                
                # æå–åŸºç¡€æŒ‡æ ‡
                if record.get('heart_rate'):
                    heart_rates.append(float(record['heart_rate']))
                if record.get('blood_oxygen'):
                    blood_oxygens.append(float(record['blood_oxygen']))
                if record.get('temperature'):
                    temperatures.append(float(record['temperature']))
                if record.get('step'):
                    steps.append(int(record['step']))
                if record.get('calorie'):
                    calories.append(float(record['calorie']))
                if record.get('distance'):
                    distances.append(float(record['distance']))
            
            # æ„å»ºæ¯æ—¥èšåˆæ•°æ®
            daily_data = {
                'device_sn': device_sn,
                'date': date_str,
                'record_count': len(records),
                'user_id': records[0].get('user_id'),
                'org_id': records[0].get('org_id')
            }
            
            # å¤åˆæ•°æ®å¤„ç†
            if sleep_data_list:
                daily_data['sleep_data'] = json.dumps(sleep_data_list[-1])  # ä½¿ç”¨æœ€æ–°çš„ç¡çœ æ•°æ®
            if exercise_data_list:
                daily_data['exercise_daily_data'] = json.dumps(exercise_data_list[-1])
            if workout_data_list:
                daily_data['workout_data'] = json.dumps(workout_data_list[-1])
            
            # åŸºç¡€æŒ‡æ ‡ç»Ÿè®¡
            if heart_rates:
                daily_data['avg_heart_rate'] = round(sum(heart_rates) / len(heart_rates), 1)
                daily_data['max_heart_rate'] = max(heart_rates)
                daily_data['min_heart_rate'] = min(heart_rates)
            
            if blood_oxygens:
                daily_data['avg_blood_oxygen'] = round(sum(blood_oxygens) / len(blood_oxygens), 1)
                daily_data['min_blood_oxygen'] = min(blood_oxygens)
            
            if temperatures:
                daily_data['avg_temperature'] = round(sum(temperatures) / len(temperatures), 1)
                daily_data['max_temperature'] = max(temperatures)
            
            if steps:
                daily_data['total_steps'] = sum(steps)
                daily_data['max_steps'] = max(steps)
            
            if calories:
                daily_data['total_calories'] = round(sum(calories), 1)
            
            if distances:
                daily_data['total_distance'] = round(sum(distances), 2)
            
            return daily_data
            
        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥æ•°æ®èšåˆå¤±è´¥: {e}")
            return None
    
    def _aggregate_weekly_data(self, device_sn: str, week_start_str: str, records: List[dict]) -> Optional[dict]:
        """èšåˆæ¯å‘¨æ•°æ®"""
        try:
            if not records:
                return None
            
            # æå–è¿åŠ¨æ•°æ®
            exercise_week_data_list = []
            
            # åŸºç¡€æŒ‡æ ‡èšåˆ
            total_steps = 0
            total_calories = 0
            total_distance = 0
            heart_rate_sum = 0
            heart_rate_count = 0
            
            for record in records:
                if record.get('exerciseWeekData'):
                    exercise_week_data_list.append(record['exerciseWeekData'])
                
                # ç´¯è®¡æŒ‡æ ‡
                if record.get('step'):
                    total_steps += int(record['step'])
                if record.get('calorie'):
                    total_calories += float(record['calorie'])
                if record.get('distance'):
                    total_distance += float(record['distance'])
                if record.get('heart_rate'):
                    heart_rate_sum += float(record['heart_rate'])
                    heart_rate_count += 1
            
            # æ„å»ºæ¯å‘¨èšåˆæ•°æ®
            weekly_data = {
                'device_sn': device_sn,
                'week_start': week_start_str,
                'record_count': len(records),
                'user_id': records[0].get('user_id'),
                'org_id': records[0].get('org_id'),
                'total_steps': total_steps,
                'total_calories': round(total_calories, 1),
                'total_distance': round(total_distance, 2)
            }
            
            # è¿åŠ¨æ•°æ®
            if exercise_week_data_list:
                weekly_data['exercise_week_data'] = json.dumps(exercise_week_data_list[-1])
            
            # å¹³å‡å¿ƒç‡
            if heart_rate_count > 0:
                weekly_data['avg_heart_rate'] = round(heart_rate_sum / heart_rate_count, 1)
            
            return weekly_data
            
        except Exception as e:
            logger.error(f"âŒ æ¯å‘¨æ•°æ®èšåˆå¤±è´¥: {e}")
            return None
    
    def _save_daily_data_sync(self, device_sn: str, daily_stats: List[dict]):
        """åŒæ­¥ä¿å­˜æ¯æ—¥æ•°æ®"""
        try:
            for daily_data in daily_stats:
                # è¿™é‡Œåº”è¯¥è°ƒç”¨æ•°æ®åº“æ“ä½œ
                # å¯ä»¥é›†æˆç°æœ‰çš„ä¿å­˜é€»è¾‘ï¼Œæˆ–ç›´æ¥æ“ä½œæ•°æ®åº“
                logger.info(f"ğŸ“… ä¿å­˜æ¯æ—¥æ•°æ®: è®¾å¤‡{device_sn}, æ—¥æœŸ{daily_data.get('date')}, "
                           f"è®°å½•æ•°: {daily_data.get('record_count', 0)}")
                
                # æ¨¡æ‹Ÿæ•°æ®åº“ä¿å­˜æ“ä½œ
                # å®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨ UserHealthDataDaily.create() æˆ–ç±»ä¼¼æ–¹æ³•
            
            self.stats['daily_processed'] += len(daily_stats)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¯æ—¥æ•°æ®å¤±è´¥: {e}")
    
    def _save_weekly_data_sync(self, device_sn: str, weekly_stats: List[dict]):
        """åŒæ­¥ä¿å­˜æ¯å‘¨æ•°æ®"""
        try:
            for weekly_data in weekly_stats:
                # è¿™é‡Œåº”è¯¥è°ƒç”¨æ•°æ®åº“æ“ä½œ
                logger.info(f"ğŸ“Š ä¿å­˜æ¯å‘¨æ•°æ®: è®¾å¤‡{device_sn}, å‘¨å¼€å§‹{weekly_data.get('week_start')}, "
                           f"è®°å½•æ•°: {weekly_data.get('record_count', 0)}")
                
                # æ¨¡æ‹Ÿæ•°æ®åº“ä¿å­˜æ“ä½œ
                # å®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨ UserHealthDataWeekly.create() æˆ–ç±»ä¼¼æ–¹æ³•
            
            self.stats['weekly_processed'] += len(weekly_stats)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¯å‘¨æ•°æ®å¤±è´¥: {e}")
    
    def _update_avg_daily_time(self, new_time: float):
        """æ›´æ–°å¹³å‡æ¯æ—¥å¤„ç†æ—¶é—´"""
        current_avg = self.stats['avg_daily_time']
        self.stats['avg_daily_time'] = (current_avg * 0.9) + (new_time * 0.1)
    
    def _update_avg_weekly_time(self, new_time: float):
        """æ›´æ–°å¹³å‡æ¯å‘¨å¤„ç†æ—¶é—´"""
        current_avg = self.stats['avg_weekly_time']
        self.stats['avg_weekly_time'] = (current_avg * 0.9) + (new_time * 0.1)
    
    def get_aggregation_stats(self) -> dict:
        """è·å–èšåˆå¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'config': {
                'max_workers': self.aggregation_executor._max_workers,
                'pool_active': not self.aggregation_executor._shutdown
            },
            'performance': {
                'daily_processed': self.stats['daily_processed'],
                'weekly_processed': self.stats['weekly_processed'],
                'device_groups_processed': self.stats['device_groups_processed'],
                'aggregation_errors': self.stats['aggregation_errors'],
                'avg_daily_time': round(self.stats['avg_daily_time'], 4),
                'avg_weekly_time': round(self.stats['avg_weekly_time'], 4),
                'success_rate': round((self.stats['daily_processed'] + self.stats['weekly_processed']) / 
                                    max(1, self.stats['daily_processed'] + self.stats['weekly_processed'] + 
                                    self.stats['aggregation_errors']) * 100, 2)
            }
        }
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'aggregation_executor') and not self.aggregation_executor._shutdown:
            self.aggregation_executor.shutdown(wait=True)

# å…¨å±€èšåˆå¤„ç†å™¨å®ä¾‹
aggregation_processor = AsyncAggregationProcessor()

# ================================ åŠ¨æ€èµ„æºç®¡ç†ç³»ç»Ÿ ================================

class DynamicResourceManager:
    """åŠ¨æ€èµ„æºç®¡ç†å™¨ - å®æ—¶ç›‘æ§ç³»ç»Ÿèµ„æºå¹¶åŠ¨æ€è°ƒæ•´å¤„ç†å™¨é…ç½®"""
    
    def __init__(self, monitoring_interval: float = 30.0):
        self.monitoring_interval = monitoring_interval
        self.running = False
        self.monitor_task = None
        
        self.stats = {
            'adjustments_made': 0,
            'cpu_readings': [],
            'memory_readings': [],
            'queue_depth_readings': [],
            'last_adjustment_time': 0,
            'performance_trend': 'stable'  # 'improving', 'degrading', 'stable'
        }
        
        # è°ƒæ•´é˜ˆå€¼
        self.cpu_high_threshold = 85.0
        self.cpu_low_threshold = 30.0
        self.memory_high_threshold = 80.0
        self.queue_depth_threshold = 1000
        self.min_adjustment_interval = 60.0  # æœ€å°è°ƒæ•´é—´éš”60ç§’
        
        logger.info(f"ğŸ“Š DynamicResourceManager åˆå§‹åŒ– - ç›‘æ§é—´éš”: {monitoring_interval}ç§’")
    
    async def start_monitoring(self):
        """å¯åŠ¨èµ„æºç›‘æ§"""
        if self.running:
            return
            
        self.running = True
        self.monitor_task = asyncio.create_task(self._monitoring_loop())
        logger.info("ğŸ” åŠ¨æ€èµ„æºç›‘æ§å·²å¯åŠ¨")
    
    async def stop_monitoring(self):
        """åœæ­¢èµ„æºç›‘æ§"""
        if not self.running:
            return
            
        self.running = False
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                await self.monitor_task
            except asyncio.CancelledError:
                pass
        
        logger.info("ğŸ›‘ åŠ¨æ€èµ„æºç›‘æ§å·²åœæ­¢")
    
    async def _monitoring_loop(self):
        """èµ„æºç›‘æ§å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                cpu_percent = psutil.cpu_percent(interval=1.0)
                memory_info = psutil.virtual_memory()
                memory_percent = memory_info.percent
                
                # æ”¶é›†é˜Ÿåˆ—æ·±åº¦ä¿¡æ¯
                queue_depths = self._get_queue_depths()
                
                # è®°å½•æŒ‡æ ‡
                self._record_metrics(cpu_percent, memory_percent, queue_depths)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´
                adjustment_needed = self._analyze_adjustment_need(
                    cpu_percent, memory_percent, queue_depths
                )
                
                if adjustment_needed:
                    await self._perform_dynamic_adjustment(
                        cpu_percent, memory_percent, queue_depths
                    )
                
                # ç­‰å¾…ä¸‹æ¬¡ç›‘æ§
                await asyncio.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"âŒ èµ„æºç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(self.monitoring_interval)
    
    def _get_queue_depths(self) -> dict:
        """è·å–æ‰€æœ‰é˜Ÿåˆ—æ·±åº¦"""
        queue_depths = {
            'total_depth': 0,
            'parsing_queue': 0,
            'alert_queue': 0,
            'aggregation_queue': 0,
            'notification_queue': 0
        }
        
        try:
            # è·å–å¼‚æ­¥å¤„ç†å™¨é˜Ÿåˆ—æ·±åº¦
            if async_processor and async_processor.running:
                queue_depths['parsing_queue'] = async_processor.parsing_queue.qsize()
                queue_depths['alert_queue'] = async_processor.alert_queue.qsize()
                queue_depths['aggregation_queue'] = async_processor.aggregation_queue.qsize()
                queue_depths['notification_queue'] = async_processor.notification_queue.qsize()
                
                queue_depths['total_depth'] = (
                    queue_depths['parsing_queue'] + 
                    queue_depths['alert_queue'] + 
                    queue_depths['aggregation_queue'] + 
                    queue_depths['notification_queue']
                )
            
            # è·å–ä¼ ç»Ÿä¼˜åŒ–å™¨é˜Ÿåˆ—æ·±åº¦
            if hasattr(optimizer, 'batch_queue'):
                queue_depths['batch_queue'] = optimizer.batch_queue.qsize()
                queue_depths['total_depth'] += queue_depths['batch_queue']
        
        except Exception as e:
            logger.error(f"âŒ è·å–é˜Ÿåˆ—æ·±åº¦å¤±è´¥: {e}")
        
        return queue_depths
    
    def _record_metrics(self, cpu_percent: float, memory_percent: float, queue_depths: dict):
        """è®°å½•ç³»ç»ŸæŒ‡æ ‡"""
        current_time = time.time()
        
        # ä¿ç•™æœ€è¿‘100ä¸ªè¯»æ•°
        max_readings = 100
        
        self.stats['cpu_readings'].append({
            'value': cpu_percent,
            'timestamp': current_time
        })
        self.stats['memory_readings'].append({
            'value': memory_percent,
            'timestamp': current_time
        })
        self.stats['queue_depth_readings'].append({
            'value': queue_depths['total_depth'],
            'timestamp': current_time
        })
        
        # ä¿æŒè®°å½•æ•°é‡é™åˆ¶
        for readings_key in ['cpu_readings', 'memory_readings', 'queue_depth_readings']:
            if len(self.stats[readings_key]) > max_readings:
                self.stats[readings_key] = self.stats[readings_key][-max_readings:]
    
    def _analyze_adjustment_need(self, cpu_percent: float, memory_percent: float, 
                               queue_depths: dict) -> bool:
        """åˆ†ææ˜¯å¦éœ€è¦è¿›è¡ŒåŠ¨æ€è°ƒæ•´"""
        current_time = time.time()
        
        # æ£€æŸ¥è°ƒæ•´é—´éš”
        if (current_time - self.stats['last_adjustment_time']) < self.min_adjustment_interval:
            return False
        
        # CPUå‹åŠ›æ£€æŸ¥
        cpu_pressure = cpu_percent > self.cpu_high_threshold
        cpu_idle = cpu_percent < self.cpu_low_threshold
        
        # å†…å­˜å‹åŠ›æ£€æŸ¥
        memory_pressure = memory_percent > self.memory_high_threshold
        
        # é˜Ÿåˆ—ç§¯å‹æ£€æŸ¥
        queue_backlog = queue_depths['total_depth'] > self.queue_depth_threshold
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        performance_degrading = self._analyze_performance_trend()
        
        # éœ€è¦è°ƒæ•´çš„æ¡ä»¶
        adjustment_conditions = [
            cpu_pressure and queue_backlog,  # CPUé«˜ä¸”é˜Ÿåˆ—ç§¯å‹
            memory_pressure,  # å†…å­˜å‹åŠ›è¿‡å¤§
            cpu_idle and not queue_backlog,  # CPUç©ºé—²ä¸”æ²¡æœ‰ç§¯å‹
            performance_degrading and queue_backlog,  # æ€§èƒ½ä¸‹é™ä¸”æœ‰ç§¯å‹
        ]
        
        return any(adjustment_conditions)
    
    def _analyze_performance_trend(self) -> bool:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.stats['cpu_readings']) < 10:
            return False
        
        # åˆ†ææœ€è¿‘çš„CPUå’Œé˜Ÿåˆ—è¶‹åŠ¿
        recent_cpu = [r['value'] for r in self.stats['cpu_readings'][-10:]]
        recent_queues = [r['value'] for r in self.stats['queue_depth_readings'][-10:]]
        
        # ç®€å•è¶‹åŠ¿åˆ†æï¼šæ£€æŸ¥æ˜¯å¦å‘ˆä¸Šå‡è¶‹åŠ¿
        cpu_trend_up = recent_cpu[-1] > recent_cpu[0] + 10  # CPUä¸Šå‡è¶…è¿‡10%
        queue_trend_up = recent_queues[-1] > recent_queues[0] + 100  # é˜Ÿåˆ—æ·±åº¦ä¸Šå‡è¶…è¿‡100
        
        if cpu_trend_up and queue_trend_up:
            self.stats['performance_trend'] = 'degrading'
            return True
        elif not cpu_trend_up and not queue_trend_up:
            self.stats['performance_trend'] = 'improving'
        else:
            self.stats['performance_trend'] = 'stable'
        
        return False
    
    async def _perform_dynamic_adjustment(self, cpu_percent: float, memory_percent: float, 
                                        queue_depths: dict):
        """æ‰§è¡ŒåŠ¨æ€è°ƒæ•´"""
        try:
            adjustment_actions = []
            
            # ğŸš€ æ ¹æ®ç³»ç»ŸçŠ¶æ€å†³å®šè°ƒæ•´ç­–ç•¥
            if cpu_percent > self.cpu_high_threshold or memory_percent > self.memory_high_threshold:
                # ç³»ç»Ÿèµ„æºç´§å¼ ï¼Œå‡å°‘å·¥ä½œçº¿ç¨‹
                adjustment_actions.extend(await self._scale_down_resources(
                    cpu_percent, memory_percent, queue_depths
                ))
            
            elif cpu_percent < self.cpu_low_threshold and queue_depths['total_depth'] > self.queue_depth_threshold:
                # CPUç©ºé—²ä½†æœ‰ç§¯å‹ï¼Œå¢åŠ å·¥ä½œçº¿ç¨‹
                adjustment_actions.extend(await self._scale_up_resources(
                    cpu_percent, memory_percent, queue_depths
                ))
            
            elif queue_depths['total_depth'] > self.queue_depth_threshold * 2:
                # ä¸¥é‡ç§¯å‹ï¼Œä¼˜åŒ–é˜Ÿåˆ—å¤„ç†
                adjustment_actions.extend(await self._optimize_queue_processing(queue_depths))
            
            # è®°å½•è°ƒæ•´
            if adjustment_actions:
                self.stats['adjustments_made'] += 1
                self.stats['last_adjustment_time'] = time.time()
                
                logger.info(f"ğŸ¯ åŠ¨æ€èµ„æºè°ƒæ•´å®Œæˆ: {', '.join(adjustment_actions)}")
                logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€ - CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_percent:.1f}%, "
                           f"é˜Ÿåˆ—æ·±åº¦: {queue_depths['total_depth']}")
        
        except Exception as e:
            logger.error(f"âŒ åŠ¨æ€è°ƒæ•´æ‰§è¡Œå¤±è´¥: {e}")
    
    async def _scale_down_resources(self, cpu_percent: float, memory_percent: float, 
                                  queue_depths: dict) -> List[str]:
        """ç¼©å‡èµ„æºä½¿ç”¨"""
        actions = []
        
        try:
            # è°ƒæ•´ä¼ ç»Ÿä¼˜åŒ–å™¨æ‰¹æ¬¡å¤§å°
            if hasattr(optimizer, 'batch_size') and optimizer.batch_size > 50:
                old_batch_size = optimizer.batch_size
                optimizer.batch_size = max(50, int(optimizer.batch_size * 0.8))
                actions.append(f"æ‰¹æ¬¡å¤§å°: {old_batch_size} â†’ {optimizer.batch_size}")
            
            # è°ƒæ•´å¼‚æ­¥å¤„ç†å™¨é…ç½®ï¼ˆå¦‚æœæ”¯æŒï¼‰
            if async_processor and async_processor.running:
                # è¿™é‡Œå¯ä»¥æ·»åŠ åŠ¨æ€è°ƒæ•´å¼‚æ­¥å·¥ä½œè€…æ•°é‡çš„é€»è¾‘
                # ç”±äºasyncio.create_taskçš„é™åˆ¶ï¼Œæˆ‘ä»¬è®°å½•å»ºè®®è°ƒæ•´
                actions.append("å»ºè®®é‡å¯æ—¶å‡å°‘å¼‚æ­¥å·¥ä½œè€…æ•°é‡")
            
        except Exception as e:
            logger.error(f"âŒ ç¼©å‡èµ„æºå¤±è´¥: {e}")
        
        return actions
    
    async def _scale_up_resources(self, cpu_percent: float, memory_percent: float, 
                                queue_depths: dict) -> List[str]:
        """æ‰©å±•èµ„æºä½¿ç”¨"""
        actions = []
        
        try:
            # å¢åŠ ä¼ ç»Ÿä¼˜åŒ–å™¨æ‰¹æ¬¡å¤§å°
            if hasattr(optimizer, 'batch_size') and optimizer.batch_size < 500:
                old_batch_size = optimizer.batch_size
                optimizer.batch_size = min(500, int(optimizer.batch_size * 1.2))
                actions.append(f"æ‰¹æ¬¡å¤§å°: {old_batch_size} â†’ {optimizer.batch_size}")
            
            # è°ƒæ•´çº¿ç¨‹æ± ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if hasattr(optimizer, 'executor') and hasattr(optimizer.executor, '_max_workers'):
                current_workers = optimizer.executor._max_workers
                if current_workers < psutil.cpu_count() * 2:
                    actions.append(f"å»ºè®®å¢åŠ å·¥ä½œçº¿ç¨‹: {current_workers} â†’ {current_workers + 2}")
        
        except Exception as e:
            logger.error(f"âŒ æ‰©å±•èµ„æºå¤±è´¥: {e}")
        
        return actions
    
    async def _optimize_queue_processing(self, queue_depths: dict) -> List[str]:
        """ä¼˜åŒ–é˜Ÿåˆ—å¤„ç†"""
        actions = []
        
        try:
            # ä¸´æ—¶é™ä½æ‰¹æ¬¡è¶…æ—¶æ—¶é—´ä»¥åŠ å¿«å¤„ç†
            if hasattr(optimizer, 'batch_timeout') and optimizer.batch_timeout > 1.0:
                old_timeout = optimizer.batch_timeout
                optimizer.batch_timeout = max(1.0, optimizer.batch_timeout * 0.8)
                actions.append(f"æ‰¹æ¬¡è¶…æ—¶: {old_timeout}s â†’ {optimizer.batch_timeout}s")
            
            # é˜Ÿåˆ—ç§¯å‹ä¸¥é‡æ—¶çš„ç´§æ€¥å¤„ç†
            if queue_depths['total_depth'] > self.queue_depth_threshold * 3:
                actions.append("å¯åŠ¨ç´§æ€¥é˜Ÿåˆ—å¤„ç†æ¨¡å¼")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šç´§æ€¥å¤„ç†é€»è¾‘
        
        except Exception as e:
            logger.error(f"âŒ é˜Ÿåˆ—ä¼˜åŒ–å¤±è´¥: {e}")
        
        return actions
    
    def get_resource_stats(self) -> dict:
        """è·å–èµ„æºç®¡ç†ç»Ÿè®¡ä¿¡æ¯"""
        current_metrics = {}
        
        try:
            # å½“å‰ç³»ç»ŸçŠ¶æ€
            current_metrics = {
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'queue_depths': self._get_queue_depths()
            }
        except Exception:
            pass
        
        return {
            'config': {
                'monitoring_interval': self.monitoring_interval,
                'cpu_high_threshold': self.cpu_high_threshold,
                'memory_high_threshold': self.memory_high_threshold,
                'queue_depth_threshold': self.queue_depth_threshold
            },
            'runtime': {
                'running': self.running,
                'adjustments_made': self.stats['adjustments_made'],
                'performance_trend': self.stats['performance_trend']
            },
            'current_metrics': current_metrics,
            'historical_data': {
                'cpu_readings_count': len(self.stats['cpu_readings']),
                'memory_readings_count': len(self.stats['memory_readings']),
                'queue_depth_readings_count': len(self.stats['queue_depth_readings'])
            }
        }

# å…¨å±€èµ„æºç®¡ç†å™¨å®ä¾‹
resource_manager = DynamicResourceManager()

async def initialize_async_system():
    """åˆå§‹åŒ–å¼‚æ­¥å¤„ç†ç³»ç»Ÿ"""
    global async_processor, resource_manager
    
    try:
        # å¯åŠ¨å¼‚æ­¥å¤„ç†å™¨
        if async_processor is None:
            async_processor = await get_async_processor()
        
        # å¯åŠ¨åŠ¨æ€èµ„æºç®¡ç†
        await resource_manager.start_monitoring()
        
        logger.info("ğŸš€ å¼‚æ­¥å¥åº·æ•°æ®å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

async def shutdown_async_system():
    """å…³é—­å¼‚æ­¥å¤„ç†ç³»ç»Ÿ"""
    global async_processor, resource_manager
    
    try:
        # åœæ­¢èµ„æºç®¡ç†
        await resource_manager.stop_monitoring()
        
        # åœæ­¢å¼‚æ­¥å¤„ç†å™¨
        if async_processor:
            await async_processor.stop()
        
        logger.info("ğŸ›‘ å¼‚æ­¥å¥åº·æ•°æ®å¤„ç†ç³»ç»Ÿå·²å…³é—­")
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥ç³»ç»Ÿå…³é—­å¤±è´¥: {e}")

def parse_sleep_data_optimized(sleep_data_json) -> Optional[float]:
    """ä¼˜åŒ–çš„ç¡çœ æ•°æ®è§£æå‡½æ•° - åŒæ­¥ç‰ˆæœ¬å…¼å®¹æ¥å£"""
    try:
        # å¦‚æœåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œä½¿ç”¨å¼‚æ­¥è§£æ
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œåˆ›å»ºä»»åŠ¡ä½†ä¸ç­‰å¾…
            task = asyncio.create_task(sleep_parser.parse_sleep_data_async(sleep_data_json))
            # æ³¨ï¼šè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´ä»¥é€‚åº”ä¸åŒçš„ä½¿ç”¨åœºæ™¯
            return None  # å¼‚æ­¥å¤„ç†ï¼Œç«‹å³è¿”å›
        else:
            # åœ¨åŒæ­¥ç¯å¢ƒä¸­ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å‡½æ•°
            return parse_sleep_data(sleep_data_json)
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
        return parse_sleep_data(sleep_data_json)

# å‘åå…¼å®¹ï¼šå¢å¼ºåŸå§‹parse_sleep_dataå‡½æ•°
def parse_sleep_data_enhanced(sleep_data_json) -> Optional[float]:
    """å¢å¼ºç‰ˆç¡çœ æ•°æ®è§£æ - æå‡åŸæœ‰å‡½æ•°æ€§èƒ½"""
    if not sleep_data_json or sleep_data_json in ['null', None]:
        return None
    
    try:
        # ç¼“å­˜æœºåˆ¶ï¼šå¯¹äºç›¸åŒçš„è¾“å…¥è¿”å›ç¼“å­˜ç»“æœ
        data_hash = hash(str(sleep_data_json)) if isinstance(sleep_data_json, (str, dict)) else None
        
        if isinstance(sleep_data_json, str):
            # é¢„å¤„ç†JSONå­—ç¬¦ä¸²ï¼Œä¿®å¤å¸¸è§æ ¼å¼é”™è¯¯
            sleep_data_json = sleep_data_json.strip()
            if sleep_data_json.startswith('"') and sleep_data_json.endswith('"'):
                sleep_data_json = sleep_data_json[1:-1]  # ç§»é™¤å¤–å±‚å¼•å·
            
            # ä¿®å¤JSONæ ¼å¼é”™è¯¯
            sleep_data_json = sleep_data_json.replace('"0"data"', '"0","data"')
            sleep_data_json = sleep_data_json.replace('""', '"')  # ä¿®å¤åŒå¼•å·é—®é¢˜
            
            try:
                sleep_data = json.loads(sleep_data_json)
            except json.JSONDecodeError as e:
                logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
                # å°è¯•æ›´å¤šä¿®å¤æ–¹æ³•
                sleep_data_json = sleep_data_json.replace("'", '"')  # å•å¼•å·è½¬åŒå¼•å·
                sleep_data = json.loads(sleep_data_json)
        elif isinstance(sleep_data_json, dict):
            sleep_data = sleep_data_json
        else:
            return None
            
        # å¿«é€ŸéªŒè¯
        if not isinstance(sleep_data, dict):
            return None
            
        code = sleep_data.get('code')
        if code == -1 or str(code) == '-1':
            return None
            
        data_list = sleep_data.get('data', [])
        if not isinstance(data_list, list) or len(data_list) == 0:
            return None
            
        # ä¼˜åŒ–çš„æ—¶é—´è®¡ç®—
        total_sleep_seconds = 0
        
        for sleep_period in data_list:
            if not isinstance(sleep_period, dict):
                continue
                
            start_time = sleep_period.get('startTimeStamp')
            end_time = sleep_period.get('endTimeStamp')
            
            if start_time is None or end_time is None:
                continue
                
            try:
                # ä¼˜åŒ–çš„æ—¶é—´æˆ³å¤„ç†
                start_seconds = int(start_time)
                end_seconds = int(end_time)
                
                # æ¯«ç§’è½¬ç§’ï¼ˆä¸€æ¬¡æ€§åˆ¤æ–­ï¼‰
                if start_seconds > 9999999999:
                    start_seconds //= 1000
                if end_seconds > 9999999999:
                    end_seconds //= 1000
                
                # è®¡ç®—æ—¶é—´å·®
                if end_seconds > start_seconds:
                    total_sleep_seconds += (end_seconds - start_seconds)
                    
            except (ValueError, TypeError):
                continue
                
        # è½¬æ¢ä¸ºå°æ—¶
        if total_sleep_seconds > 0:
            total_sleep_hours = round(total_sleep_seconds / 3600, 2)
            return total_sleep_hours
        else:
            return None
            
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆç¡çœ æ•°æ®è§£æå¤±è´¥: {e}, æ•°æ®: {sleep_data_json}")
        return None

def get_optimizer_stats():#è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡
    """è·å–ä¼˜åŒ–å™¨è¿è¡Œç»Ÿè®¡ä¿¡æ¯"""
    return jsonify(optimizer.get_stats())

def get_async_system_stats():
    """è·å–å®Œæ•´å¼‚æ­¥ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = {
            "timestamp": datetime.now().isoformat(),
            "system_overview": {
                "version": "AsyncHealthDataProcessor v2.0",
                "status": "è¿è¡Œä¸­" if async_processor and async_processor.running else "æœªè¿è¡Œ",
                "cpu_cores": psutil.cpu_count(logical=True),
                "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 1)
            },
            "processors": {}
        }
        
        # ä¼ ç»Ÿä¼˜åŒ–å™¨ç»Ÿè®¡
        stats["processors"]["health_optimizer"] = {
            "name": "HealthDataOptimizer",
            "version": "4.0",
            **optimizer.get_stats()
        }
        
        # å¼‚æ­¥å¤„ç†å™¨ç»Ÿè®¡
        if async_processor:
            stats["processors"]["async_processor"] = {
                "name": "AsyncHealthDataProcessor", 
                "version": "2.0",
                **async_processor.get_async_stats()
            }
        
        # ç¡çœ è§£æå™¨ç»Ÿè®¡
        if sleep_parser:
            stats["processors"]["sleep_parser"] = {
                "name": "AsyncSleepDataParser",
                "version": "1.0",
                **sleep_parser.get_parser_stats()
            }
        
        # å‘Šè­¦å¤„ç†å™¨ç»Ÿè®¡
        if alert_processor:
            stats["processors"]["alert_processor"] = {
                "name": "AsyncAlertProcessor",
                "version": "1.0", 
                **alert_processor.get_alert_stats()
            }
        
        # èšåˆå¤„ç†å™¨ç»Ÿè®¡
        if aggregation_processor:
            stats["processors"]["aggregation_processor"] = {
                "name": "AsyncAggregationProcessor",
                "version": "1.0",
                **aggregation_processor.get_aggregation_stats()
            }
        
        # èµ„æºç®¡ç†å™¨ç»Ÿè®¡
        if resource_manager:
            stats["processors"]["resource_manager"] = {
                "name": "DynamicResourceManager",
                "version": "1.0",
                **resource_manager.get_resource_stats()
            }
        
        # æ€§èƒ½æ€»ç»“
        total_processed = 0
        total_errors = 0
        
        for processor_name, processor_stats in stats["processors"].items():
            performance = processor_stats.get("performance", {})
            if "processed" in performance:
                total_processed += performance["processed"]
            elif "alerts_processed" in performance:
                total_processed += performance["alerts_processed"] 
            elif "daily_processed" in performance:
                total_processed += performance["daily_processed"]
            elif "parsed_total" in performance:
                total_processed += performance["parsed_total"]
            
            # ç´¯è®¡é”™è¯¯
            if "errors" in performance:
                total_errors += performance["errors"]
            elif "processing_errors" in performance:
                total_errors += performance["processing_errors"]
            elif "parse_errors" in performance:
                total_errors += performance["parse_errors"]
        
        stats["performance_summary"] = {
            "total_processed": total_processed,
            "total_errors": total_errors,
            "overall_success_rate": round((total_processed - total_errors) / max(1, total_processed) * 100, 2),
            "system_efficiency": "é«˜æ•ˆ" if total_errors < total_processed * 0.01 else "æ­£å¸¸" if total_errors < total_processed * 0.05 else "éœ€ä¼˜åŒ–"
        }
        
        return jsonify(stats)
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¼‚æ­¥ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({"error": str(e), "timestamp": datetime.now().isoformat()})

def save_health_data_fast(*args,**kwargs):#å¿«é€Ÿä¿å­˜(å…¼å®¹åŸæ¥å£)
    """å…¼å®¹åŸæœ‰æ¥å£çš„å¿«é€Ÿä¿å­˜æ–¹æ³•"""
    try:
        #æ„å»ºæ•°æ®å­—å…¸
        data={
            'heartRate':args[0] if len(args)>0 else kwargs.get('heartRate'),
            'pressureHigh':args[1] if len(args)>1 else kwargs.get('pressureHigh'),
            'pressureLow':args[2] if len(args)>2 else kwargs.get('pressureLow'),
            'bloodOxygen':args[3] if len(args)>3 else kwargs.get('bloodOxygen'),
            'temperature':args[4] if len(args)>4 else kwargs.get('temperature'),
            'stress':args[5] if len(args)>5 else kwargs.get('stress'),
            'step':args[6] if len(args)>6 else kwargs.get('step'),
            'timestamp':args[7] if len(args)>7 else kwargs.get('timestamp',datetime.now()),
            'deviceSn':args[8] if len(args)>8 else kwargs.get('deviceSn'),
            'distance':args[9] if len(args)>9 else kwargs.get('distance'),
            'calorie':args[10] if len(args)>10 else kwargs.get('calorie'),
            'latitude':args[11] if len(args)>11 else kwargs.get('latitude'),
            'longitude':args[12] if len(args)>12 else kwargs.get('longitude'),
            'altitude':args[13] if len(args)>13 else kwargs.get('altitude'),
            'sleepData':args[14] if len(args)>14 else kwargs.get('sleepData'),
            'exerciseDailyData':args[15] if len(args)>15 else kwargs.get('exerciseDailyData'),
            'exerciseWeekData':args[16] if len(args)>16 else kwargs.get('exerciseWeekData'),
            'scientificSleepData':args[17] if len(args)>17 else kwargs.get('scientificSleepData'),
            'workoutData':args[18] if len(args)>18 else kwargs.get('workoutData'),
            'uploadMethod':args[19] if len(args)>19 else kwargs.get('uploadMethod','wifi')
        }
        
        device_sn=data.get('deviceSn')
        if not device_sn:
            logger.error('å¿«é€Ÿä¿å­˜ç¼ºå°‘è®¾å¤‡SN')
            return None
            
        #ä½¿ç”¨ä¼˜åŒ–å™¨å¤„ç†
        result=optimizer.add_data(data,device_sn)
        if result.get('success'):
            return True
        else:
            logger.warning(f'å¿«é€Ÿä¿å­˜å¤±è´¥: {result.get("message","æœªçŸ¥é”™è¯¯")}')
            return None
        
    except Exception as e:
        logger.error(f'å¿«é€Ÿä¿å­˜å¤±è´¥: {e}')
        return None 
def parse_sleep_data(sleep_data_json):
    """
    è§£æsleepData JSONï¼Œè®¡ç®—ç¡çœ æ—¶é•¿(å°æ—¶)
    æ”¯æŒçš„æ ¼å¼ï¼š
    1. '{"code":0,"data":[{"endTimeStamp":1747440420000,"startTimeStamp":1747418280000,"type":2}],"name":"sleep","type":"history"}'
    2. null, "null", '{"code":-1,"data":[],"name":"sleep","type":"history"}'
    3. '{"code":"0","data":[],"name":"sleep","type":"history"}'
    
    è¿”å›: ç¡çœ æ—¶é•¿(å°æ—¶)ï¼Œå‡ºé”™æ—¶è¿”å›None
    """
    if not sleep_data_json or sleep_data_json in ['null', None]:
        return None
    
    try:
        if isinstance(sleep_data_json, str):
            # å¤„ç†JSONå­—ç¬¦ä¸²æ ¼å¼é”™è¯¯çš„æƒ…å†µï¼Œå¦‚'{"code":"0"data":[]}'
            sleep_data_json = sleep_data_json.replace('"0"data"', '"0","data"')
            sleep_data = json.loads(sleep_data_json)
        elif isinstance(sleep_data_json, dict):
            sleep_data = sleep_data_json
        else:
            return None
            
        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if not isinstance(sleep_data, dict):
            return None
            
        code = sleep_data.get('code')
        if code == -1 or code == '-1' or str(code) == '-1':
            return None
            
        data_list = sleep_data.get('data', [])
        if not isinstance(data_list, list) or len(data_list) == 0:
            return None
            
        total_sleep_seconds = 0
        
        # éå†æ‰€æœ‰ç¡çœ æ—¶é—´æ®µï¼Œä¸ç®¡typeï¼ŒæŒ‰ç…§endTimeStamp-startTimeStampè®¡ç®—
        for sleep_period in data_list:
            if not isinstance(sleep_period, dict):
                continue
                
            start_time = sleep_period.get('startTimeStamp')
            end_time = sleep_period.get('endTimeStamp')
            
            if start_time is None or end_time is None:
                continue
                
            try:
                # æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’
                start_seconds = int(start_time) / 1000 if int(start_time) > 9999999999 else int(start_time)
                end_seconds = int(end_time) / 1000 if int(end_time) > 9999999999 else int(end_time)
                
                # è®¡ç®—æ—¶é—´å·®(ç§’)
                if end_seconds > start_seconds:
                    total_sleep_seconds += (end_seconds - start_seconds)
                    
            except (ValueError, TypeError):
                continue
                
        # è½¬æ¢ä¸ºå°æ—¶ï¼Œä¿ç•™2ä½å°æ•°
        if total_sleep_seconds > 0:
            total_sleep_hours = round(total_sleep_seconds / 3600, 2)
            return total_sleep_hours
        else:
            return None
            
    except (json.JSONDecodeError, Exception) as e:
        print(f"è§£æsleepDataæ—¶å‡ºé”™: {e}, åŸå§‹æ•°æ®: {sleep_data_json}")
        return None