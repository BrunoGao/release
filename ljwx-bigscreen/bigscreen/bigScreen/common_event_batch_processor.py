#!/usr/bin/env python3
"""
é€šç”¨äº‹ä»¶æ‰¹é‡å¤„ç†å™¨ v2.0
CPUè‡ªé€‚åº”é«˜æ€§èƒ½æ‰¹å¤„ç†ç³»ç»Ÿï¼Œä¸“é—¨å¤„ç†upload_common_eventæ¥å£æ•°æ®
"""

import threading
import queue
import time
import json
import psutil
from datetime import datetime
from typing import Dict, List, Any, Optional
from .redis_helper import RedisHelper
from .time_config import get_now
import logging
from config import MYSQL_HOST, MYSQL_PORT, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE
import pymysql
from .models import db, AlertInfo, AlertRules
from sqlalchemy import and_

class CommonEventBatchProcessor:
    """CPUè‡ªé€‚åº”é€šç”¨äº‹ä»¶æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, app=None):
        # ç³»ç»Ÿä¿¡æ¯æ£€æµ‹
        self.cpu_cores = psutil.cpu_count(logical=True)
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # CPUè‡ªé€‚åº”é…ç½®
        self.batch_size = self._calculate_optimal_batch_size()
        self.max_workers = self._calculate_optimal_workers()
        self.max_wait_time = 1.5  # é€šç”¨äº‹ä»¶å“åº”è¦æ±‚è¾ƒé«˜
        
        # é˜Ÿåˆ—å’Œçº¿ç¨‹ç®¡ç†
        self.event_queue = queue.Queue(maxsize=5000)
        self.redis = RedisHelper()
        self.running = False
        self.workers = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'processed': 0,
            'failed': 0, 
            'queued': 0,
            'batch_count': 0,
            'avg_processing_time': 0.0,
            'last_adjustment_time': time.time()
        }
        
        # æ€§èƒ½ç›‘æ§
        self.performance_window = []
        self.adjustment_interval = 30  # 30ç§’è°ƒæ•´ä¸€æ¬¡
        
        self.logger = logging.getLogger(__name__)
        self.app = app
        
        # å·²å¤„ç†äº‹ä»¶å»é‡
        self.processed_keys = set()
        
        self.logger.info(f"ğŸš€ CommonEventBatchProcessoråˆå§‹åŒ–:")
        self.logger.info(f"   CPUæ ¸å¿ƒ: {self.cpu_cores}")
        self.logger.info(f"   å†…å­˜: {self.memory_gb:.1f}GB") 
        self.logger.info(f"   æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        self.logger.info(f"   å·¥ä½œçº¿ç¨‹: {self.max_workers}")
    
    def _calculate_optimal_batch_size(self) -> int:
        """è®¡ç®—CPUè‡ªé€‚åº”æ‰¹æ¬¡å¤§å°"""
        # åŸºç¡€æ‰¹æ¬¡å¤§å°ï¼šCPUæ ¸å¿ƒæ•° Ã— 8ï¼ˆé€šç”¨äº‹ä»¶ç›¸å¯¹ç®€å•ï¼‰
        base_size = self.cpu_cores * 8
        
        # å†…å­˜è°ƒæ•´ç³»æ•°
        memory_factor = min(1.5, self.memory_gb / 8.0)  # 8GBä¸ºåŸºå‡†
        
        # æœ€ç»ˆæ‰¹æ¬¡å¤§å°
        batch_size = int(base_size * memory_factor)
        
        return max(10, min(200, batch_size))  # é™åˆ¶åœ¨10-200ä¹‹é—´
    
    def _calculate_optimal_workers(self) -> int:
        """è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°"""
        # é€šç”¨äº‹ä»¶å¤„ç†ï¼šCPUæ ¸å¿ƒæ•° Ã— 1ï¼ˆä¸»è¦æ˜¯æ•°æ®åº“I/Oï¼‰
        workers = max(2, min(16, self.cpu_cores))
        
        return workers
    
    def start(self):
        """å¯åŠ¨æ‰¹é‡å¤„ç†å™¨"""
        if self.running:
            return
            
        self.running = True
        self.start_time = time.time()
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(self.max_workers):
            worker = threading.Thread(
                target=self._worker_thread, 
                name=f'CommonEventWorker-{i}',
                daemon=True
            )
            worker.start()
            self.workers.append(worker)
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(
            target=self._performance_monitor,
            name='CommonEventMonitor',
            daemon=True
        )
        monitor_thread.start()
        
        self.logger.info(f"ğŸ¯ é€šç”¨äº‹ä»¶æ‰¹å¤„ç†å™¨å¯åŠ¨å®Œæˆï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
    
    def stop(self):
        """åœæ­¢æ‰¹é‡å¤„ç†å™¨"""
        self.running = False
        for worker in self.workers:
            worker.join(timeout=5)
        self.logger.info("â›” é€šç”¨äº‹ä»¶æ‰¹å¤„ç†å™¨å·²åœæ­¢")
    
    def submit(self, event_data: Dict[str, Any]) -> bool:
        """æäº¤é€šç”¨äº‹ä»¶æ•°æ®åˆ°å¤„ç†é˜Ÿåˆ—"""
        try:
            # ç”Ÿæˆäº‹ä»¶å”¯ä¸€é”®
            device_sn = event_data.get('deviceSn', '')
            event_type = event_data.get('eventType', '')
            timestamp = event_data.get('timestamp', get_now().strftime('%Y-%m-%d %H:%M:%S'))
            
            event_key = f"{device_sn}:{event_type}:{timestamp}"
            
            # å»é‡æ£€æŸ¥
            if event_key in self.processed_keys:
                self.logger.debug(f"âš ï¸ é‡å¤äº‹ä»¶è·³è¿‡: {event_key}")
                return True
            
            # æ·»åŠ å¤„ç†æ ‡è¯†
            enriched_data = {
                **event_data,
                'event_key': event_key,
                'submit_time': time.time()
            }
            
            # æäº¤åˆ°é˜Ÿåˆ—
            self.event_queue.put(enriched_data, timeout=0.1)
            self.stats['queued'] += 1
            
            return True
            
        except queue.Full:
            self.logger.warning("âŒ é€šç”¨äº‹ä»¶é˜Ÿåˆ—å·²æ»¡ï¼Œæ•°æ®ä¸¢å¼ƒ")
            return False
        except Exception as e:
            self.logger.error(f"âŒ æäº¤é€šç”¨äº‹ä»¶å¤±è´¥: {e}")
            return False
    
    def _worker_thread(self):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        batch_buffer = []
        last_process_time = time.time()
        
        while self.running:
            try:
                # å°è¯•è·å–æ•°æ®
                try:
                    event_data = self.event_queue.get(timeout=0.5)
                    batch_buffer.append(event_data)
                except queue.Empty:
                    pass
                
                # æ£€æŸ¥æ‰¹å¤„ç†æ¡ä»¶
                current_time = time.time()
                should_process = (
                    len(batch_buffer) >= self.batch_size or  # è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    (batch_buffer and current_time - last_process_time >= self.max_wait_time)  # è¶…æ—¶
                )
                
                if should_process and batch_buffer:
                    success = self._process_batch(batch_buffer)
                    if success:
                        # æ ‡è®°å·²å¤„ç†
                        for event in batch_buffer:
                            self.processed_keys.add(event.get('event_key', ''))
                    
                    batch_buffer.clear()
                    last_process_time = current_time
                    
            except Exception as e:
                self.logger.error(f"âŒ å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
                batch_buffer.clear()
                time.sleep(1)
    
    def _process_batch(self, batch_events: List[Dict[str, Any]]) -> bool:
        """æ‰¹é‡å¤„ç†é€šç”¨äº‹ä»¶"""
        start_time = time.time()
        
        try:
            if not self.app:
                self.logger.error("âŒ Flaskåº”ç”¨å®ä¾‹æœªè®¾ç½®")
                return False
            
            with self.app.app_context():
                alerts_to_create = []
                
                for event_data in batch_events:
                    try:
                        alert = self._create_alert_from_event(event_data)
                        if alert:
                            alerts_to_create.append(alert)
                    except Exception as e:
                        self.logger.error(f"âŒ å¤„ç†å•ä¸ªäº‹ä»¶å¤±è´¥: {e}")
                        continue
                
                # æ‰¹é‡æ’å…¥å‘Šè­¦è®°å½•
                if alerts_to_create:
                    try:
                        db.session.bulk_save_objects(alerts_to_create)
                        db.session.commit()
                        
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        processing_time = time.time() - start_time
                        self._update_performance_stats(len(batch_events), processing_time)
                        
                        self.logger.info(f"âœ… æ‰¹é‡å¤„ç†é€šç”¨äº‹ä»¶æˆåŠŸ: {len(alerts_to_create)}æ¡å‘Šè­¦, "
                                       f"è€—æ—¶: {processing_time:.2f}ç§’")
                        return True
                        
                    except Exception as e:
                        self.logger.error(f"âŒ æ‰¹é‡æ’å…¥å‘Šè­¦å¤±è´¥: {e}")
                        db.session.rollback()
                        return False
                else:
                    self.logger.info("âš ï¸ æ— æœ‰æ•ˆå‘Šè­¦éœ€è¦åˆ›å»º")
                    return True
                    
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡å¤„ç†å¼‚å¸¸: {e}")
            return False
    
    def _create_alert_from_event(self, event_data: Dict[str, Any]) -> Optional[AlertInfo]:
        """ä»äº‹ä»¶æ•°æ®åˆ›å»ºå‘Šè­¦è®°å½•"""
        try:
            # æå–äº‹ä»¶ç±»å‹
            event_type = event_data.get('eventType', '').split('.')[-1]
            device_sn = event_data.get('deviceSn', '')
            
            if not event_type or not device_sn:
                return None
            
            # æŸ¥è¯¢å‘Šè­¦è§„åˆ™
            rule = AlertRules.query.filter_by(
                rule_type=event_type, 
                is_deleted=False
            ).first()
            
            if not rule:
                self.logger.debug(f"âš ï¸ æœªæ‰¾åˆ°äº‹ä»¶ç±»å‹çš„å‘Šè­¦è§„åˆ™: {event_type}")
                return None
            
            # è·å–è®¾å¤‡ç”¨æˆ·ç»„ç»‡ä¿¡æ¯
            device_user_org = self._get_device_user_org_info(device_sn)
            
            # åˆ›å»ºå‘Šè­¦è®°å½•
            alert = AlertInfo(
                rule_id=rule.id,
                alert_type=event_type,
                device_sn=device_sn,
                alert_desc=f"{rule.alert_message}(äº‹ä»¶å€¼:{event_data.get('eventValue', '')})",
                severity_level=rule.severity_level,
                latitude=event_data.get('latitude', 22.54036796),
                longitude=event_data.get('longitude', 114.01508952),
                altitude=event_data.get('altitude', 0),
                org_id=device_user_org.get('org_id') if device_user_org.get('success') else None,
                user_id=device_user_org.get('user_id') if device_user_org.get('success') else None,
                alert_timestamp=event_data.get('timestamp', get_now().strftime('%Y-%m-%d %H:%M:%S'))
            )
            
            return alert
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå‘Šè­¦è®°å½•å¤±è´¥: {e}")
            return None
    
    def _get_device_user_org_info(self, device_sn: str) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ç”¨æˆ·ç»„ç»‡ä¿¡æ¯"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è®¾å¤‡ä¿¡æ¯æŸ¥è¯¢é€»è¾‘
            # æš‚æ—¶è¿”å›é»˜è®¤å€¼
            return {
                'success': True,
                'org_id': 1,
                'user_id': 1
            }
        except Exception as e:
            self.logger.error(f"âŒ è·å–è®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
            return {'success': False}
    
    def _update_performance_stats(self, batch_size: int, processing_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['processed'] += batch_size
        self.stats['batch_count'] += 1
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        total_time = self.stats['avg_processing_time'] * (self.stats['batch_count'] - 1)
        self.stats['avg_processing_time'] = (total_time + processing_time) / self.stats['batch_count']
        
        # è®°å½•æ€§èƒ½æ•°æ®
        self.performance_window.append({
            'batch_size': batch_size,
            'processing_time': processing_time,
            'throughput': batch_size / processing_time if processing_time > 0 else 0,
            'timestamp': time.time()
        })
        
        # ä¿æŒæ€§èƒ½çª—å£å¤§å°
        if len(self.performance_window) > 100:
            self.performance_window.pop(0)
    
    def _performance_monitor(self):
        """æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜"""
        while self.running:
            try:
                time.sleep(self.adjustment_interval)
                
                if len(self.performance_window) >= 5:
                    self._auto_adjust_configuration()
                
                # æ¸…ç†è¿‡æœŸçš„å¤„ç†é”®
                self._cleanup_processed_keys()
                
            except Exception as e:
                self.logger.error(f"âŒ æ€§èƒ½ç›‘æ§å¼‚å¸¸: {e}")
    
    def _auto_adjust_configuration(self):
        """è‡ªåŠ¨è°ƒæ•´é…ç½®å‚æ•°"""
        if len(self.performance_window) < 5:
            return
            
        # è®¡ç®—æœ€è¿‘æ€§èƒ½æŒ‡æ ‡
        recent_performance = self.performance_window[-10:]
        avg_throughput = sum(p['throughput'] for p in recent_performance) / len(recent_performance)
        avg_processing_time = sum(p['processing_time'] for p in recent_performance) / len(recent_performance)
        
        # è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent
        queue_size = self.event_queue.qsize()
        
        old_batch_size = self.batch_size
        
        # è°ƒæ•´ç­–ç•¥
        if cpu_percent < 50 and avg_throughput < 30:
            # CPUåˆ©ç”¨ç‡ä½ï¼Œååé‡ä½ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°
            self.batch_size = min(200, int(self.batch_size * 1.2))
        elif cpu_percent > 85 or memory_percent > 80:
            # èµ„æºå‹åŠ›å¤§ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°
            self.batch_size = max(10, int(self.batch_size * 0.8))
        elif queue_size > 1000:
            # é˜Ÿåˆ—å †ç§¯ï¼Œå¢åŠ å¤„ç†èƒ½åŠ›
            self.batch_size = min(200, int(self.batch_size * 1.1))
        
        # è®°å½•è°ƒæ•´
        if old_batch_size != self.batch_size:
            self.logger.info(f"ğŸ“Š è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°: {old_batch_size} â†’ {self.batch_size} "
                           f"(CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_percent:.1f}%, "
                           f"é˜Ÿåˆ—: {queue_size}, ååé‡: {avg_throughput:.1f}/ç§’)")
    
    def _cleanup_processed_keys(self):
        """æ¸…ç†è¿‡æœŸçš„å·²å¤„ç†é”®"""
        # ä¿ç•™æœ€è¿‘1å°æ—¶çš„é”®ï¼Œé¿å…å†…å­˜æ³„æ¼
        if len(self.processed_keys) > 10000:
            # ç®€å•çš„LRUç­–ç•¥ï¼Œæ¸…ç†ä¸€åŠ
            keys_list = list(self.processed_keys)
            self.processed_keys = set(keys_list[len(keys_list)//2:])
            self.logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸå¤„ç†é”®ï¼Œå½“å‰æ•°é‡: {len(self.processed_keys)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        uptime = time.time() - getattr(self, 'start_time', time.time())
        
        return {
            'processor_name': 'CommonEventBatchProcessor',
            'cpu_cores': self.cpu_cores,
            'memory_gb': self.memory_gb,
            'batch_size': self.batch_size,
            'max_workers': self.max_workers,
            'queue_size': self.event_queue.qsize(),
            'processed_total': self.stats['processed'],
            'batch_count': self.stats['batch_count'],
            'failed_count': self.stats['failed'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'uptime_seconds': uptime,
            'processed_keys_count': len(self.processed_keys),
            'running': self.running
        }
    
    def get_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        stats = self.get_stats()
        
        throughput = stats['processed_total'] / max(1, stats['uptime_seconds'])
        
        return f"""
ğŸ¯ CommonEventBatchProcessor æ€§èƒ½æŠ¥å‘Š
=====================================
ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   - CPUæ ¸å¿ƒæ•°: {stats['cpu_cores']}
   - å†…å­˜: {stats['memory_gb']:.1f}GB
   - å½“å‰æ‰¹æ¬¡å¤§å°: {stats['batch_size']}
   - å·¥ä½œçº¿ç¨‹æ•°: {stats['max_workers']}

ğŸ“ˆ å¤„ç†ç»Ÿè®¡:
   - æ€»å¤„ç†é‡: {stats['processed_total']}æ¡
   - æ‰¹æ¬¡æ•°: {stats['batch_count']}
   - å¤±è´¥æ•°: {stats['failed_count']}  
   - å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.2f}ç§’
   - æ•´ä½“ååé‡: {throughput:.1f}æ¡/ç§’

ğŸ”„ è¿è¡ŒçŠ¶æ€:
   - è¿è¡Œæ—¶é—´: {stats['uptime_seconds']:.0f}ç§’
   - é˜Ÿåˆ—é•¿åº¦: {stats['queue_size']}
   - å¤„ç†é”®æ•°: {stats['processed_keys_count']}
   - è¿è¡ŒçŠ¶æ€: {'âœ… æ­£å¸¸' if stats['running'] else 'âŒ å·²åœæ­¢'}
"""


# å…¨å±€å¤„ç†å™¨å®ä¾‹
_common_event_processor = None

def get_common_event_processor(app=None) -> CommonEventBatchProcessor:
    """è·å–å…¨å±€é€šç”¨äº‹ä»¶å¤„ç†å™¨å®ä¾‹"""
    global _common_event_processor
    
    if _common_event_processor is None:
        _common_event_processor = CommonEventBatchProcessor(app=app)
        _common_event_processor.start()
    
    return _common_event_processor

def init_common_event_processor(app):
    """åˆå§‹åŒ–é€šç”¨äº‹ä»¶å¤„ç†å™¨"""
    processor = get_common_event_processor(app)
    app.logger.info("âœ… CommonEventBatchProcessor åˆå§‹åŒ–å®Œæˆ")
    return processor