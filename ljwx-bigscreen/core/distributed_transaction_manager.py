#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å¸ƒå¼äº‹åŠ¡å’Œç¼“å­˜ä¸€è‡´æ€§ç®¡ç†å™¨

ä¸»è¦åŠŸèƒ½:
1. åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒï¼ˆSagaæ¨¡å¼ï¼‰
2. æœ€ç»ˆä¸€è‡´æ€§ä¿è¯
3. ç¼“å­˜å¤±æ•ˆç­–ç•¥
4. è¡¥å¿æœºåˆ¶
5. äº‹ä»¶é©±åŠ¨æ¶æ„
6. æ­»ä¿¡é˜Ÿåˆ—å¤„ç†

æŠ€æœ¯ç‰¹æ€§:
- Redis Streamæ¶ˆæ¯é˜Ÿåˆ—
- äº‹åŠ¡æ—¥å¿—è®°å½•
- è‡ªåŠ¨é‡è¯•å’Œè¡¥å¿
- ç›‘æ§å’Œå‘Šè­¦é›†æˆ

@Author: brunoGao
@CreateTime: 2025-09-11
@Version: 2.0-Fixed
"""

import json
import time
import logging
import asyncio
from typing import Dict, Any, List, Optional, Callable, Union
from datetime import datetime, timezone, timedelta
from enum import Enum
from dataclasses import dataclass, field
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor, as_completed
import uuid

import redis
from redis import Redis
from sqlalchemy import create_engine, text
from sqlalchemy.orm import Session, sessionmaker
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError
import pymongo
from pymongo import MongoClient
from pymongo.errors import PyMongoError

logger = logging.getLogger(__name__)


# ==================== æšä¸¾å’Œå¸¸é‡ ====================

class TransactionState(Enum):
    """äº‹åŠ¡çŠ¶æ€"""
    PENDING = "pending"           # å¾…å¤„ç†
    PROCESSING = "processing"     # å¤„ç†ä¸­
    COMPLETED = "completed"       # å®Œæˆ
    FAILED = "failed"            # å¤±è´¥
    COMPENSATING = "compensating" # è¡¥å¿ä¸­
    COMPENSATED = "compensated"   # å·²è¡¥å¿


class EventType(Enum):
    """äº‹ä»¶ç±»å‹"""
    TRANSACTION_START = "transaction_start"
    TRANSACTION_COMMIT = "transaction_commit"
    TRANSACTION_ROLLBACK = "transaction_rollback"
    CACHE_INVALIDATE = "cache_invalidate"
    DATA_SYNC = "data_sync"
    COMPENSATION = "compensation"


class CacheInvalidationType(Enum):
    """ç¼“å­˜å¤±æ•ˆç±»å‹"""
    IMMEDIATE = "immediate"       # ç«‹å³å¤±æ•ˆ
    DELAYED = "delayed"          # å»¶è¿Ÿå¤±æ•ˆ
    PATTERN = "pattern"          # æ¨¡å¼å¤±æ•ˆ
    SELECTIVE = "selective"      # é€‰æ‹©æ€§å¤±æ•ˆ


# ==================== æ•°æ®ç»“æ„å®šä¹‰ ====================

@dataclass
class TransactionStep:
    """äº‹åŠ¡æ­¥éª¤"""
    step_id: str
    step_name: str
    action: Callable
    compensation: Optional[Callable] = None
    retry_count: int = 0
    max_retries: int = 3
    timeout: int = 30
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DistributedTransaction:
    """åˆ†å¸ƒå¼äº‹åŠ¡"""
    transaction_id: str
    customer_id: int
    transaction_type: str
    steps: List[TransactionStep]
    state: TransactionState = TransactionState.PENDING
    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class CacheInvalidationEvent:
    """ç¼“å­˜å¤±æ•ˆäº‹ä»¶"""
    event_id: str
    customer_id: int
    invalidation_type: CacheInvalidationType
    cache_keys: List[str]
    cache_patterns: List[str] = field(default_factory=list)
    delay_seconds: int = 0
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class EventMessage:
    """äº‹ä»¶æ¶ˆæ¯"""
    event_id: str
    event_type: EventType
    customer_id: int
    payload: Dict[str, Any]
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    retry_count: int = 0
    max_retries: int = 5


# ==================== äº‹åŠ¡æ—¥å¿—ç®¡ç†å™¨ ====================

class TransactionLogger:
    """äº‹åŠ¡æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, redis_client: Redis, db_session: Session):
        self.redis = redis_client
        self.db_session = db_session
    
    def log_transaction_start(self, transaction: DistributedTransaction):
        """è®°å½•äº‹åŠ¡å¼€å§‹"""
        log_entry = {
            'transaction_id': transaction.transaction_id,
            'customer_id': transaction.customer_id,
            'transaction_type': transaction.transaction_type,
            'state': transaction.state.value,
            'start_time': transaction.start_time.isoformat(),
            'step_count': len(transaction.steps),
            'context': transaction.context
        }
        
        # Redisæ—¥å¿—ï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
        self.redis.hset(
            f"transaction_log:{transaction.transaction_id}", 
            mapping=log_entry
        )
        self.redis.expire(f"transaction_log:{transaction.transaction_id}", 86400)  # 24å°æ—¶è¿‡æœŸ
        
        # æ•°æ®åº“æ—¥å¿—ï¼ˆæŒä¹…åŒ–ï¼‰
        try:
            self.db_session.execute(
                text("""
                    INSERT INTO t_transaction_log 
                    (transaction_id, customer_id, transaction_type, state, start_time, context, created_at)
                    VALUES (:transaction_id, :customer_id, :transaction_type, :state, :start_time, :context, :created_at)
                """),
                {
                    'transaction_id': transaction.transaction_id,
                    'customer_id': transaction.customer_id,
                    'transaction_type': transaction.transaction_type,
                    'state': transaction.state.value,
                    'start_time': transaction.start_time,
                    'context': json.dumps(transaction.context),
                    'created_at': datetime.now(timezone.utc)
                }
            )
            self.db_session.commit()
        except SQLAlchemyError as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“äº‹åŠ¡æ—¥å¿—è®°å½•å¤±è´¥: {e}")
            self.db_session.rollback()
    
    def log_transaction_step(
        self, 
        transaction_id: str, 
        step_id: str, 
        step_state: str, 
        result: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None
    ):
        """è®°å½•äº‹åŠ¡æ­¥éª¤"""
        step_log = {
            'step_id': step_id,
            'state': step_state,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'result': json.dumps(result) if result else None,
            'error': error
        }
        
        # æ·»åŠ åˆ°Redisåˆ—è¡¨
        self.redis.lpush(f"transaction_steps:{transaction_id}", json.dumps(step_log))
        self.redis.expire(f"transaction_steps:{transaction_id}", 86400)
    
    def log_transaction_complete(self, transaction: DistributedTransaction):
        """è®°å½•äº‹åŠ¡å®Œæˆ"""
        # æ›´æ–°Redis
        self.redis.hset(
            f"transaction_log:{transaction.transaction_id}",
            mapping={
                'state': transaction.state.value,
                'end_time': transaction.end_time.isoformat() if transaction.end_time else None,
                'error_message': transaction.error_message or '',
                'duration_ms': int((transaction.end_time - transaction.start_time).total_seconds() * 1000) if transaction.end_time else 0
            }
        )
        
        # æ›´æ–°æ•°æ®åº“
        try:
            self.db_session.execute(
                text("""
                    UPDATE t_transaction_log 
                    SET state = :state, end_time = :end_time, error_message = :error_message, updated_at = :updated_at
                    WHERE transaction_id = :transaction_id
                """),
                {
                    'transaction_id': transaction.transaction_id,
                    'state': transaction.state.value,
                    'end_time': transaction.end_time,
                    'error_message': transaction.error_message,
                    'updated_at': datetime.now(timezone.utc)
                }
            )
            self.db_session.commit()
        except SQLAlchemyError as e:
            logger.warning(f"âš ï¸ äº‹åŠ¡å®Œæˆæ—¥å¿—æ›´æ–°å¤±è´¥: {e}")
            self.db_session.rollback()


# ==================== ç¼“å­˜ä¸€è‡´æ€§ç®¡ç†å™¨ ====================

class CacheConsistencyManager:
    """ç¼“å­˜ä¸€è‡´æ€§ç®¡ç†å™¨"""
    
    def __init__(self, redis_client: Redis, event_publisher: 'EventPublisher'):
        self.redis = redis_client
        self.event_publisher = event_publisher
        self.invalidation_patterns = {}  # ç¼“å­˜å¤±æ•ˆæ¨¡å¼
        
    def register_invalidation_pattern(
        self, 
        event_type: str, 
        cache_pattern: str,
        delay_seconds: int = 0
    ):
        """æ³¨å†Œç¼“å­˜å¤±æ•ˆæ¨¡å¼"""
        if event_type not in self.invalidation_patterns:
            self.invalidation_patterns[event_type] = []
            
        self.invalidation_patterns[event_type].append({
            'pattern': cache_pattern,
            'delay_seconds': delay_seconds
        })
    
    def invalidate_cache_immediate(
        self, 
        cache_keys: List[str], 
        cache_patterns: List[str] = None
    ) -> bool:
        """ç«‹å³å¤±æ•ˆç¼“å­˜"""
        try:
            # åˆ é™¤æŒ‡å®šé”®
            if cache_keys:
                deleted_count = self.redis.delete(*cache_keys)
                logger.debug(f"âœ… ç«‹å³åˆ é™¤ç¼“å­˜é”®: {deleted_count}ä¸ª")
            
            # åˆ é™¤æ¨¡å¼åŒ¹é…çš„é”®
            if cache_patterns:
                for pattern in cache_patterns:
                    keys = self.redis.keys(pattern)
                    if keys:
                        deleted_count = self.redis.delete(*keys)
                        logger.debug(f"âœ… æ¨¡å¼åˆ é™¤ç¼“å­˜é”® {pattern}: {deleted_count}ä¸ª")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç«‹å³ç¼“å­˜å¤±æ•ˆå¤±è´¥: {e}")
            return False
    
    def invalidate_cache_delayed(
        self, 
        cache_keys: List[str], 
        delay_seconds: int,
        cache_patterns: List[str] = None
    ):
        """å»¶è¿Ÿå¤±æ•ˆç¼“å­˜"""
        try:
            # åˆ›å»ºå»¶è¿Ÿå¤±æ•ˆä»»åŠ¡
            invalidation_event = CacheInvalidationEvent(
                event_id=str(uuid.uuid4()),
                customer_id=0,  # å…¨å±€å¤±æ•ˆ
                invalidation_type=CacheInvalidationType.DELAYED,
                cache_keys=cache_keys,
                cache_patterns=cache_patterns or [],
                delay_seconds=delay_seconds
            )
            
            # å‘å¸ƒå»¶è¿Ÿäº‹ä»¶
            self.event_publisher.publish_event(
                EventMessage(
                    event_id=invalidation_event.event_id,
                    event_type=EventType.CACHE_INVALIDATE,
                    customer_id=0,
                    payload=invalidation_event.__dict__
                ),
                delay_seconds=delay_seconds
            )
            
            logger.debug(f"âœ… åˆ›å»ºå»¶è¿Ÿç¼“å­˜å¤±æ•ˆä»»åŠ¡: {delay_seconds}ç§’åæ‰§è¡Œ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå»¶è¿Ÿç¼“å­˜å¤±æ•ˆä»»åŠ¡å¤±è´¥: {e}")
    
    def invalidate_by_event(self, event_type: str, customer_id: int, context: Dict[str, Any]):
        """æ ¹æ®äº‹ä»¶å¤±æ•ˆç¼“å­˜"""
        patterns = self.invalidation_patterns.get(event_type, [])
        
        for pattern_config in patterns:
            cache_pattern = pattern_config['pattern']
            delay_seconds = pattern_config['delay_seconds']
            
            # æ›¿æ¢æ¨¡å¼ä¸­çš„å ä½ç¬¦
            cache_pattern = cache_pattern.format(
                customer_id=customer_id,
                **context
            )
            
            if delay_seconds > 0:
                self.invalidate_cache_delayed([cache_pattern], delay_seconds)
            else:
                self.invalidate_cache_immediate([], [cache_pattern])
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        try:
            info = self.redis.info('stats')
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            hits = info.get('keyspace_hits', 0)
            misses = info.get('keyspace_misses', 0)
            total_requests = hits + misses
            hit_rate = (hits / max(total_requests, 1)) * 100
            
            return {
                'keyspace_hits': hits,
                'keyspace_misses': misses,
                'total_requests': total_requests,
                'hit_rate': round(hit_rate, 2),
                'connected_clients': info.get('connected_clients', 0),
                'used_memory': info.get('used_memory', 0),
                'used_memory_human': info.get('used_memory_human', '0B')
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# ==================== äº‹ä»¶å‘å¸ƒå™¨ ====================

class EventPublisher:
    """äº‹ä»¶å‘å¸ƒå™¨ - åŸºäºRedis Stream"""
    
    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.stream_name = "message_events"
        self.consumer_group = "message_processors"
        
        # ç¡®ä¿æ¶ˆè´¹è€…ç»„å­˜åœ¨
        try:
            self.redis.xgroup_create(self.stream_name, self.consumer_group, id='0', mkstream=True)
        except redis.exceptions.ResponseError:
            # ç»„å·²å­˜åœ¨
            pass
    
    def publish_event(self, event: EventMessage, delay_seconds: int = 0):
        """å‘å¸ƒäº‹ä»¶"""
        try:
            event_data = {
                'event_id': event.event_id,
                'event_type': event.event_type.value,
                'customer_id': str(event.customer_id),
                'payload': json.dumps(event.payload, default=str),
                'created_at': event.created_at.isoformat(),
                'retry_count': str(event.retry_count),
                'max_retries': str(event.max_retries)
            }
            
            if delay_seconds > 0:
                # å»¶è¿Ÿå‘å¸ƒï¼šæ·»åŠ åˆ°å»¶è¿Ÿé˜Ÿåˆ—
                execute_at = datetime.now(timezone.utc) + timedelta(seconds=delay_seconds)
                self.redis.zadd(
                    'delayed_events',
                    {json.dumps(event_data): execute_at.timestamp()}
                )
                logger.debug(f"âœ… äº‹ä»¶å·²æ·»åŠ åˆ°å»¶è¿Ÿé˜Ÿåˆ—: {event.event_id}, å»¶è¿Ÿ: {delay_seconds}ç§’")
            else:
                # ç«‹å³å‘å¸ƒ
                message_id = self.redis.xadd(self.stream_name, event_data)
                logger.debug(f"âœ… äº‹ä»¶å·²å‘å¸ƒ: {event.event_id}, æ¶ˆæ¯ID: {message_id}")
            
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶å‘å¸ƒå¤±è´¥: {event.event_id}, é”™è¯¯: {e}")
            raise
    
    def process_delayed_events(self):
        """å¤„ç†å»¶è¿Ÿäº‹ä»¶ï¼ˆå®šæ—¶ä»»åŠ¡è°ƒç”¨ï¼‰"""
        try:
            current_time = datetime.now(timezone.utc).timestamp()
            
            # è·å–åˆ°æœŸçš„å»¶è¿Ÿäº‹ä»¶
            ready_events = self.redis.zrangebyscore(
                'delayed_events', 
                '-inf', 
                current_time, 
                withscores=True
            )
            
            for event_data, score in ready_events:
                try:
                    # å‘å¸ƒåˆ°æ¶ˆæ¯æµ
                    event_dict = json.loads(event_data)
                    message_id = self.redis.xadd(self.stream_name, event_dict)
                    
                    # ä»å»¶è¿Ÿé˜Ÿåˆ—ä¸­ç§»é™¤
                    self.redis.zrem('delayed_events', event_data)
                    
                    logger.debug(f"âœ… å»¶è¿Ÿäº‹ä»¶å·²å‘å¸ƒ: {event_dict.get('event_id')}, æ¶ˆæ¯ID: {message_id}")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å»¶è¿Ÿäº‹ä»¶å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å»¶è¿Ÿäº‹ä»¶ä»»åŠ¡å¤±è´¥: {e}")


# ==================== äº‹ä»¶æ¶ˆè´¹å™¨ ====================

class EventConsumer:
    """äº‹ä»¶æ¶ˆè´¹å™¨"""
    
    def __init__(
        self, 
        redis_client: Redis, 
        consumer_name: str,
        cache_manager: CacheConsistencyManager,
        transaction_logger: TransactionLogger
    ):
        self.redis = redis_client
        self.consumer_name = consumer_name
        self.cache_manager = cache_manager
        self.transaction_logger = transaction_logger
        self.stream_name = "message_events"
        self.consumer_group = "message_processors"
        self.running = False
        
        # äº‹ä»¶å¤„ç†å™¨æ˜ å°„
        self.event_handlers = {
            EventType.CACHE_INVALIDATE: self._handle_cache_invalidate,
            EventType.DATA_SYNC: self._handle_data_sync,
            EventType.TRANSACTION_COMMIT: self._handle_transaction_commit,
            EventType.COMPENSATION: self._handle_compensation
        }
    
    def start_consuming(self):
        """å¼€å§‹æ¶ˆè´¹äº‹ä»¶"""
        self.running = True
        logger.info(f"ğŸš€ äº‹ä»¶æ¶ˆè´¹å™¨å¯åŠ¨: {self.consumer_name}")
        
        while self.running:
            try:
                # è¯»å–æ–°æ¶ˆæ¯
                messages = self.redis.xreadgroup(
                    self.consumer_group,
                    self.consumer_name,
                    {self.stream_name: '>'},
                    count=10,
                    block=5000  # 5ç§’è¶…æ—¶
                )
                
                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        self._process_message(msg_id, fields)
                        
            except KeyboardInterrupt:
                logger.info("â¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…åœæ­¢...")
                break
            except Exception as e:
                logger.error(f"âŒ äº‹ä»¶æ¶ˆè´¹å¤±è´¥: {e}")
                time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
        
        logger.info(f"â¹ï¸ äº‹ä»¶æ¶ˆè´¹å™¨å·²åœæ­¢: {self.consumer_name}")
    
    def stop_consuming(self):
        """åœæ­¢æ¶ˆè´¹äº‹ä»¶"""
        self.running = False
    
    def _process_message(self, msg_id: str, fields: Dict[str, Any]):
        """å¤„ç†å•ä¸ªæ¶ˆæ¯"""
        try:
            # è§£ææ¶ˆæ¯
            event_type = EventType(fields['event_type'])
            customer_id = int(fields['customer_id'])
            payload = json.loads(fields['payload'])
            retry_count = int(fields.get('retry_count', '0'))
            max_retries = int(fields.get('max_retries', '5'))
            
            logger.debug(f"ğŸ“¨ å¤„ç†äº‹ä»¶: {event_type.value}, ID: {fields['event_id']}")
            
            # è·å–å¤„ç†å™¨
            handler = self.event_handlers.get(event_type)
            if not handler:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°äº‹ä»¶å¤„ç†å™¨: {event_type.value}")
                self._ack_message(msg_id)
                return
            
            # æ‰§è¡Œå¤„ç†å™¨
            success = handler(customer_id, payload)
            
            if success:
                # å¤„ç†æˆåŠŸï¼Œç¡®è®¤æ¶ˆæ¯
                self._ack_message(msg_id)
                logger.debug(f"âœ… äº‹ä»¶å¤„ç†æˆåŠŸ: {fields['event_id']}")
            else:
                # å¤„ç†å¤±è´¥ï¼Œæ£€æŸ¥é‡è¯•
                if retry_count < max_retries:
                    self._retry_message(fields, retry_count + 1)
                    self._ack_message(msg_id)
                else:
                    # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œç§»åˆ°æ­»ä¿¡é˜Ÿåˆ—
                    self._move_to_dead_letter(fields)
                    self._ack_message(msg_id)
                    logger.error(f"âŒ äº‹ä»¶å¤„ç†æœ€ç»ˆå¤±è´¥ï¼Œå·²ç§»è‡³æ­»ä¿¡é˜Ÿåˆ—: {fields['event_id']}")
                    
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¼‚å¸¸: {e}")
            # ä¸ç¡®è®¤æ¶ˆæ¯ï¼Œç­‰å¾…é‡æ–°å¤„ç†
    
    def _handle_cache_invalidate(self, customer_id: int, payload: Dict[str, Any]) -> bool:
        """å¤„ç†ç¼“å­˜å¤±æ•ˆäº‹ä»¶"""
        try:
            invalidation_type = CacheInvalidationType(payload['invalidation_type'])
            cache_keys = payload.get('cache_keys', [])
            cache_patterns = payload.get('cache_patterns', [])
            
            if invalidation_type == CacheInvalidationType.IMMEDIATE:
                return self.cache_manager.invalidate_cache_immediate(cache_keys, cache_patterns)
            elif invalidation_type == CacheInvalidationType.DELAYED:
                delay_seconds = payload.get('delay_seconds', 0)
                self.cache_manager.invalidate_cache_delayed(cache_keys, delay_seconds, cache_patterns)
                return True
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜å¤±æ•ˆå¤„ç†å¤±è´¥: {e}")
            return False
    
    def _handle_data_sync(self, customer_id: int, payload: Dict[str, Any]) -> bool:
        """å¤„ç†æ•°æ®åŒæ­¥äº‹ä»¶"""
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„æ•°æ®åŒæ­¥é€»è¾‘
        logger.debug(f"ğŸ“Š æ•°æ®åŒæ­¥äº‹ä»¶: customer_id={customer_id}")
        return True
    
    def _handle_transaction_commit(self, customer_id: int, payload: Dict[str, Any]) -> bool:
        """å¤„ç†äº‹åŠ¡æäº¤äº‹ä»¶"""
        # è¿™é‡Œå¯ä»¥å®ç°äº‹åŠ¡æäº¤åçš„å¤„ç†é€»è¾‘
        logger.debug(f"ğŸ’¾ äº‹åŠ¡æäº¤äº‹ä»¶: customer_id={customer_id}")
        return True
    
    def _handle_compensation(self, customer_id: int, payload: Dict[str, Any]) -> bool:
        """å¤„ç†è¡¥å¿äº‹ä»¶"""
        # è¿™é‡Œå¯ä»¥å®ç°è¡¥å¿é€»è¾‘
        logger.debug(f"ğŸ”„ è¡¥å¿äº‹ä»¶: customer_id={customer_id}")
        return True
    
    def _ack_message(self, msg_id: str):
        """ç¡®è®¤æ¶ˆæ¯"""
        try:
            self.redis.xack(self.stream_name, self.consumer_group, msg_id)
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯ç¡®è®¤å¤±è´¥: {msg_id}, é”™è¯¯: {e}")
    
    def _retry_message(self, fields: Dict[str, Any], new_retry_count: int):
        """é‡è¯•æ¶ˆæ¯"""
        try:
            # æ›´æ–°é‡è¯•æ¬¡æ•°
            fields['retry_count'] = str(new_retry_count)
            
            # é‡æ–°å‘å¸ƒåˆ°æµ
            self.redis.xadd(self.stream_name, fields)
            
            logger.debug(f"ğŸ”„ æ¶ˆæ¯é‡è¯•: {fields['event_id']}, ç¬¬{new_retry_count}æ¬¡")
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯é‡è¯•å¤±è´¥: {e}")
    
    def _move_to_dead_letter(self, fields: Dict[str, Any]):
        """ç§»åŠ¨åˆ°æ­»ä¿¡é˜Ÿåˆ—"""
        try:
            # æ·»åŠ å¤±è´¥æ—¶é—´æˆ³
            fields['failed_at'] = datetime.now(timezone.utc).isoformat()
            
            # ç§»åˆ°æ­»ä¿¡é˜Ÿåˆ—
            self.redis.xadd('dead_letter_queue', fields)
            
            logger.warning(f"â˜ ï¸ æ¶ˆæ¯å·²ç§»è‡³æ­»ä¿¡é˜Ÿåˆ—: {fields['event_id']}")
            
        except Exception as e:
            logger.error(f"âŒ æ­»ä¿¡é˜Ÿåˆ—æ“ä½œå¤±è´¥: {e}")


# ==================== åˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†å™¨ ====================

class DistributedTransactionManager:
    """åˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†å™¨ - Sagaæ¨¡å¼å®ç°"""
    
    def __init__(
        self, 
        redis_client: Redis, 
        db_engine: Engine,
        event_publisher: EventPublisher
    ):
        self.redis = redis_client
        self.db_engine = db_engine
        self.event_publisher = event_publisher
        self.Session = sessionmaker(bind=db_engine)
        
        # ç»„ä»¶åˆå§‹åŒ–
        with self.Session() as session:
            self.transaction_logger = TransactionLogger(redis_client, session)
        
        self.cache_manager = CacheConsistencyManager(redis_client, event_publisher)
        
        # é¢„å®šä¹‰ç¼“å­˜å¤±æ•ˆæ¨¡å¼
        self._setup_cache_invalidation_patterns()
    
    def _setup_cache_invalidation_patterns(self):
        """è®¾ç½®ç¼“å­˜å¤±æ•ˆæ¨¡å¼"""
        # æ¶ˆæ¯ç›¸å…³ç¼“å­˜å¤±æ•ˆæ¨¡å¼
        self.cache_manager.register_invalidation_pattern(
            'message_created', 
            'message_list:{customer_id}:*'
        )
        self.cache_manager.register_invalidation_pattern(
            'message_acknowledged', 
            'user_unread_count:{customer_id}:*'
        )
        self.cache_manager.register_invalidation_pattern(
            'message_stats_changed', 
            'message_stats:{customer_id}:*',
            delay_seconds=60  # å»¶è¿Ÿ1åˆ†é’Ÿå¤±æ•ˆ
        )
    
    def create_transaction(
        self, 
        customer_id: int,
        transaction_type: str,
        steps: List[TransactionStep],
        context: Dict[str, Any] = None
    ) -> DistributedTransaction:
        """åˆ›å»ºåˆ†å¸ƒå¼äº‹åŠ¡"""
        transaction = DistributedTransaction(
            transaction_id=str(uuid.uuid4()),
            customer_id=customer_id,
            transaction_type=transaction_type,
            steps=steps,
            context=context or {}
        )
        
        # è®°å½•äº‹åŠ¡å¼€å§‹
        with self.Session() as session:
            logger = TransactionLogger(self.redis, session)
            logger.log_transaction_start(transaction)
        
        return transaction
    
    async def execute_transaction(self, transaction: DistributedTransaction) -> bool:
        """æ‰§è¡Œåˆ†å¸ƒå¼äº‹åŠ¡ï¼ˆå¼‚æ­¥ï¼‰"""
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œåˆ†å¸ƒå¼äº‹åŠ¡: {transaction.transaction_id}")
        
        transaction.state = TransactionState.PROCESSING
        executed_steps = []
        
        try:
            # æŒ‰ä¾èµ–å…³ç³»æ’åºæ­¥éª¤
            sorted_steps = self._sort_steps_by_dependencies(transaction.steps)
            
            # é€æ­¥æ‰§è¡Œ
            for step in sorted_steps:
                logger.debug(f"ğŸ”§ æ‰§è¡Œæ­¥éª¤: {step.step_name}")
                
                # è®°å½•æ­¥éª¤å¼€å§‹
                with self.Session() as session:
                    step_logger = TransactionLogger(self.redis, session)
                    step_logger.log_transaction_step(
                        transaction.transaction_id, 
                        step.step_id, 
                        'executing'
                    )
                
                # æ‰§è¡Œæ­¥éª¤ï¼ˆå¸¦é‡è¯•ï¼‰
                success, result = await self._execute_step_with_retry(step, transaction.context)
                
                if success:
                    executed_steps.append(step)
                    
                    # è®°å½•æ­¥éª¤æˆåŠŸ
                    with self.Session() as session:
                        step_logger = TransactionLogger(self.redis, session)
                        step_logger.log_transaction_step(
                            transaction.transaction_id, 
                            step.step_id, 
                            'completed',
                            result=result
                        )
                    
                    logger.debug(f"âœ… æ­¥éª¤æ‰§è¡ŒæˆåŠŸ: {step.step_name}")
                else:
                    # æ­¥éª¤å¤±è´¥ï¼Œå¼€å§‹è¡¥å¿
                    logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¤±è´¥: {step.step_name}")
                    
                    # è®°å½•æ­¥éª¤å¤±è´¥
                    with self.Session() as session:
                        step_logger = TransactionLogger(self.redis, session)
                        step_logger.log_transaction_step(
                            transaction.transaction_id, 
                            step.step_id, 
                            'failed',
                            error=str(result)
                        )
                    
                    # æ‰§è¡Œè¡¥å¿
                    await self._compensate_transaction(transaction, executed_steps)
                    return False
            
            # æ‰€æœ‰æ­¥éª¤æˆåŠŸ
            transaction.state = TransactionState.COMPLETED
            transaction.end_time = datetime.now(timezone.utc)
            
            # è®°å½•äº‹åŠ¡å®Œæˆ
            with self.Session() as session:
                logger = TransactionLogger(self.redis, session)
                logger.log_transaction_complete(transaction)
            
            # å‘å¸ƒäº‹åŠ¡å®Œæˆäº‹ä»¶
            await self._publish_transaction_event(transaction, EventType.TRANSACTION_COMMIT)
            
            logger.info(f"âœ… åˆ†å¸ƒå¼äº‹åŠ¡æ‰§è¡ŒæˆåŠŸ: {transaction.transaction_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ†å¸ƒå¼äº‹åŠ¡æ‰§è¡Œå¼‚å¸¸: {transaction.transaction_id}, é”™è¯¯: {e}")
            
            transaction.state = TransactionState.FAILED
            transaction.end_time = datetime.now(timezone.utc)
            transaction.error_message = str(e)
            
            # æ‰§è¡Œè¡¥å¿
            await self._compensate_transaction(transaction, executed_steps)
            return False
    
    async def _execute_step_with_retry(
        self, 
        step: TransactionStep, 
        context: Dict[str, Any]
    ) -> Tuple[bool, Any]:
        """å¸¦é‡è¯•çš„æ­¥éª¤æ‰§è¡Œ"""
        for attempt in range(step.max_retries + 1):
            try:
                # æ‰§è¡Œæ­¥éª¤
                if asyncio.iscoroutinefunction(step.action):
                    result = await step.action(context)
                else:
                    result = step.action(context)
                
                return True, result
                
            except Exception as e:
                step.retry_count = attempt + 1
                
                if attempt < step.max_retries:
                    logger.warning(f"âš ï¸ æ­¥éª¤æ‰§è¡Œå¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {step.step_name}, ç¬¬{attempt + 1}æ¬¡é‡è¯•")
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œæœ€ç»ˆå¤±è´¥: {step.step_name}, é”™è¯¯: {e}")
                    return False, str(e)
        
        return False, "é‡è¯•æ¬¡æ•°è¶…é™"
    
    async def _compensate_transaction(
        self, 
        transaction: DistributedTransaction, 
        executed_steps: List[TransactionStep]
    ):
        """è¡¥å¿äº‹åŠ¡"""
        logger.info(f"ğŸ”„ å¼€å§‹è¡¥å¿äº‹åŠ¡: {transaction.transaction_id}")
        
        transaction.state = TransactionState.COMPENSATING
        
        # æŒ‰ç›¸åé¡ºåºæ‰§è¡Œè¡¥å¿
        for step in reversed(executed_steps):
            if step.compensation:
                try:
                    logger.debug(f"ğŸ”„ æ‰§è¡Œè¡¥å¿: {step.step_name}")
                    
                    if asyncio.iscoroutinefunction(step.compensation):
                        await step.compensation(transaction.context)
                    else:
                        step.compensation(transaction.context)
                    
                    logger.debug(f"âœ… è¡¥å¿æ‰§è¡ŒæˆåŠŸ: {step.step_name}")
                    
                except Exception as e:
                    logger.error(f"âŒ è¡¥å¿æ‰§è¡Œå¤±è´¥: {step.step_name}, é”™è¯¯: {e}")
        
        transaction.state = TransactionState.COMPENSATED
        transaction.end_time = datetime.now(timezone.utc)
        
        # è®°å½•äº‹åŠ¡è¡¥å¿å®Œæˆ
        with self.Session() as session:
            logger = TransactionLogger(self.redis, session)
            logger.log_transaction_complete(transaction)
        
        # å‘å¸ƒè¡¥å¿äº‹ä»¶
        await self._publish_transaction_event(transaction, EventType.COMPENSATION)
        
        logger.info(f"âœ… äº‹åŠ¡è¡¥å¿å®Œæˆ: {transaction.transaction_id}")
    
    def _sort_steps_by_dependencies(self, steps: List[TransactionStep]) -> List[TransactionStep]:
        """æŒ‰ä¾èµ–å…³ç³»æ’åºæ­¥éª¤ï¼ˆæ‹“æ‰‘æ’åºï¼‰"""
        # ç®€åŒ–å®ç°ï¼šå¦‚æœæ²¡æœ‰ä¾èµ–å…³ç³»ï¼ŒæŒ‰åŸé¡ºåºè¿”å›
        # å®é™…å®ç°åº”è¯¥åŒ…å«å®Œæ•´çš„æ‹“æ‰‘æ’åºç®—æ³•
        return steps
    
    async def _publish_transaction_event(
        self, 
        transaction: DistributedTransaction, 
        event_type: EventType
    ):
        """å‘å¸ƒäº‹åŠ¡äº‹ä»¶"""
        try:
            event = EventMessage(
                event_id=str(uuid.uuid4()),
                event_type=event_type,
                customer_id=transaction.customer_id,
                payload={
                    'transaction_id': transaction.transaction_id,
                    'transaction_type': transaction.transaction_type,
                    'state': transaction.state.value,
                    'context': transaction.context
                }
            )
            
            self.event_publisher.publish_event(event)
            
            # è§¦å‘ç¼“å­˜å¤±æ•ˆ
            self.cache_manager.invalidate_by_event(
                f"{transaction.transaction_type}_{event_type.value}",
                transaction.customer_id,
                transaction.context
            )
            
        except Exception as e:
            logger.error(f"âŒ äº‹åŠ¡äº‹ä»¶å‘å¸ƒå¤±è´¥: {e}")
    
    def get_transaction_status(self, transaction_id: str) -> Dict[str, Any]:
        """è·å–äº‹åŠ¡çŠ¶æ€"""
        try:
            # ä»Redisè·å–å¿«é€ŸçŠ¶æ€
            transaction_data = self.redis.hgetall(f"transaction_log:{transaction_id}")
            
            if transaction_data:
                # è·å–æ­¥éª¤è¯¦æƒ…
                steps_data = self.redis.lrange(f"transaction_steps:{transaction_id}", 0, -1)
                steps = [json.loads(step) for step in steps_data]
                
                return {
                    'transaction_id': transaction_id,
                    'state': transaction_data.get('state'),
                    'start_time': transaction_data.get('start_time'),
                    'end_time': transaction_data.get('end_time'),
                    'duration_ms': transaction_data.get('duration_ms'),
                    'error_message': transaction_data.get('error_message'),
                    'steps': steps
                }
            
            return {'error': 'Transaction not found'}
            
        except Exception as e:
            logger.error(f"âŒ è·å–äº‹åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return {'error': str(e)}


# ==================== å·¥å‚å‡½æ•° ====================

def create_distributed_transaction_manager(
    redis_client: Redis,
    db_engine: Engine
) -> DistributedTransactionManager:
    """åˆ›å»ºåˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†å™¨"""
    
    # åˆ›å»ºäº‹ä»¶å‘å¸ƒå™¨
    event_publisher = EventPublisher(redis_client)
    
    # åˆ›å»ºäº‹åŠ¡ç®¡ç†å™¨
    manager = DistributedTransactionManager(redis_client, db_engine, event_publisher)
    
    return manager


def create_event_consumer(
    redis_client: Redis,
    db_engine: Engine,
    consumer_name: str = None
) -> EventConsumer:
    """åˆ›å»ºäº‹ä»¶æ¶ˆè´¹å™¨"""
    
    if not consumer_name:
        consumer_name = f"consumer_{uuid.uuid4().hex[:8]}"
    
    # åˆ›å»ºç»„ä»¶
    event_publisher = EventPublisher(redis_client)
    cache_manager = CacheConsistencyManager(redis_client, event_publisher)
    
    with sessionmaker(bind=db_engine)() as session:
        transaction_logger = TransactionLogger(redis_client, session)
    
    # åˆ›å»ºæ¶ˆè´¹å™¨
    consumer = EventConsumer(
        redis_client, 
        consumer_name,
        cache_manager,
        transaction_logger
    )
    
    return consumer


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    import redis
    from sqlalchemy import create_engine
    
    # åˆ›å»ºRediså®¢æˆ·ç«¯
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    
    # åˆ›å»ºæ•°æ®åº“å¼•æ“
    db_engine = create_engine('sqlite:///distributed_transaction.db', echo=True)
    
    # åˆ›å»ºåˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†å™¨
    manager = create_distributed_transaction_manager(redis_client, db_engine)
    
    print("âœ… åˆ†å¸ƒå¼äº‹åŠ¡å’Œç¼“å­˜ä¸€è‡´æ€§ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
    print("ä¸»è¦åŠŸèƒ½:")
    print("1. ğŸ”„ åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒï¼ˆSagaæ¨¡å¼ï¼‰")
    print("2. ğŸ’¾ æœ€ç»ˆä¸€è‡´æ€§ä¿è¯")
    print("3. ğŸ—„ï¸ ç¼“å­˜å¤±æ•ˆç­–ç•¥")
    print("4. ğŸ”§ è¡¥å¿æœºåˆ¶")
    print("5. ğŸ“¡ äº‹ä»¶é©±åŠ¨æ¶æ„")
    print("6. â˜ ï¸ æ­»ä¿¡é˜Ÿåˆ—å¤„ç†")