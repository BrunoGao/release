#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¶ˆæ¯ç³»ç»ŸV2æ•°æ®åº“è¿ç§»è„šæœ¬

ä¸»è¦åŠŸèƒ½:
1. æ•°æ®åº“è¡¨ç»“æ„å‡çº§
2. æ•°æ®è¿ç§»å’Œè½¬æ¢
3. ç´¢å¼•ä¼˜åŒ–å’Œåˆ†åŒºä¿®å¤
4. çº¦æŸæ·»åŠ å’ŒéªŒè¯
5. å¤‡ä»½å’Œå›æ»šæ”¯æŒ
6. ç‰ˆæœ¬ç®¡ç†

è¿ç§»å†…å®¹:
- ä¿®å¤åˆ†åŒºå‡½æ•°é—®é¢˜
- æ·»åŠ ç¼ºå¤±çš„å…³é”®ç´¢å¼•
- åˆ›å»ºæ–°çš„V2è¡¨ç»“æ„
- æ•°æ®å¹³æ»‘è¿ç§»
- æ€§èƒ½éªŒè¯

@Author: brunoGao
@CreateTime: 2025-09-11
@Version: 2.0-Fixed
"""

import os
import sys
import json
import time
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone
import traceback
from pathlib import Path

from sqlalchemy import create_engine, text, MetaData, inspect
from sqlalchemy.engine import Engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError
import pymysql

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from models.message_v2_fixed_model import (
    create_fixed_tables, 
    Base,
    DatabaseFixGenerator,
    validate_database_performance
)

logger = logging.getLogger(__name__)


# ==================== è¿ç§»é…ç½® ====================

class MigrationConfig:
    """è¿ç§»é…ç½®"""
    
    def __init__(self, config_dict: Dict[str, Any]):
        # æ•°æ®åº“é…ç½®
        self.database_url = config_dict.get('database_url', 'mysql://localhost:3306/ljwx')
        self.backup_enabled = config_dict.get('backup_enabled', True)
        self.backup_directory = config_dict.get('backup_directory', './backups')
        
        # è¿ç§»é…ç½®
        self.batch_size = config_dict.get('batch_size', 1000)
        self.timeout_seconds = config_dict.get('timeout_seconds', 3600)  # 1å°æ—¶
        self.dry_run = config_dict.get('dry_run', False)
        self.force_migration = config_dict.get('force_migration', False)
        
        # éªŒè¯é…ç½®
        self.skip_validation = config_dict.get('skip_validation', False)
        self.performance_check = config_dict.get('performance_check', True)
        
        # æ—¥å¿—é…ç½®
        self.log_level = config_dict.get('log_level', 'INFO')
        self.log_file = config_dict.get('log_file', 'migration.log')


# ==================== è¿ç§»çŠ¶æ€ç®¡ç† ====================

class MigrationTracker:
    """è¿ç§»çŠ¶æ€è·Ÿè¸ªå™¨"""
    
    def __init__(self, engine: Engine):
        self.engine = engine
        self.session_factory = sessionmaker(bind=engine)
        self._ensure_migration_table()
    
    def _ensure_migration_table(self):
        """ç¡®ä¿è¿ç§»è¡¨å­˜åœ¨"""
        create_sql = """
        CREATE TABLE IF NOT EXISTS migration_history (
            id INT AUTO_INCREMENT PRIMARY KEY,
            migration_name VARCHAR(255) NOT NULL UNIQUE,
            migration_version VARCHAR(50) NOT NULL,
            status ENUM('started', 'completed', 'failed', 'rolled_back') NOT NULL,
            start_time DATETIME NOT NULL,
            end_time DATETIME NULL,
            error_message TEXT NULL,
            metadata JSON NULL,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """
        
        with self.engine.connect() as conn:
            conn.execute(text(create_sql))
            conn.commit()
    
    def start_migration(self, migration_name: str, migration_version: str, metadata: Dict[str, Any] = None):
        """è®°å½•è¿ç§»å¼€å§‹"""
        with self.session_factory() as session:
            session.execute(
                text("""
                    INSERT INTO migration_history 
                    (migration_name, migration_version, status, start_time, metadata)
                    VALUES (:name, :version, 'started', :start_time, :metadata)
                    ON DUPLICATE KEY UPDATE
                    status = 'started',
                    start_time = :start_time,
                    end_time = NULL,
                    error_message = NULL,
                    metadata = :metadata,
                    updated_at = CURRENT_TIMESTAMP
                """),
                {
                    'name': migration_name,
                    'version': migration_version,
                    'start_time': datetime.now(timezone.utc),
                    'metadata': json.dumps(metadata or {})
                }
            )
            session.commit()
    
    def complete_migration(self, migration_name: str, metadata: Dict[str, Any] = None):
        """è®°å½•è¿ç§»å®Œæˆ"""
        with self.session_factory() as session:
            session.execute(
                text("""
                    UPDATE migration_history 
                    SET status = 'completed',
                        end_time = :end_time,
                        metadata = :metadata,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE migration_name = :name
                """),
                {
                    'name': migration_name,
                    'end_time': datetime.now(timezone.utc),
                    'metadata': json.dumps(metadata or {})
                }
            )
            session.commit()
    
    def fail_migration(self, migration_name: str, error_message: str, metadata: Dict[str, Any] = None):
        """è®°å½•è¿ç§»å¤±è´¥"""
        with self.session_factory() as session:
            session.execute(
                text("""
                    UPDATE migration_history 
                    SET status = 'failed',
                        end_time = :end_time,
                        error_message = :error_message,
                        metadata = :metadata,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE migration_name = :name
                """),
                {
                    'name': migration_name,
                    'end_time': datetime.now(timezone.utc),
                    'error_message': error_message,
                    'metadata': json.dumps(metadata or {})
                }
            )
            session.commit()
    
    def is_migration_completed(self, migration_name: str) -> bool:
        """æ£€æŸ¥è¿ç§»æ˜¯å¦å·²å®Œæˆ"""
        with self.session_factory() as session:
            result = session.execute(
                text("SELECT status FROM migration_history WHERE migration_name = :name"),
                {'name': migration_name}
            ).fetchone()
            
            return result and result[0] == 'completed'
    
    def get_migration_history(self) -> List[Dict[str, Any]]:
        """è·å–è¿ç§»å†å²"""
        with self.session_factory() as session:
            result = session.execute(
                text("SELECT * FROM migration_history ORDER BY created_at DESC")
            ).fetchall()
            
            return [dict(row._mapping) for row in result]


# ==================== æ•°æ®åº“å¤‡ä»½ç®¡ç†å™¨ ====================

class DatabaseBackupManager:
    """æ•°æ®åº“å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self, config: MigrationConfig):
        self.config = config
        self.backup_dir = Path(config.backup_directory)
        self.backup_dir.mkdir(exist_ok=True)
    
    def create_backup(self, backup_name: str = None) -> str:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        if not self.config.backup_enabled:
            logger.info("âš ï¸ å¤‡ä»½åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡å¤‡ä»½")
            return ""
        
        if not backup_name:
            backup_name = f"migration_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
        
        backup_path = self.backup_dir / backup_name
        
        try:
            # è§£ææ•°æ®åº“URL
            db_config = self._parse_database_url()
            
            # æ„å»ºmysqldumpå‘½ä»¤
            cmd = [
                'mysqldump',
                f"-h{db_config['host']}",
                f"-P{db_config['port']}",
                f"-u{db_config['username']}",
                f"-p{db_config['password']}",
                '--single-transaction',
                '--routines',
                '--triggers',
                '--events',
                '--set-gtid-purged=OFF',
                db_config['database']
            ]
            
            # æ‰§è¡Œå¤‡ä»½
            import subprocess
            with open(backup_path, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)
            
            if result.returncode != 0:
                raise Exception(f"mysqldump failed: {result.stderr}")
            
            logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
            return str(backup_path)
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            raise
    
    def restore_backup(self, backup_path: str):
        """æ¢å¤æ•°æ®åº“å¤‡ä»½"""
        backup_file = Path(backup_path)
        
        if not backup_file.exists():
            raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
        
        try:
            # è§£ææ•°æ®åº“URL
            db_config = self._parse_database_url()
            
            # æ„å»ºmysqlå‘½ä»¤
            cmd = [
                'mysql',
                f"-h{db_config['host']}",
                f"-P{db_config['port']}",
                f"-u{db_config['username']}",
                f"-p{db_config['password']}",
                db_config['database']
            ]
            
            # æ‰§è¡Œæ¢å¤
            import subprocess
            with open(backup_path, 'r') as f:
                result = subprocess.run(cmd, stdin=f, stderr=subprocess.PIPE, text=True)
            
            if result.returncode != 0:
                raise Exception(f"mysql restore failed: {result.stderr}")
            
            logger.info(f"âœ… æ•°æ®åº“æ¢å¤æˆåŠŸ: {backup_path}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æ¢å¤å¤±è´¥: {e}")
            raise
    
    def _parse_database_url(self) -> Dict[str, str]:
        """è§£ææ•°æ®åº“URL"""
        from urllib.parse import urlparse
        
        parsed = urlparse(self.config.database_url)
        
        return {
            'host': parsed.hostname or 'localhost',
            'port': str(parsed.port or 3306),
            'username': parsed.username or 'root',
            'password': parsed.password or '',
            'database': parsed.path.lstrip('/') or 'ljwx'
        }


# ==================== ä¸»è¿ç§»å™¨ ====================

class MessageV2Migrator:
    """æ¶ˆæ¯ç³»ç»ŸV2ä¸»è¿ç§»å™¨"""
    
    def __init__(self, config: MigrationConfig):
        self.config = config
        self.engine = create_engine(config.database_url)
        self.session_factory = sessionmaker(bind=self.engine)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tracker = MigrationTracker(self.engine)
        self.backup_manager = DatabaseBackupManager(config)
        self.fix_generator = DatabaseFixGenerator()
        
        # è¿ç§»æ­¥éª¤å®šä¹‰
        self.migration_steps = [
            {
                'name': 'pre_migration_check',
                'description': 'è¿ç§»å‰æ£€æŸ¥',
                'function': self._pre_migration_check,
                'required': True
            },
            {
                'name': 'create_backup',
                'description': 'åˆ›å»ºæ•°æ®åº“å¤‡ä»½',
                'function': self._create_backup,
                'required': False
            },
            {
                'name': 'fix_partition_functions',
                'description': 'ä¿®å¤åˆ†åŒºå‡½æ•°',
                'function': self._fix_partition_functions,
                'required': True
            },
            {
                'name': 'add_missing_indexes',
                'description': 'æ·»åŠ ç¼ºå¤±ç´¢å¼•',
                'function': self._add_missing_indexes,
                'required': True
            },
            {
                'name': 'create_v2_tables',
                'description': 'åˆ›å»ºV2è¡¨ç»“æ„',
                'function': self._create_v2_tables,
                'required': True
            },
            {
                'name': 'migrate_existing_data',
                'description': 'è¿ç§»ç°æœ‰æ•°æ®',
                'function': self._migrate_existing_data,
                'required': True
            },
            {
                'name': 'add_constraints',
                'description': 'æ·»åŠ çº¦æŸ',
                'function': self._add_constraints,
                'required': True
            },
            {
                'name': 'validate_migration',
                'description': 'éªŒè¯è¿ç§»ç»“æœ',
                'function': self._validate_migration,
                'required': True
            },
            {
                'name': 'performance_check',
                'description': 'æ€§èƒ½æ£€æŸ¥',
                'function': self._performance_check,
                'required': False
            },
            {
                'name': 'cleanup',
                'description': 'æ¸…ç†ä¸´æ—¶æ•°æ®',
                'function': self._cleanup,
                'required': False
            }
        ]
        
        logger.info("âœ… MessageV2è¿ç§»å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def run_migration(self) -> bool:
        """è¿è¡Œå®Œæ•´è¿ç§»"""
        migration_name = "message_system_v2_migration"
        migration_version = "2.0-Fixed"
        
        logger.info(f"ğŸš€ å¼€å§‹æ¶ˆæ¯ç³»ç»ŸV2è¿ç§»: {migration_name}")
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if self.tracker.is_migration_completed(migration_name) and not self.config.force_migration:
            logger.info("âœ… è¿ç§»å·²å®Œæˆï¼Œè·³è¿‡æ‰§è¡Œ")
            return True
        
        # å¼€å§‹è¿ç§»
        start_time = time.time()
        self.tracker.start_migration(migration_name, migration_version, {
            'config': vars(self.config),
            'steps': [step['name'] for step in self.migration_steps]
        })
        
        backup_path = ""
        success = False
        
        try:
            # é€æ­¥æ‰§è¡Œè¿ç§»
            for i, step in enumerate(self.migration_steps):
                step_name = step['name']
                step_desc = step['description']
                step_func = step['function']
                is_required = step['required']
                
                logger.info(f"ğŸ“‹ æ‰§è¡Œè¿ç§»æ­¥éª¤ {i+1}/{len(self.migration_steps)}: {step_desc}")
                
                try:
                    if self.config.dry_run and step_name not in ['pre_migration_check', 'validate_migration']:
                        logger.info(f"ğŸ” DRY RUNæ¨¡å¼: è·³è¿‡æ­¥éª¤ {step_name}")
                        continue
                    
                    # æ‰§è¡Œæ­¥éª¤
                    step_result = step_func()
                    
                    # ä¿å­˜å¤‡ä»½è·¯å¾„
                    if step_name == 'create_backup' and step_result:
                        backup_path = step_result
                    
                    logger.info(f"âœ… æ­¥éª¤å®Œæˆ: {step_desc}")
                    
                except Exception as e:
                    logger.error(f"âŒ æ­¥éª¤å¤±è´¥: {step_desc}, é”™è¯¯: {e}")
                    
                    if is_required:
                        # å¿…éœ€æ­¥éª¤å¤±è´¥ï¼Œä¸­æ­¢è¿ç§»
                        raise Exception(f"å¿…éœ€æ­¥éª¤å¤±è´¥: {step_name}, é”™è¯¯: {e}")
                    else:
                        # å¯é€‰æ­¥éª¤å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ
                        logger.warning(f"âš ï¸ å¯é€‰æ­¥éª¤å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: {step_name}")
            
            # æ‰€æœ‰æ­¥éª¤å®Œæˆ
            duration = time.time() - start_time
            
            self.tracker.complete_migration(migration_name, {
                'duration_seconds': duration,
                'backup_path': backup_path,
                'dry_run': self.config.dry_run
            })
            
            logger.info(f"ğŸ‰ æ¶ˆæ¯ç³»ç»ŸV2è¿ç§»å®Œæˆ! è€—æ—¶: {duration:.2f}ç§’")
            success = True
            
            return True
            
        except Exception as e:
            # è¿ç§»å¤±è´¥
            duration = time.time() - start_time
            error_msg = f"è¿ç§»å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            
            self.tracker.fail_migration(migration_name, error_msg, {
                'duration_seconds': duration,
                'backup_path': backup_path,
                'dry_run': self.config.dry_run
            })
            
            logger.error(f"âŒ æ¶ˆæ¯ç³»ç»ŸV2è¿ç§»å¤±è´¥: {e}")
            
            # å¦‚æœæœ‰å¤‡ä»½ï¼Œè¯¢é—®æ˜¯å¦å›æ»š
            if backup_path and not self.config.dry_run:
                logger.info(f"ğŸ’¡ å¯ä½¿ç”¨å¤‡ä»½å›æ»š: python {__file__} --rollback {backup_path}")
            
            return False
    
    # ==================== è¿ç§»æ­¥éª¤å®ç° ====================
    
    def _pre_migration_check(self) -> bool:
        """è¿ç§»å‰æ£€æŸ¥"""
        logger.info("ğŸ” æ‰§è¡Œè¿ç§»å‰æ£€æŸ¥...")
        
        checks = []
        
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text('SELECT 1'))
            checks.append(("æ•°æ®åº“è¿æ¥", True, ""))
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            inspector = inspect(self.engine)
            existing_tables = inspector.get_table_names()
            
            required_tables = ['t_device_message', 't_device_message_v2']
            missing_tables = [table for table in required_tables if table not in existing_tables]
            
            if missing_tables:
                checks.append(("å¿…éœ€è¡¨æ£€æŸ¥", False, f"ç¼ºå¤±è¡¨: {missing_tables}"))
            else:
                checks.append(("å¿…éœ€è¡¨æ£€æŸ¥", True, ""))
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            import shutil
            free_space = shutil.disk_usage('/').free
            free_gb = free_space / (1024**3)
            
            if free_gb < 1.0:  # è‡³å°‘1GBç©ºé—²ç©ºé—´
                checks.append(("ç£ç›˜ç©ºé—´", False, f"ç©ºé—²ç©ºé—´ä¸è¶³: {free_gb:.1f}GB"))
            else:
                checks.append(("ç£ç›˜ç©ºé—´", True, f"ç©ºé—²ç©ºé—´: {free_gb:.1f}GB"))
            
            # æ£€æŸ¥æƒé™
            try:
                with self.engine.connect() as conn:
                    conn.execute(text('CREATE TEMPORARY TABLE test_permissions (id INT)'))
                    conn.execute(text('DROP TEMPORARY TABLE test_permissions'))
                checks.append(("æ•°æ®åº“æƒé™", True, ""))
            except Exception as e:
                checks.append(("æ•°æ®åº“æƒé™", False, str(e)))
            
            # è¾“å‡ºæ£€æŸ¥ç»“æœ
            all_passed = True
            for check_name, passed, message in checks:
                status = "âœ…" if passed else "âŒ"
                logger.info(f"{status} {check_name}: {message}")
                if not passed:
                    all_passed = False
            
            if not all_passed:
                raise Exception("è¿ç§»å‰æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è¿ç§»")
            
            logger.info("âœ… è¿ç§»å‰æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»å‰æ£€æŸ¥å¤±è´¥: {e}")
            raise
    
    def _create_backup(self) -> str:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        if not self.config.backup_enabled:
            logger.info("âš ï¸ å¤‡ä»½åŠŸèƒ½å·²ç¦ç”¨")
            return ""
        
        logger.info("ğŸ’¾ åˆ›å»ºæ•°æ®åº“å¤‡ä»½...")
        backup_path = self.backup_manager.create_backup()
        logger.info(f"âœ… å¤‡ä»½åˆ›å»ºå®Œæˆ: {backup_path}")
        
        return backup_path
    
    def _fix_partition_functions(self) -> bool:
        """ä¿®å¤åˆ†åŒºå‡½æ•°"""
        logger.info("ğŸ”§ ä¿®å¤åˆ†åŒºå‡½æ•°...")
        
        try:
            partition_sql = self.fix_generator.generate_partition_fix_sql()
            
            if self.config.dry_run:
                logger.info(f"ğŸ” DRY RUN - åˆ†åŒºä¿®å¤SQL:\n{partition_sql}")
                return True
            
            # æ‰§è¡Œåˆ†åŒºä¿®å¤
            with self.engine.connect() as conn:
                # åˆ†æ‰¹æ‰§è¡ŒSQLè¯­å¥
                statements = [stmt.strip() for stmt in partition_sql.split(';') if stmt.strip()]
                
                for stmt in statements:
                    if stmt:
                        logger.debug(f"æ‰§è¡ŒSQL: {stmt[:100]}...")
                        conn.execute(text(stmt))
                
                conn.commit()
            
            logger.info("âœ… åˆ†åŒºå‡½æ•°ä¿®å¤å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ†åŒºå‡½æ•°ä¿®å¤å¤±è´¥: {e}")
            raise
    
    def _add_missing_indexes(self) -> bool:
        """æ·»åŠ ç¼ºå¤±ç´¢å¼•"""
        logger.info("ğŸ“Š æ·»åŠ ç¼ºå¤±ç´¢å¼•...")
        
        try:
            index_sql = self.fix_generator.generate_index_fix_sql()
            
            if self.config.dry_run:
                logger.info(f"ğŸ” DRY RUN - ç´¢å¼•ä¿®å¤SQL:\n{index_sql}")
                return True
            
            # æ‰§è¡Œç´¢å¼•æ·»åŠ 
            with self.engine.connect() as conn:
                statements = [stmt.strip() for stmt in index_sql.split(';') if stmt.strip()]
                
                for stmt in statements:
                    if stmt:
                        try:
                            logger.debug(f"æ‰§è¡ŒSQL: {stmt[:100]}...")
                            conn.execute(text(stmt))
                        except SQLAlchemyError as e:
                            # ç´¢å¼•å¯èƒ½å·²å­˜åœ¨ï¼Œè®°å½•ä½†ä¸ä¸­æ–­
                            logger.warning(f"âš ï¸ ç´¢å¼•æ·»åŠ è­¦å‘Š: {e}")
                
                conn.commit()
            
            logger.info("âœ… ç¼ºå¤±ç´¢å¼•æ·»åŠ å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç¼ºå¤±ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def _create_v2_tables(self) -> bool:
        """åˆ›å»ºV2è¡¨ç»“æ„"""
        logger.info("ğŸ—ï¸ åˆ›å»ºV2è¡¨ç»“æ„...")
        
        try:
            if self.config.dry_run:
                logger.info("ğŸ” DRY RUN - è·³è¿‡V2è¡¨åˆ›å»º")
                return True
            
            # åˆ›å»ºV2è¡¨
            create_fixed_tables(self.engine)
            
            logger.info("âœ… V2è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ V2è¡¨ç»“æ„åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def _migrate_existing_data(self) -> bool:
        """è¿ç§»ç°æœ‰æ•°æ®"""
        logger.info("ğŸ”„ è¿ç§»ç°æœ‰æ•°æ®...")
        
        try:
            if self.config.dry_run:
                logger.info("ğŸ” DRY RUN - è·³è¿‡æ•°æ®è¿ç§»")
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®éœ€è¦è¿ç§»
            with self.engine.connect() as conn:
                # æ£€æŸ¥æ—§è¡¨æ•°æ®é‡
                result = conn.execute(text("""
                    SELECT COUNT(*) as count 
                    FROM information_schema.tables 
                    WHERE table_schema = DATABASE() 
                    AND table_name = 't_device_message'
                """)).fetchone()
                
                if not result or result[0] == 0:
                    logger.info("ğŸ“­ æœªæ‰¾åˆ°éœ€è¦è¿ç§»çš„æ•°æ®")
                    return True
                
                # è·å–æ—§æ•°æ®æ€»æ•°
                old_count = conn.execute(text("SELECT COUNT(*) FROM t_device_message")).scalar()
                logger.info(f"ğŸ“Š å‘ç°å¾…è¿ç§»æ•°æ®: {old_count}æ¡")
                
                if old_count == 0:
                    logger.info("ğŸ“­ æ— æ•°æ®éœ€è¦è¿ç§»")
                    return True
                
                # åˆ†æ‰¹è¿ç§»æ•°æ®
                migrated_count = 0
                batch_size = self.config.batch_size
                
                while migrated_count < old_count:
                    # è·å–ä¸€æ‰¹æ•°æ®
                    batch_data = conn.execute(text(f"""
                        SELECT * FROM t_device_message 
                        WHERE id > :offset 
                        ORDER BY id 
                        LIMIT :limit
                    """), {'offset': migrated_count, 'limit': batch_size}).fetchall()
                    
                    if not batch_data:
                        break
                    
                    # è½¬æ¢å¹¶æ’å…¥åˆ°V2è¡¨
                    for row in batch_data:
                        self._migrate_single_message(conn, row)
                    
                    migrated_count += len(batch_data)
                    progress = (migrated_count / old_count) * 100
                    
                    logger.info(f"ğŸ“ˆ æ•°æ®è¿ç§»è¿›åº¦: {migrated_count}/{old_count} ({progress:.1f}%)")
                
                conn.commit()
            
            logger.info(f"âœ… æ•°æ®è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_count} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {e}")
            raise
    
    def _migrate_single_message(self, conn, old_row):
        """è¿ç§»å•æ¡æ¶ˆæ¯æ•°æ®"""
        try:
            # è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆæ ¹æ®å®é™…è¡¨ç»“æ„è°ƒæ•´ï¼‰
            new_data = {
                'customer_id': old_row.customer_id if hasattr(old_row, 'customer_id') else 1,
                'department_id': old_row.department_id if hasattr(old_row, 'department_id') else 1,
                'user_id': old_row.user_id if hasattr(old_row, 'user_id') else None,
                'device_sn': old_row.device_sn if hasattr(old_row, 'device_sn') else '',
                'title': old_row.title if hasattr(old_row, 'title') else '',
                'message': old_row.message if hasattr(old_row, 'message') else '',
                'message_type': old_row.message_type if hasattr(old_row, 'message_type') else 'notification',
                'sender_type': old_row.sender_type if hasattr(old_row, 'sender_type') else 'system',
                'receiver_type': old_row.receiver_type if hasattr(old_row, 'receiver_type') else 'user',
                'priority_level': old_row.priority_level if hasattr(old_row, 'priority_level') else 3,
                'urgency': 'medium',  # é»˜è®¤å€¼
                'message_status': old_row.message_status if hasattr(old_row, 'message_status') else 'pending',
                'create_time': old_row.create_time if hasattr(old_row, 'create_time') else datetime.now(timezone.utc),
                'channels': '["message"]',  # JSONæ ¼å¼
                'require_ack': False,
                'metadata': '{}'  # JSONæ ¼å¼
            }
            
            # æ’å…¥åˆ°V2è¡¨
            conn.execute(text("""
                INSERT INTO t_device_message_v2 
                (customer_id, department_id, user_id, device_sn, title, message, 
                 message_type, sender_type, receiver_type, priority_level, urgency,
                 message_status, create_time, channels, require_ack, metadata)
                VALUES 
                (:customer_id, :department_id, :user_id, :device_sn, :title, :message,
                 :message_type, :sender_type, :receiver_type, :priority_level, :urgency,
                 :message_status, :create_time, :channels, :require_ack, :metadata)
            """), new_data)
            
        except Exception as e:
            logger.error(f"âŒ å•æ¡æ¶ˆæ¯è¿ç§»å¤±è´¥: {e}, æ•°æ®: {old_row}")
            # å¯ä»¥é€‰æ‹©è·³è¿‡æˆ–ä¸­æ–­
            raise
    
    def _add_constraints(self) -> bool:
        """æ·»åŠ çº¦æŸ"""
        logger.info("ğŸ”— æ·»åŠ çº¦æŸ...")
        
        try:
            constraint_sql = self.fix_generator.generate_constraint_fix_sql()
            
            if self.config.dry_run:
                logger.info(f"ğŸ” DRY RUN - çº¦æŸSQL:\n{constraint_sql}")
                return True
            
            # æ‰§è¡Œçº¦æŸæ·»åŠ 
            with self.engine.connect() as conn:
                statements = [stmt.strip() for stmt in constraint_sql.split(';') if stmt.strip()]
                
                for stmt in statements:
                    if stmt:
                        try:
                            logger.debug(f"æ‰§è¡ŒSQL: {stmt[:100]}...")
                            conn.execute(text(stmt))
                        except SQLAlchemyError as e:
                            # çº¦æŸå¯èƒ½å·²å­˜åœ¨
                            logger.warning(f"âš ï¸ çº¦æŸæ·»åŠ è­¦å‘Š: {e}")
                
                conn.commit()
            
            logger.info("âœ… çº¦æŸæ·»åŠ å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ çº¦æŸå¤±è´¥: {e}")
            raise
    
    def _validate_migration(self) -> bool:
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info("ğŸ” éªŒè¯è¿ç§»ç»“æœ...")
        
        if self.config.skip_validation:
            logger.info("âš ï¸ è·³è¿‡è¿ç§»éªŒè¯")
            return True
        
        try:
            validation_results = []
            
            with self.engine.connect() as conn:
                # 1. æ£€æŸ¥è¡¨ç»“æ„
                inspector = inspect(self.engine)
                v2_tables = [
                    't_device_message_v2',
                    't_device_message_detail_v2',
                    't_message_lifecycle_v2'
                ]
                
                for table_name in v2_tables:
                    if table_name in inspector.get_table_names():
                        validation_results.append((f"è¡¨å­˜åœ¨æ£€æŸ¥: {table_name}", True, ""))
                    else:
                        validation_results.append((f"è¡¨å­˜åœ¨æ£€æŸ¥: {table_name}", False, "è¡¨ä¸å­˜åœ¨"))
                
                # 2. æ£€æŸ¥ç´¢å¼•
                for table_name in v2_tables:
                    try:
                        indexes = inspector.get_indexes(table_name)
                        index_count = len(indexes)
                        validation_results.append((f"ç´¢å¼•æ£€æŸ¥: {table_name}", index_count > 0, f"ç´¢å¼•æ•°é‡: {index_count}"))
                    except:
                        validation_results.append((f"ç´¢å¼•æ£€æŸ¥: {table_name}", False, "è·å–ç´¢å¼•ä¿¡æ¯å¤±è´¥"))
                
                # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
                if 't_device_message' in inspector.get_table_names():
                    old_count = conn.execute(text("SELECT COUNT(*) FROM t_device_message")).scalar()
                    new_count = conn.execute(text("SELECT COUNT(*) FROM t_device_message_v2")).scalar()
                    
                    data_consistent = old_count == new_count
                    validation_results.append((
                        "æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥", 
                        data_consistent, 
                        f"æ—§è¡¨: {old_count}æ¡, æ–°è¡¨: {new_count}æ¡"
                    ))
                
                # 4. åŸºæœ¬æŸ¥è¯¢æµ‹è¯•
                try:
                    conn.execute(text("SELECT * FROM t_device_message_v2 LIMIT 1")).fetchone()
                    validation_results.append(("æŸ¥è¯¢æµ‹è¯•", True, ""))
                except Exception as e:
                    validation_results.append(("æŸ¥è¯¢æµ‹è¯•", False, str(e)))
            
            # è¾“å‡ºéªŒè¯ç»“æœ
            all_passed = True
            for check_name, passed, message in validation_results:
                status = "âœ…" if passed else "âŒ"
                logger.info(f"{status} {check_name}: {message}")
                if not passed:
                    all_passed = False
            
            if not all_passed:
                raise Exception("è¿ç§»éªŒè¯å¤±è´¥")
            
            logger.info("âœ… è¿ç§»éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»éªŒè¯å¤±è´¥: {e}")
            raise
    
    def _performance_check(self) -> bool:
        """æ€§èƒ½æ£€æŸ¥"""
        logger.info("âš¡ æ‰§è¡Œæ€§èƒ½æ£€æŸ¥...")
        
        if not self.config.performance_check:
            logger.info("âš ï¸ è·³è¿‡æ€§èƒ½æ£€æŸ¥")
            return True
        
        try:
            with self.session_factory() as session:
                # æ‰§è¡Œæ€§èƒ½éªŒè¯
                performance_results = validate_database_performance(session)
                
                # è¾“å‡ºç»“æœ
                for test_name, result in performance_results.items():
                    if isinstance(result, dict):
                        status = "âœ…" if result.get('status') == 'PASS' else "âŒ"
                        query_time = result.get('query_time_ms', 0)
                        expected = result.get('expected_max_ms', 0)
                        
                        logger.info(f"{status} {test_name}: {query_time:.2f}ms (æœŸæœ›: <{expected}ms)")
                    else:
                        logger.info(f"ğŸ“Š {test_name}: {result}")
                
                logger.info("âœ… æ€§èƒ½æ£€æŸ¥å®Œæˆ")
                return True
                
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ£€æŸ¥å¤±è´¥: {e}")
            # æ€§èƒ½æ£€æŸ¥å¤±è´¥ä¸ä¸­æ–­è¿ç§»
            return True
    
    def _cleanup(self) -> bool:
        """æ¸…ç†ä¸´æ—¶æ•°æ®"""
        logger.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ•°æ®...")
        
        try:
            if self.config.dry_run:
                logger.info("ğŸ” DRY RUN - è·³è¿‡æ¸…ç†")
                return True
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘
            # ä¾‹å¦‚ï¼šåˆ é™¤ä¸´æ—¶è¡¨ã€æ¸…ç†æ—¥å¿—ç­‰
            
            logger.info("âœ… æ¸…ç†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")
            # æ¸…ç†å¤±è´¥ä¸å½±å“è¿ç§»ç»“æœ
            return True
    
    # ==================== å›æ»šåŠŸèƒ½ ====================
    
    def rollback_migration(self, backup_path: str) -> bool:
        """å›æ»šè¿ç§»"""
        logger.info(f"ğŸ”„ å¼€å§‹å›æ»šè¿ç§»: {backup_path}")
        
        try:
            # æ¢å¤å¤‡ä»½
            self.backup_manager.restore_backup(backup_path)
            
            # æ›´æ–°è¿ç§»çŠ¶æ€
            self.tracker.fail_migration(
                "message_system_v2_migration",
                "æ‰‹åŠ¨å›æ»š",
                {'rollback_backup': backup_path}
            )
            
            logger.info("âœ… è¿ç§»å›æ»šå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»å›æ»šå¤±è´¥: {e}")
            return False
    
    def get_migration_status(self) -> Dict[str, Any]:
        """è·å–è¿ç§»çŠ¶æ€"""
        history = self.tracker.get_migration_history()
        
        return {
            'migration_history': history,
            'latest_migration': history[0] if history else None,
            'database_url': self.config.database_url,
            'backup_directory': self.config.backup_directory
        }


# ==================== å‘½ä»¤è¡Œå·¥å…· ====================

def main():
    """å‘½ä»¤è¡Œä¸»å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¶ˆæ¯ç³»ç»ŸV2æ•°æ®åº“è¿ç§»å·¥å…·')
    parser.add_argument('--config', type=str, default='migration_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--database-url', type=str, help='æ•°æ®åº“è¿æ¥URL')
    parser.add_argument('--dry-run', action='store_true', help='é¢„æ¼”æ¨¡å¼ï¼ˆä¸æ‰§è¡Œå®é™…æ“ä½œï¼‰')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶è¿ç§»ï¼ˆå³ä½¿å·²å®Œæˆï¼‰')
    parser.add_argument('--rollback', type=str, help='å›æ»šåˆ°æŒ‡å®šå¤‡ä»½')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹è¿ç§»çŠ¶æ€')
    parser.add_argument('--log-level', type=str, default='INFO', help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('migration.log')
        ]
    )
    
    # åŠ è½½é…ç½®
    config_dict = {}
    if os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.database_url:
        config_dict['database_url'] = args.database_url
    if args.dry_run:
        config_dict['dry_run'] = True
    if args.force:
        config_dict['force_migration'] = True
    
    config = MigrationConfig(config_dict)
    
    # åˆ›å»ºè¿ç§»å™¨
    migrator = MessageV2Migrator(config)
    
    try:
        if args.status:
            # æŸ¥çœ‹è¿ç§»çŠ¶æ€
            status = migrator.get_migration_status()
            print("ğŸ” è¿ç§»çŠ¶æ€:")
            print(json.dumps(status, indent=2, default=str, ensure_ascii=False))
            
        elif args.rollback:
            # æ‰§è¡Œå›æ»š
            success = migrator.rollback_migration(args.rollback)
            sys.exit(0 if success else 1)
            
        else:
            # æ‰§è¡Œè¿ç§»
            success = migrator.run_migration()
            sys.exit(0 if success else 1)
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸ è¿ç§»è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¿ç§»å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()