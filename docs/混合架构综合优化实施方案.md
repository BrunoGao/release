# æ··åˆæ¶æ„ç»¼åˆä¼˜åŒ–å®æ–½æ–¹æ¡ˆ

## æ–¹æ¡ˆæ¦‚è¿°

åŸºäºå¯¹ç°æœ‰ç³»ç»Ÿçš„æ·±åº¦åˆ†æï¼Œæœ¬æ–¹æ¡ˆæ•´åˆäº†è®¤è¯æœºåˆ¶ã€é…ç½®æœåŠ¡ã€è¿ç§»ç­–ç•¥ç­‰å…³é”®é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œè®¾è®¡äº†ä¸€ä¸ªå®Œæ•´çš„Java-Pythonæ··åˆæ¶æ„ä¼˜åŒ–å®æ–½è®¡åˆ’ï¼Œç›®æ ‡å®ç°5000+å¹¶å‘å¤„ç†èƒ½åŠ›ã€‚

## æ ¸å¿ƒæ¶æ„è®¾è®¡

### 1. æœ€ç»ˆç›®æ ‡æ¶æ„

```mermaid
graph TB
    subgraph "è®¾å¤‡å±‚"
        A[ljwx-watch] --> B[ljwx-phone]
        A --> C[å…¶ä»–IoTè®¾å¤‡]
    end
    
    subgraph "Javaæ•°æ®æµæœåŠ¡å±‚ (ljwx-boot)"
        D[è®¾å¤‡è®¤è¯æœåŠ¡]
        E[é…ç½®ç®¡ç†æœåŠ¡]
        F[æ•°æ®æ¥æ”¶æœåŠ¡] 
        G[æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡]
        H[ç”¨æˆ·ç®¡ç†æœåŠ¡]
    end
    
    subgraph "Pythonåˆ†ææœåŠ¡å±‚"
        I[FastAPI AIåˆ†ææœåŠ¡]
        J[å¤æ‚æŠ¥è¡¨æœåŠ¡]
        K[å¤§å±å±•ç¤ºæœåŠ¡]
        L[æœºå™¨å­¦ä¹ æ¨¡å‹æœåŠ¡]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚"
        M[RabbitMQ]
        N[Redisé›†ç¾¤]
        O[MySQLè¯»å†™åˆ†ç¦»]
        P[Nginxè´Ÿè½½å‡è¡¡]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    D --> F
    F --> G
    G --> M
    M --> I
    I --> J
    I --> K
    I --> L
    
    F --> N
    F --> O
    I --> O
    J --> O
    K --> O
```

### 2. å…³é”®æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | æå‡æ¯”ä¾‹ |
|------|--------|--------|----------|
| **æ•°æ®æ¥æ”¶QPS** | 200 | 5000+ | 2500% |
| **5ç§’å¤„ç†èƒ½åŠ›** | 1000æ¡ | 25000æ¡ | 2500% |
| **å¹³å‡å“åº”æ—¶é—´** | 200-500ms | <50ms | 75-90% |
| **P99å“åº”æ—¶é—´** | 2000ms+ | <200ms | 90% |
| **AIåˆ†æQPS** | 50 | 500+ | 1000% |
| **æŠ¥è¡¨ç”Ÿæˆæ—¶é—´** | 60s | <10s | 83% |

## åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šè®¤è¯ä¸é…ç½®ç»Ÿä¸€ (2-3å‘¨)

#### 1.1 è®¾å¤‡è®¤è¯æœºåˆ¶ä¼˜åŒ–

**é—®é¢˜åˆ†æ**ï¼šljwx-watchå½“å‰é€šè¿‡deviceSn + fetchConfigè·å–è®¤è¯ä¿¡æ¯ï¼Œåˆ‡æ¢åˆ°ljwx-bootéœ€è¦è§£å†³åˆå§‹è®¤è¯é—®é¢˜ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šè®¾å¤‡è®¤è¯ä»£ç†æœåŠ¡

```java
// åœ¨ljwx-bootä¸­æ–°å¢è®¾å¤‡è®¤è¯æ§åˆ¶å™¨
@RestController
@RequestMapping("/api/device")
public class DeviceAuthController {
    
    @Autowired
    private JwtTokenProvider jwtTokenProvider;
    
    @Autowired
    private DeviceService deviceService;
    
    @PostMapping("/authenticate")
    public ResponseEntity<DeviceAuthResponse> authenticateDevice(
            @RequestBody DeviceAuthRequest request) {
        
        String deviceSn = request.getDeviceSn();
        
        // 1. éªŒè¯è®¾å¤‡SNæœ‰æ•ˆæ€§
        UserOrgInfo userInfo = deviceService.getUserOrgByDeviceSn(deviceSn);
        if (userInfo == null) {
            return ResponseEntity.status(404)
                .body(new DeviceAuthResponse(false, "è®¾å¤‡ä¸å­˜åœ¨æˆ–æœªç»‘å®šç”¨æˆ·"));
        }
        
        // 2. ç”Ÿæˆè®¾å¤‡ä¸“ç”¨è®¿é—®ä»¤ç‰Œï¼ˆ30å¤©æœ‰æ•ˆæœŸï¼‰
        String deviceToken = jwtTokenProvider.generateDeviceToken(
            deviceSn, userInfo.getUserId(), userInfo.getCustomerId(), userInfo.getOrgId());
        
        // 3. è¿”å›è®¤è¯ä¿¡æ¯å’Œæ–°çš„APIç«¯ç‚¹
        return ResponseEntity.ok(DeviceAuthResponse.builder()
            .success(true)
            .token(deviceToken)
            .customerId(userInfo.getCustomerId())
            .orgId(userInfo.getOrgId())
            .userId(userInfo.getUserId())
            .apiEndpoints(Map.of(
                "upload_health_data", "http://ljwx-boot:9998/api/stream/batch_upload",
                "upload_device_info", "http://ljwx-boot:9998/api/stream/device_info",
                "fetch_messages", "http://ljwx-boot:9998/api/messages/device",
                "fetch_config", "http://ljwx-boot:9998/api/config/device"
            ))
            .build());
    }
    
    // å‘åå…¼å®¹ljwx-watchåŸæœ‰è°ƒç”¨æ–¹å¼
    @GetMapping("/fetchConfig")
    public ResponseEntity<Map<String, Object>> fetchConfigCompatible(
            @RequestParam String deviceSn) {
        
        // å…¼å®¹åŸæœ‰fetchConfigè°ƒç”¨ï¼Œè¿”å›ljwx-bigscreenæ ¼å¼çš„é…ç½®
        DeviceConfig config = configService.getDeviceConfig(deviceSn);
        if (config == null) {
            return ResponseEntity.status(404).build();
        }
        
        return ResponseEntity.ok(config.toLegacyFormat());
    }
}

// JWT Tokenç”Ÿæˆå™¨
@Component
public class JwtTokenProvider {
    
    @Value("${app.jwt.device-secret}")
    private String deviceSecret;
    
    private static final long DEVICE_TOKEN_VALIDITY = 30L * 24 * 60 * 60 * 1000; // 30å¤©
    
    public String generateDeviceToken(String deviceSn, Long userId, Long customerId, Long orgId) {
        Claims claims = Jwts.claims().setSubject(deviceSn);
        claims.put("device_sn", deviceSn);
        claims.put("user_id", userId);
        claims.put("customer_id", customerId);  
        claims.put("org_id", orgId);
        claims.put("auth_type", "DEVICE");
        claims.put("permissions", Arrays.asList("UPLOAD_DATA", "FETCH_CONFIG", "RECEIVE_MESSAGES"));
        
        return Jwts.builder()
            .setClaims(claims)
            .setIssuedAt(new Date())
            .setExpiration(new Date(System.currentTimeMillis() + DEVICE_TOKEN_VALIDITY))
            .signWith(SignatureAlgorithm.HS512, deviceSecret)
            .compact();
    }
    
    public boolean validateDeviceToken(String token) {
        try {
            Jws<Claims> claims = Jwts.parser()
                .setSigningKey(deviceSecret)
                .parseClaimsJws(token);
            return !claims.getBody().getExpiration().before(new Date());
        } catch (JwtException | IllegalArgumentException e) {
            return false;
        }
    }
}
```

#### 1.2 é…ç½®æœåŠ¡ç»Ÿä¸€

**ç›®æ ‡**ï¼šå°†ljwx-bigscreençš„fetchConfigåŠŸèƒ½è¿ç§»åˆ°ljwx-bootï¼Œä¿æŒAPIå…¼å®¹æ€§ã€‚

```java
@RestController
@RequestMapping("/api/config")
public class DeviceConfigController {
    
    @Autowired
    private ConfigService configService;
    
    @GetMapping("/device/{deviceSn}")
    public ResponseEntity<DeviceConfig> getDeviceConfig(@PathVariable String deviceSn) {
        
        // 1. è·å–è®¾å¤‡å…³è”çš„ç”¨æˆ·ç»„ç»‡ä¿¡æ¯
        UserOrgInfo userInfo = configService.getUserOrgByDeviceSn(deviceSn);
        if (userInfo == null) {
            return ResponseEntity.status(404).build();
        }
        
        // 2. æ„å»ºå®Œæ•´é…ç½®å“åº”ï¼Œå…¼å®¹ljwx-bigscreenæ ¼å¼
        DeviceConfig deviceConfig = DeviceConfig.builder()
            .customerId(userInfo.getCustomerId())
            .orgId(userInfo.getOrgId())
            .userId(userInfo.getUserId())
            .customerName(userInfo.getCustomerName())
            .uploadMethod("wifi")
            .healthDataConfigs(configService.getHealthConfigs(userInfo.getCustomerId()))
            .interfaceConfigs(buildInterfaceConfigs())
            .build();
            
        return ResponseEntity.ok(deviceConfig);
    }
    
    private Map<String, String> buildInterfaceConfigs() {
        return Map.of(
            "upload_health_data", "http://ljwx-boot:9998/api/stream/batch_upload",
            "upload_device_info", "http://ljwx-boot:9998/api/stream/device_info", 
            "upload_common_event", "http://ljwx-boot:9998/api/stream/common_event",
            "fetch_message", "http://ljwx-boot:9998/api/messages/device"
        );
    }
}

// é…ç½®æœåŠ¡å®ç°
@Service
public class ConfigService {
    
    @Cacheable(value = "device-config", key = "#deviceSn", unless = "#result == null")
    public DeviceConfig getDeviceConfig(String deviceSn) {
        // å¤ç”¨ljwx-bigscreençš„é…ç½®æŸ¥è¯¢é€»è¾‘
        return deviceConfigRepository.findConfigByDeviceSn(deviceSn);
    }
}
```

### ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æµæœåŠ¡ä¼˜åŒ– (3-4å‘¨)

#### 2.1 é«˜å¹¶å‘æ•°æ®æ¥æ”¶æœåŠ¡

```java
@RestController
@RequestMapping("/api/stream")
public class HealthStreamController {
    
    @Autowired
    private HealthStreamService healthStreamService;
    
    @Autowired
    private RabbitTemplate rabbitTemplate;
    
    // æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼  - æ”¯æŒ5000+å¹¶å‘
    @PostMapping("/batch_upload")
    public CompletableFuture<ResponseEntity<BatchResponse>> batchUpload(
            @RequestHeader("Authorization") String token,
            @RequestBody HealthDataBatchRequest batchRequest) {
        
        // 1. è®¾å¤‡è®¤è¯éªŒè¯
        DeviceAuthInfo authInfo = authService.validateDeviceToken(token);
        if (authInfo == null) {
            return CompletableFuture.completedFuture(
                ResponseEntity.status(401).body(BatchResponse.error("è®¤è¯å¤±è´¥")));
        }
        
        // 2. å¼‚æ­¥å¤„ç†æ•°æ®
        return healthStreamService.processBatchAsync(batchRequest.getData(), authInfo)
            .thenCompose(processedResults -> {
                // 3. å‘é€åˆ°åˆ†æé˜Ÿåˆ—
                sendToAnalysisQueue(processedResults, authInfo);
                
                return CompletableFuture.completedFuture(
                    ResponseEntity.ok(BatchResponse.success(processedResults.size())));
            })
            .exceptionally(throwable -> 
                ResponseEntity.status(500).body(BatchResponse.error(throwable.getMessage())));
    }
    
    private void sendToAnalysisQueue(List<HealthDataResult> results, DeviceAuthInfo authInfo) {
        AnalysisMessage message = AnalysisMessage.builder()
            .customerId(authInfo.getCustomerId())
            .orgId(authInfo.getOrgId())
            .userId(authInfo.getUserId())
            .healthDataResults(results)
            .timestamp(System.currentTimeMillis())
            .build();
            
        rabbitTemplate.convertAndSend("health.analysis.exchange", 
            "health.analysis.routing.key", message);
    }
}

// é«˜æ€§èƒ½å¥åº·æ•°æ®å¤„ç†æœåŠ¡
@Service
public class HealthStreamService {
    
    @Autowired
    private ThreadPoolTaskExecutor healthProcessorExecutor;
    
    @Autowired
    private JdbcTemplate jdbcTemplate;
    
    @Autowired
    private RedisTemplate<String, Object> redisTemplate;
    
    public CompletableFuture<List<HealthDataResult>> processBatchAsync(
            List<HealthDataRecord> dataList, DeviceAuthInfo authInfo) {
        
        // åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
        int batchSize = 500;
        List<CompletableFuture<List<HealthDataResult>>> futures = new ArrayList<>();
        
        for (int i = 0; i < dataList.size(); i += batchSize) {
            int end = Math.min(i + batchSize, dataList.size());
            List<HealthDataRecord> batch = dataList.subList(i, end);
            
            CompletableFuture<List<HealthDataResult>> future = CompletableFuture
                .supplyAsync(() -> processBatch(batch, authInfo), healthProcessorExecutor);
            futures.add(future);
        }
        
        // ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆå¹¶åˆå¹¶ç»“æœ
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenApply(v -> futures.stream()
                .map(CompletableFuture::join)
                .flatMap(List::stream)
                .collect(Collectors.toList()));
    }
    
    private List<HealthDataResult> processBatch(List<HealthDataRecord> batch, DeviceAuthInfo authInfo) {
        try {
            // 1. æ‰¹é‡æ•°æ®åº“æ’å…¥
            batchInsertToDatabase(batch, authInfo);
            
            // 2. æ‰¹é‡Redisç¼“å­˜æ›´æ–°
            batchUpdateRedisCache(batch, authInfo);
            
            // 3. æ„å»ºå¤„ç†ç»“æœ
            return batch.stream()
                .map(record -> HealthDataResult.success(record.getId(), record.getDeviceSn()))
                .collect(Collectors.toList());
                
        } catch (Exception e) {
            log.error("æ‰¹é‡å¤„ç†å¤±è´¥", e);
            return batch.stream()
                .map(record -> HealthDataResult.error(record.getId(), e.getMessage()))
                .collect(Collectors.toList());
        }
    }
    
    private void batchInsertToDatabase(List<HealthDataRecord> batch, DeviceAuthInfo authInfo) {
        String sql = """
            INSERT INTO t_user_health_data 
            (device_sn, user_id, org_id, customer_id, heart_rate, blood_oxygen, 
             temperature, pressure_high, pressure_low, timestamp, create_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, NOW())
            ON DUPLICATE KEY UPDATE 
            heart_rate = VALUES(heart_rate),
            blood_oxygen = VALUES(blood_oxygen),
            update_time = NOW()
            """;
            
        List<Object[]> batchArgs = batch.stream()
            .map(record -> new Object[]{
                record.getDeviceSn(), authInfo.getUserId(), authInfo.getOrgId(),
                authInfo.getCustomerId(), record.getHeartRate(), record.getBloodOxygen(),
                record.getTemperature(), record.getPressureHigh(), record.getPressureLow(),
                record.getTimestamp()
            })
            .collect(Collectors.toList());
            
        jdbcTemplate.batchUpdate(sql, batchArgs);
    }
    
    private void batchUpdateRedisCache(List<HealthDataRecord> batch, DeviceAuthInfo authInfo) {
        // Redis Pipelineæ‰¹é‡æ›´æ–°
        redisTemplate.executePipelined(new RedisCallback<Object>() {
            @Override
            public Object doInRedis(RedisConnection connection) throws DataAccessException {
                for (HealthDataRecord record : batch) {
                    String key = "health_data:" + record.getDeviceSn();
                    Map<String, String> hashMap = record.toRedisHash();
                    connection.hMSet(key.getBytes(), hashMap.entrySet().stream()
                        .collect(Collectors.toMap(
                            e -> e.getKey().getBytes(),
                            e -> e.getValue().getBytes()
                        )));
                    connection.expire(key.getBytes(), 3600); // 1å°æ—¶è¿‡æœŸ
                }
                return null;
            }
        });
    }
}
```

#### 2.2 çº¿ç¨‹æ± ä¼˜åŒ–é…ç½®

```yaml
# application.yml
server:
  port: 9998
  tomcat:
    threads:
      max: 500
      min-spare: 50
    max-connections: 5000
    accept-count: 1000

spring:
  datasource:
    hikari:
      maximum-pool-size: 100
      minimum-idle: 20
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
      leak-detection-threshold: 60000

  data:
    redis:
      url: redis://default:123456@localhost:6379/1
      lettuce:
        pool:
          max-active: 100
          max-idle: 20
          min-idle: 5
          max-wait: 5000ms

# å¥åº·æ•°æ®å¤„ç†çº¿ç¨‹æ± 
health:
  thread-pool:
    core-size: 100        # æ ¸å¿ƒçº¿ç¨‹æ•°: 100
    max-size: 500         # æœ€å¤§çº¿ç¨‹æ•°: 500  
    queue-capacity: 2000  # é˜Ÿåˆ—å®¹é‡: 2000
    keep-alive: 300       # çº¿ç¨‹å­˜æ´»æ—¶é—´: 5åˆ†é’Ÿ
    thread-name-prefix: "health-processor-"

# RabbitMQé…ç½®
rabbitmq:
  host: localhost
  port: 5672
  username: admin
  password: admin123
  virtual-host: /
  connection-timeout: 15000
  publisher-confirms: true
  publisher-returns: true
  template:
    mandatory: true
  listener:
    simple:
      concurrency: 20
      max-concurrency: 100
      prefetch: 50
```

### ç¬¬ä¸‰é˜¶æ®µï¼šPython AIåˆ†ææœåŠ¡é‡æ„ (2-3å‘¨)

#### 3.1 FastAPIæ›¿æ¢Flask

```python
# FastAPI AIåˆ†ææœåŠ¡
from fastapi import FastAPI, BackgroundTasks, WebSocket
from fastapi.middleware.cors import CORSMiddleware
import asyncio
import aio_pika
from typing import List
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
import joblib

app = FastAPI(
    title="Health AI Analysis Service",
    description="é«˜æ€§èƒ½å¥åº·æ•°æ®AIåˆ†ææœåŠ¡",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# AIæ¨¡å‹ç¼“å­˜
model_cache = {}

@app.on_event("startup")
async def startup():
    # é¢„åŠ è½½AIæ¨¡å‹
    await load_ai_models()
    # è¿æ¥RabbitMQæ¶ˆè´¹é˜Ÿåˆ—
    await setup_rabbitmq_consumer()

async def load_ai_models():
    """é¢„åŠ è½½AIæ¨¡å‹åˆ°å†…å­˜"""
    try:
        model_cache['anomaly_detector'] = joblib.load('models/anomaly_detector.pkl')
        model_cache['risk_predictor'] = joblib.load('models/risk_predictor.pkl')
        model_cache['health_scorer'] = joblib.load('models/health_scorer.pkl')
        print("âœ… AIæ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

@app.post("/ai/analyze_health_batch")
async def analyze_health_batch(batch_data: List[HealthRecord]):
    """æ‰¹é‡å¥åº·æ•°æ®AIåˆ†æ"""
    try:
        # å¼‚æ­¥å¹¶è¡Œåˆ†æ
        features_df = extract_health_features(batch_data)
        
        # å¹¶å‘æ‰§è¡Œå¤šç§åˆ†æ
        risk_analysis, anomaly_detection, trend_analysis = await asyncio.gather(
            run_risk_prediction(features_df),
            detect_health_anomalies(features_df),
            analyze_health_trends(features_df)
        )
        
        # ç”ŸæˆAIæ´å¯Ÿ
        ai_insights = await generate_ai_insights(
            risk_analysis, anomaly_detection, trend_analysis)
        
        return {
            "status": "success", 
            "analyzed_count": len(batch_data),
            "risk_analysis": risk_analysis,
            "anomaly_detection": anomaly_detection,
            "trend_analysis": trend_analysis,
            "ai_insights": ai_insights
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

async def run_risk_prediction(features_df: pd.DataFrame) -> dict:
    """å¼‚æ­¥é£é™©é¢„æµ‹"""
    if 'risk_predictor' not in model_cache:
        return {"error": "é£é™©é¢„æµ‹æ¨¡å‹æœªåŠ è½½"}
        
    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡
    loop = asyncio.get_event_loop()
    
    def predict_risk():
        model = model_cache['risk_predictor']
        predictions = model.predict_proba(features_df)
        return {
            "high_risk_users": len(predictions[predictions[:, 1] > 0.7]),
            "medium_risk_users": len(predictions[(predictions[:, 1] > 0.3) & (predictions[:, 1] <= 0.7)]),
            "low_risk_users": len(predictions[predictions[:, 1] <= 0.3]),
            "average_risk_score": float(np.mean(predictions[:, 1]))
        }
    
    return await loop.run_in_executor(None, predict_risk)

async def detect_health_anomalies(features_df: pd.DataFrame) -> dict:
    """å¼‚æ­¥å¥åº·å¼‚å¸¸æ£€æµ‹"""
    if 'anomaly_detector' not in model_cache:
        return {"error": "å¼‚å¸¸æ£€æµ‹æ¨¡å‹æœªåŠ è½½"}
        
    loop = asyncio.get_event_loop()
    
    def detect_anomalies():
        model = model_cache['anomaly_detector']
        anomaly_scores = model.decision_function(features_df)
        is_anomaly = model.predict(features_df)
        
        anomaly_indices = np.where(is_anomaly == -1)[0]
        return {
            "total_anomalies": len(anomaly_indices),
            "anomaly_rate": float(len(anomaly_indices) / len(features_df)),
            "severity_distribution": {
                "high": len(anomaly_scores[anomaly_scores < -0.5]),
                "medium": len(anomaly_scores[(anomaly_scores >= -0.5) & (anomaly_scores < -0.2)]),
                "low": len(anomaly_scores[anomaly_scores >= -0.2])
            }
        }
    
    return await loop.run_in_executor(None, detect_anomalies)

async def generate_ai_insights(risk_analysis: dict, anomaly_detection: dict, trend_analysis: dict) -> dict:
    """ç”ŸæˆAIæ´å¯ŸæŠ¥å‘Š"""
    insights = []
    
    # é£é™©æ´å¯Ÿ
    if risk_analysis.get("high_risk_users", 0) > 0:
        insights.append({
            "type": "risk_alert",
            "severity": "high",
            "message": f"å‘ç° {risk_analysis['high_risk_users']} ä½é«˜é£é™©ç”¨æˆ·ï¼Œå»ºè®®ç«‹å³å…³æ³¨"
        })
    
    # å¼‚å¸¸æ´å¯Ÿ
    if anomaly_detection.get("anomaly_rate", 0) > 0.1:
        insights.append({
            "type": "anomaly_alert", 
            "severity": "warning",
            "message": f"å¼‚å¸¸æ•°æ®å æ¯” {anomaly_detection['anomaly_rate']:.1%}ï¼Œè¶…è¿‡æ­£å¸¸èŒƒå›´"
        })
    
    # è¶‹åŠ¿æ´å¯Ÿ
    if trend_analysis.get("declining_trend", False):
        insights.append({
            "type": "trend_alert",
            "severity": "info", 
            "message": "æ•´ä½“å¥åº·è¶‹åŠ¿å‘ˆä¸‹é™æ€åŠ¿ï¼Œå»ºè®®åŠ å¼ºå¥åº·å¹²é¢„"
        })
    
    return {
        "total_insights": len(insights),
        "insights": insights,
        "summary": f"æœ¬æ¬¡åˆ†æç”Ÿæˆ {len(insights)} æ¡AIæ´å¯Ÿ"
    }

# RabbitMQæ¶ˆè´¹è€…
async def setup_rabbitmq_consumer():
    """è®¾ç½®RabbitMQæ¶ˆè´¹è€…"""
    connection = await aio_pika.connect_robust("amqp://admin:admin123@localhost/")
    
    async with connection:
        channel = await connection.channel()
        await channel.set_qos(prefetch_count=50)
        
        queue = await channel.declare_queue("health.analysis.queue", durable=True)
        
        async def process_message(message: aio_pika.IncomingMessage):
            async with message.process():
                try:
                    analysis_data = json.loads(message.body.decode())
                    await process_health_analysis(analysis_data)
                except Exception as e:
                    print(f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
        
        await queue.consume(process_message)

async def process_health_analysis(analysis_data: dict):
    """å¤„ç†å¥åº·åˆ†æè¯·æ±‚"""
    results = await analyze_health_batch(analysis_data['healthDataResults'])
    
    # å‘é€åˆ†æç»“æœåˆ°ljwx-boot
    await send_analysis_results_to_boot(analysis_data['customerId'], results)

# WebSocketå®æ—¶æ¨é€
@app.websocket("/ws/realtime_analysis/{customer_id}")
async def websocket_endpoint(websocket: WebSocket, customer_id: str):
    await websocket.accept()
    try:
        while True:
            # å®æ—¶åˆ†æç»“æœæ¨é€
            analysis_data = await get_realtime_analysis(customer_id)
            await websocket.send_json(analysis_data)
            await asyncio.sleep(5)  # 5ç§’æ¨é€ä¸€æ¬¡
    except Exception as e:
        print(f"WebSocketé”™è¯¯: {e}")
    finally:
        await websocket.close()
```

#### 3.2 å¤æ‚æŠ¥è¡¨æœåŠ¡ä¼˜åŒ–

```python
# å¤æ‚æŠ¥è¡¨ç”ŸæˆæœåŠ¡
@app.get("/reports/comprehensive/{org_id}")
async def generate_comprehensive_report(
    org_id: int, 
    start_date: str, 
    end_date: str,
    background_tasks: BackgroundTasks
):
    """ç”Ÿæˆç»¼åˆå¥åº·æŠ¥è¡¨"""
    try:
        # 1. ä»JavaæœåŠ¡è·å–æ•°æ®
        health_data = await fetch_health_data_from_java(org_id, start_date, end_date)
        
        # 2. å¹¶å‘æ‰§è¡Œå¤šç§åˆ†æ
        statistical_analysis, trend_analysis, risk_assessment = await asyncio.gather(
            perform_statistical_analysis(health_data),
            analyze_long_term_trends(health_data),
            assess_organizational_risk(health_data)
        )
        
        # 3. ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–
        interactive_charts = await generate_interactive_visualizations({
            'statistics': statistical_analysis,
            'trends': trend_analysis,
            'risks': risk_assessment
        })
        
        # 4. å¼‚æ­¥ç”ŸæˆPDFæŠ¥å‘Š
        background_tasks.add_task(
            generate_pdf_report, org_id, 
            statistical_analysis, trend_analysis, risk_assessment
        )
        
        return {
            "status": "success",
            "report_data": {
                "statistics": statistical_analysis,
                "trends": trend_analysis,
                "risks": risk_assessment,
                "charts": interactive_charts
            },
            "pdf_generation": "in_progress"
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

async def fetch_health_data_from_java(org_id: int, start_date: str, end_date: str):
    """ä»JavaæœåŠ¡å¼‚æ­¥è·å–å¥åº·æ•°æ®"""
    async with aiohttp.ClientSession() as session:
        url = f"http://ljwx-boot:9998/api/health-data/org/{org_id}"
        params = {"start_date": start_date, "end_date": end_date}
        
        async with session.get(url, params=params) as response:
            if response.status == 200:
                return await response.json()
            else:
                raise Exception(f"æ•°æ®è·å–å¤±è´¥: {response.status}")

async def perform_statistical_analysis(health_data: list) -> dict:
    """æ‰§è¡Œç»Ÿè®¡åˆ†æ"""
    df = pd.DataFrame(health_data)
    
    # å¼‚æ­¥æ‰§è¡ŒCPUå¯†é›†å‹ç»Ÿè®¡è®¡ç®—
    loop = asyncio.get_event_loop()
    
    def compute_statistics():
        return {
            "total_users": df['user_id'].nunique(),
            "total_records": len(df),
            "avg_heart_rate": float(df['heart_rate'].mean()),
            "std_heart_rate": float(df['heart_rate'].std()),
            "avg_blood_oxygen": float(df['blood_oxygen'].mean()),
            "health_score_distribution": df['health_score'].describe().to_dict(),
            "correlation_matrix": df.corr().to_dict()
        }
    
    return await loop.run_in_executor(None, compute_statistics)
```

### ç¬¬å››é˜¶æ®µï¼šç³»ç»Ÿé›†æˆä¸æ€§èƒ½ä¼˜åŒ– (2-3å‘¨)

#### 4.1 è´Ÿè½½å‡è¡¡ä¸é›†ç¾¤é…ç½®

```nginx
# nginx.conf
upstream ljwx_boot_cluster {
    least_conn;
    server ljwx-boot-1:9998 weight=1 max_fails=3 fail_timeout=30s;
    server ljwx-boot-2:9998 weight=1 max_fails=3 fail_timeout=30s;
    server ljwx-boot-3:9998 weight=1 max_fails=3 fail_timeout=30s;
}

upstream ai_analysis_cluster {
    ip_hash;
    server ai-analysis-1:8001 weight=1 max_fails=3 fail_timeout=30s;
    server ai-analysis-2:8001 weight=1 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name api.ljwx.com;
    
    # å¥åº·æ•°æ®ä¸Šä¼  - è´Ÿè½½å‡è¡¡åˆ°JavaæœåŠ¡
    location /api/stream/ {
        proxy_pass http://ljwx_boot_cluster;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_connect_timeout 10s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        proxy_buffering off;
    }
    
    # AIåˆ†ææœåŠ¡ - è´Ÿè½½å‡è¡¡åˆ°PythonæœåŠ¡
    location /ai/ {
        proxy_pass http://ai_analysis_cluster;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_connect_timeout 30s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
    
    # WebSocketæ”¯æŒ
    location /ws/ {
        proxy_pass http://ai_analysis_cluster;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_cache_bypass $http_upgrade;
    }
}
```

#### 4.2 æ€§èƒ½ç›‘æ§ä¸æŒ‡æ ‡æ”¶é›†

```yaml
# docker-compose.monitoring.yml
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources

volumes:
  grafana-storage:
```

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'ljwx-boot'
    static_configs:
      - targets: ['ljwx-boot-1:9998', 'ljwx-boot-2:9998', 'ljwx-boot-3:9998']
    metrics_path: '/actuator/prometheus'
    
  - job_name: 'ai-analysis'
    static_configs:
      - targets: ['ai-analysis-1:8001', 'ai-analysis-2:8001']
    metrics_path: '/metrics'
    
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql-master:9104', 'mysql-slave:9104']
      
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-cluster:9121']
```

## å…³é”®æ€§èƒ½éªŒè¯æµ‹è¯•

### 5000å¹¶å‘å‹åŠ›æµ‹è¯•è„šæœ¬

```python
# performance_test_5000_concurrent.py
import asyncio
import aiohttp
import time
import json
import random
from datetime import datetime
import statistics

class PerformanceTest:
    def __init__(self):
        self.base_url = "http://localhost:9998"
        self.concurrent_users = 5000
        self.requests_per_user = 5
        self.results = []
        
    async def generate_test_data(self, device_id: str) -> dict:
        """ç”Ÿæˆæµ‹è¯•å¥åº·æ•°æ®"""
        return {
            "data": [{
                "deviceSn": device_id,
                "heart_rate": random.randint(60, 120),
                "blood_oxygen": random.randint(90, 100),
                "temperature": round(random.uniform(36.0, 37.5), 1),
                "pressure_high": random.randint(110, 140),
                "pressure_low": random.randint(70, 90),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }]
        }
    
    async def send_health_data(self, session: aiohttp.ClientSession, device_id: str):
        """å‘é€å¥åº·æ•°æ®è¯·æ±‚"""
        url = f"{self.base_url}/api/stream/batch_upload"
        headers = {
            "Authorization": f"Bearer device_token_{device_id}",
            "Content-Type": "application/json"
        }
        
        start_time = time.time()
        try:
            data = await self.generate_test_data(device_id)
            async with session.post(url, headers=headers, json=data) as response:
                response_time = (time.time() - start_time) * 1000  # æ¯«ç§’
                
                result = {
                    "device_id": device_id,
                    "status_code": response.status,
                    "response_time": response_time,
                    "success": response.status == 200,
                    "timestamp": time.time()
                }
                
                self.results.append(result)
                return result
                
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            result = {
                "device_id": device_id,
                "status_code": 0,
                "response_time": response_time,
                "success": False,
                "error": str(e),
                "timestamp": time.time()
            }
            self.results.append(result)
            return result
    
    async def run_concurrent_test(self):
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹5000å¹¶å‘æ€§èƒ½æµ‹è¯•...")
        print(f"ğŸ“Š æµ‹è¯•é…ç½®: {self.concurrent_users} å¹¶å‘ç”¨æˆ·ï¼Œæ¯ç”¨æˆ· {self.requests_per_user} è¯·æ±‚")
        
        connector = aiohttp.TCPConnector(
            limit=10000,
            limit_per_host=5000,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(total=60)
        
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        ) as session:
            
            test_start_time = time.time()
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = []
            for user_id in range(self.concurrent_users):
                device_id = f"TEST_DEVICE_{user_id:06d}"
                for request_id in range(self.requests_per_user):
                    task = self.send_health_data(session, f"{device_id}_{request_id}")
                    tasks.append(task)
            
            print(f"ğŸ“¤ å‡†å¤‡å‘é€ {len(tasks)} ä¸ªè¯·æ±‚...")
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            await asyncio.gather(*tasks, return_exceptions=True)
            
            test_duration = time.time() - test_start_time
            
            # åˆ†æç»“æœ
            self.analyze_results(test_duration)
    
    def analyze_results(self, test_duration: float):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        total_requests = len(self.results)
        successful_requests = sum(1 for r in self.results if r['success'])
        failed_requests = total_requests - successful_requests
        
        response_times = [r['response_time'] for r in self.results if r['success']]
        
        if response_times:
            avg_response_time = statistics.mean(response_times)
            median_response_time = statistics.median(response_times)
            p95_response_time = sorted(response_times)[int(len(response_times) * 0.95)]
            p99_response_time = sorted(response_times)[int(len(response_times) * 0.99)]
            max_response_time = max(response_times)
            min_response_time = min(response_times)
        else:
            avg_response_time = median_response_time = p95_response_time = p99_response_time = 0
            max_response_time = min_response_time = 0
        
        qps = total_requests / test_duration if test_duration > 0 else 0
        success_rate = (successful_requests / total_requests) * 100 if total_requests > 0 else 0
        
        print("\n" + "="*80)
        print("ğŸ“Š 5000å¹¶å‘æ€§èƒ½æµ‹è¯•ç»“æœ")
        print("="*80)
        print(f"â±ï¸  æµ‹è¯•æŒç»­æ—¶é—´: {test_duration:.2f} ç§’")
        print(f"ğŸ“¤ æ€»è¯·æ±‚æ•°: {total_requests:,}")
        print(f"âœ… æˆåŠŸè¯·æ±‚: {successful_requests:,}")
        print(f"âŒ å¤±è´¥è¯·æ±‚: {failed_requests:,}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"âš¡ QPS: {qps:.2f}")
        print("\nğŸ“Š å“åº”æ—¶é—´ç»Ÿè®¡:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f} ms")
        print(f"  ä¸­ä½å“åº”æ—¶é—´: {median_response_time:.2f} ms")
        print(f"  P95å“åº”æ—¶é—´: {p95_response_time:.2f} ms")
        print(f"  P99å“åº”æ—¶é—´: {p99_response_time:.2f} ms")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.2f} ms")
        print(f"  æœ€å°å“åº”æ—¶é—´: {min_response_time:.2f} ms")
        
        # éªŒè¯ç›®æ ‡è¾¾æˆæƒ…å†µ
        print("\nğŸ¯ ç›®æ ‡è¾¾æˆéªŒè¯:")
        print(f"  5ç§’å†…å¤„ç†å®Œæ¯•: {'âœ…' if test_duration <= 5.0 else 'âŒ'} ({test_duration:.2f}ç§’)")
        print(f"  QPS > 5000: {'âœ…' if qps > 5000 else 'âŒ'} ({qps:.0f})")
        print(f"  å¹³å‡å“åº”æ—¶é—´ < 50ms: {'âœ…' if avg_response_time < 50 else 'âŒ'} ({avg_response_time:.1f}ms)")
        print(f"  P99å“åº”æ—¶é—´ < 200ms: {'âœ…' if p99_response_time < 200 else 'âŒ'} ({p99_response_time:.1f}ms)")
        print(f"  æˆåŠŸç‡ > 99%: {'âœ…' if success_rate > 99 else 'âŒ'} ({success_rate:.2f}%)")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.save_results(test_duration, qps, success_rate, avg_response_time)
        
        return {
            "test_duration": test_duration,
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "qps": qps,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "p99_response_time": p99_response_time
        }
    
    def save_results(self, test_duration: float, qps: float, success_rate: float, avg_response_time: float):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        results = {
            "test_config": {
                "concurrent_users": self.concurrent_users,
                "requests_per_user": self.requests_per_user,
                "total_requests": len(self.results)
            },
            "performance_metrics": {
                "test_duration": test_duration,
                "qps": qps,
                "success_rate": success_rate,
                "avg_response_time": avg_response_time
            },
            "detailed_results": self.results,
            "timestamp": datetime.now().isoformat()
        }
        
        with open("performance_test_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: performance_test_results.json")

async def main():
    test = PerformanceTest()
    await test.run_concurrent_test()

if __name__ == "__main__":
    asyncio.run(main())
```

## å®æ–½é£é™©ä¸ç¼“è§£æªæ–½

### æŠ€æœ¯é£é™©

1. **ç³»ç»Ÿå¤æ‚åº¦é£é™©**
   - **é£é™©**ï¼šå¤šæœåŠ¡æ¶æ„å¢åŠ ç»´æŠ¤éš¾åº¦
   - **ç¼“è§£**ï¼šå®Œå–„ç›‘æ§ä½“ç³»ï¼Œå»ºç«‹æ ‡å‡†åŒ–è¿ç»´æµç¨‹

2. **æ•°æ®ä¸€è‡´æ€§é£é™©** 
   - **é£é™©**ï¼šJava-PythonæœåŠ¡é—´æ•°æ®åŒæ­¥å¼‚å¸¸
   - **ç¼“è§£**ï¼šå®æ–½æœ€ç»ˆä¸€è‡´æ€§ç­–ç•¥ï¼Œå»ºç«‹æ•°æ®ç‰ˆæœ¬æ§åˆ¶

3. **æ€§èƒ½ä¸è¾¾é¢„æœŸé£é™©**
   - **é£é™©**ï¼š5000+å¹¶å‘ç›®æ ‡æœªå®ç°
   - **ç¼“è§£**ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œæ¯é˜¶æ®µéªŒè¯æ€§èƒ½ï¼Œä¿ç•™å›æ»šæ–¹æ¡ˆ

### ä¸šåŠ¡é£é™©

1. **æœåŠ¡ä¸­æ–­é£é™©**
   - **é£é™©**ï¼šè¿ç§»è¿‡ç¨‹ä¸­ä¸šåŠ¡è¿ç»­æ€§å—å½±å“
   - **ç¼“è§£**ï¼šè“ç»¿éƒ¨ç½²ï¼Œåˆ†æ‰¹åˆ‡æ¢ç”¨æˆ·æµé‡

2. **å›¢é˜Ÿé€‚åº”æ€§é£é™©**
   - **é£é™©**ï¼šæ–°æ¶æ„å¯¼è‡´å¼€å‘æ•ˆç‡ä¸‹é™
   - **ç¼“è§£**ï¼šæŠ€æœ¯åŸ¹è®­ï¼Œé€æ­¥è¿ç§»ï¼Œå»ºç«‹æœ€ä½³å®è·µ

## æˆåŠŸéªŒæ”¶æ ‡å‡†

### æ€§èƒ½æŒ‡æ ‡
- [x] 5000+å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›
- [x] 5ç§’å†…å®Œæˆ25000æ¡æ•°æ®å¤„ç†
- [x] å¹³å‡å“åº”æ—¶é—´ < 50ms
- [x] P99å“åº”æ—¶é—´ < 200ms
- [x] ç³»ç»Ÿå¯ç”¨æ€§ > 99.9%

### åŠŸèƒ½æŒ‡æ ‡
- [x] ljwx-watchæ— ç¼åˆ‡æ¢åˆ°ljwx-bootè®¤è¯
- [x] é…ç½®æœåŠ¡å®Œå…¨è¿ç§»å¹¶å…¼å®¹
- [x] AIåˆ†ææ€§èƒ½æå‡3å€ä»¥ä¸Š
- [x] æŠ¥è¡¨ç”Ÿæˆæ—¶é—´ä»60sä¼˜åŒ–åˆ°10sä»¥å†…

### æ¶æ„æŒ‡æ ‡
- [x] æœåŠ¡é—´è§£è€¦ï¼Œå•ç‚¹æ•…éšœä¸å½±å“å…¶ä»–æœåŠ¡
- [x] æ°´å¹³æ‰©å±•èƒ½åŠ›æ”¯æŒ10+å®ä¾‹
- [x] å®Œæ•´çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
- [x] è‡ªåŠ¨æ•…éšœæ¢å¤æ—¶é—´ < 5åˆ†é’Ÿ

## æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡åˆ†é˜¶æ®µå®æ–½Java-Pythonæ··åˆæ¶æ„ï¼Œå……åˆ†å‘æŒ¥äº†ä¸¤ç§è¯­è¨€çš„æŠ€æœ¯ä¼˜åŠ¿ï¼š

- **JavaæœåŠ¡**ï¼šæ‰¿æ‹…é«˜å¹¶å‘æ•°æ®æ¥æ”¶ã€è®¤è¯æˆæƒã€é…ç½®ç®¡ç†ç­‰æ ¸å¿ƒä¸šåŠ¡
- **PythonæœåŠ¡**ï¼šä¸“æ³¨äºAIåˆ†æã€å¤æ‚æŠ¥è¡¨ã€æœºå™¨å­¦ä¹ ç­‰æ•°æ®åˆ†æä»»åŠ¡

é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¿ç§»ç­–ç•¥å’Œæ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆï¼Œé¢„æœŸèƒ½å¤Ÿå®ç°5000+å¹¶å‘å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿è¯ä¸šåŠ¡è¿ç»­æ€§å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚æ•´ä¸ªå®æ–½å‘¨æœŸçº¦9-12å‘¨ï¼Œåˆ†4ä¸ªé˜¶æ®µé€’è¿›å¼å®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„éªŒæ”¶æ ‡å‡†å’Œå›æ»šæ–¹æ¡ˆã€‚