# 5000+并发优化实施代码

## 1. 异步数据接收优化

### 1.1 FastAPI替换Flask (ljwx-bigscreen升级)

```python
# bigscreen/bigScreen/async_bigscreen.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, validator
from typing import List, Optional
import asyncio
import aioredis
import aiomysql
import time
import logging
from datetime import datetime

app = FastAPI(title="LJWX BigScreen API", version="2.0.0")

# 添加CORS中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 数据模型
class HealthData(BaseModel):
    deviceSn: str
    heartRate: Optional[int] = None
    bloodOxygen: Optional[int] = None  
    temperature: Optional[float] = None
    pressureHigh: Optional[int] = None
    pressureLow: Optional[int] = None
    stress: Optional[int] = None
    step: Optional[int] = None
    distance: Optional[int] = None
    calorie: Optional[int] = None
    latitude: Optional[float] = None
    longitude: Optional[float] = None
    sleep: Optional[str] = None
    timestamp: int
    uploadMethod: str = "api"
    
    @validator('timestamp')
    def validate_timestamp(cls, v):
        if v <= 0:
            raise ValueError('timestamp must be positive')
        return v

class HealthDataBatch(BaseModel):
    data: List[HealthData]
    batchId: Optional[str] = None

# 全局连接池
redis_pool = None
db_pool = None

@app.on_event("startup")
async def startup_event():
    global redis_pool, db_pool
    
    # Redis连接池
    redis_pool = aioredis.ConnectionPool.from_url(
        "redis://localhost:6379/0", 
        max_connections=500,
        retry_on_timeout=True
    )
    
    # MySQL连接池
    db_pool = await aiomysql.create_pool(
        host='127.0.0.1',
        port=3306,
        user='root', 
        password='123456',
        db='test',
        minsize=50,
        maxsize=300,
        autocommit=True,
        charset='utf8mb4'
    )
    
    # 启动后台处理任务
    asyncio.create_task(background_data_processor())

@app.on_event("shutdown")
async def shutdown_event():
    global redis_pool, db_pool
    if redis_pool:
        await redis_pool.disconnect()
    if db_pool:
        db_pool.close()
        await db_pool.wait_closed()

# 优化的批量上传接口
@app.post("/upload_health_data_batch")
async def upload_health_data_batch(
    batch: HealthDataBatch, 
    background_tasks: BackgroundTasks
):
    """批量健康数据上传接口 - 支持5000+并发"""
    try:
        start_time = time.time()
        batch_id = batch.batchId or f"batch_{int(time.time() * 1000000)}"
        
        # 数据验证和预处理
        validated_data = []
        errors = []
        
        for i, data in enumerate(batch.data):
            try:
                # 基础验证
                if not data.deviceSn or len(data.deviceSn) < 5:
                    errors.append(f"Invalid deviceSn at index {i}")
                    continue
                    
                validated_data.append({
                    'device_sn': data.deviceSn,
                    'heart_rate': data.heartRate,
                    'blood_oxygen': data.bloodOxygen,
                    'temperature': data.temperature,
                    'pressure_high': data.pressureHigh,
                    'pressure_low': data.pressureLow,
                    'stress': data.stress,
                    'step': data.step,
                    'distance': data.distance,
                    'calorie': data.calorie,
                    'latitude': data.latitude,
                    'longitude': data.longitude,
                    'sleep': data.sleep,
                    'timestamp': datetime.fromtimestamp(data.timestamp),
                    'upload_method': data.uploadMethod,
                    'batch_id': batch_id
                })
            except Exception as e:
                errors.append(f"Data validation error at index {i}: {str(e)}")
                
        if not validated_data:
            raise HTTPException(status_code=400, detail="No valid data in batch")
            
        # 异步处理数据
        background_tasks.add_task(process_health_data_async, validated_data, batch_id)
        
        processing_time = time.time() - start_time
        
        return {
            "status": "accepted",
            "batch_id": batch_id,
            "total_records": len(batch.data),
            "valid_records": len(validated_data),
            "errors": len(errors),
            "processing_time_ms": round(processing_time * 1000, 2),
            "error_details": errors[:10] if errors else []  # 最多返回10个错误
        }
        
    except Exception as e:
        logging.error(f"Batch upload failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

# 单条上传接口（兼容性保持）
@app.post("/upload_health_data")
async def upload_health_data(data: HealthData, background_tasks: BackgroundTasks):
    """单条健康数据上传 - 兼容原有接口"""
    batch = HealthDataBatch(data=[data])
    return await upload_health_data_batch(batch, background_tasks)

# 异步数据处理器
async def process_health_data_async(data_list: List[dict], batch_id: str):
    """异步处理健康数据"""
    try:
        # Step 1: 设备用户映射查询（批量）
        device_sns = [d['device_sn'] for d in data_list]
        device_user_mapping = await get_device_user_mapping_batch(device_sns)
        
        # Step 2: 数据增强
        enhanced_data = []
        for data in data_list:
            device_sn = data['device_sn']
            if device_sn in device_user_mapping:
                user_info = device_user_mapping[device_sn]
                data.update({
                    'user_id': user_info['user_id'],
                    'customer_id': user_info['customer_id'], 
                    'org_id': user_info['org_id']
                })
                enhanced_data.append(data)
                
        if not enhanced_data:
            logging.warning(f"No valid device mappings found for batch {batch_id}")
            return
            
        # Step 3: 批量数据库写入
        inserted_count = await bulk_insert_health_data(enhanced_data)
        
        # Step 4: 异步告警处理
        asyncio.create_task(process_health_alerts_async(enhanced_data))
        
        # Step 5: 更新统计信息
        await update_batch_statistics(batch_id, len(data_list), inserted_count)
        
        logging.info(f"Batch {batch_id} processed: {inserted_count}/{len(data_list)} records")
        
    except Exception as e:
        logging.error(f"Async processing failed for batch {batch_id}: {str(e)}")
        await log_batch_error(batch_id, str(e))

# 批量设备用户映射查询
async def get_device_user_mapping_batch(device_sns: List[str]) -> dict:
    """批量查询设备用户映射 - 三级缓存优化"""
    mapping = {}
    uncached_devices = []
    
    # L1: Redis批量查询
    async with aioredis.Redis(connection_pool=redis_pool) as redis:
        pipe = redis.pipeline()
        for device_sn in device_sns:
            pipe.get(f"device_user:{device_sn}")
        cached_results = await pipe.execute()
        
        for i, result in enumerate(cached_results):
            if result:
                device_sn = device_sns[i]
                mapping[device_sn] = eval(result.decode('utf-8'))
            else:
                uncached_devices.append(device_sns[i])
    
    # L2: 数据库批量查询未缓存的设备
    if uncached_devices:
        async with db_pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                placeholders = ','.join(['%s'] * len(uncached_devices))
                sql = f"""
                SELECT u.device_sn, u.id as user_id, u.customer_id, uo.org_id
                FROM sys_user u 
                LEFT JOIN sys_user_org uo ON u.id = uo.user_id AND uo.is_deleted = 0
                WHERE u.device_sn IN ({placeholders}) AND u.is_deleted = 0
                """
                await cursor.execute(sql, uncached_devices)
                results = await cursor.fetchall()
                
                # 更新缓存和映射
                async with aioredis.Redis(connection_pool=redis_pool) as redis:
                    pipe = redis.pipeline()
                    for result in results:
                        device_sn = result['device_sn']
                        user_info = {
                            'user_id': result['user_id'],
                            'customer_id': result['customer_id'],
                            'org_id': result['org_id']
                        }
                        mapping[device_sn] = user_info
                        pipe.setex(f"device_user:{device_sn}", 3600, str(user_info))
                    await pipe.execute()
    
    return mapping

# 批量数据库写入优化
async def bulk_insert_health_data(data_list: List[dict]) -> int:
    """批量插入健康数据 - 优化版本"""
    if not data_list:
        return 0
        
    try:
        async with db_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                # 构造批量插入SQL
                sql = """
                INSERT IGNORE INTO t_user_health_data 
                (user_id, device_sn, heart_rate, blood_oxygen, temperature, 
                 pressure_high, pressure_low, stress, step, distance, calorie,
                 latitude, longitude, sleep, timestamp, upload_method, 
                 customer_id, org_id, create_time, batch_id)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), %s)
                """
                
                # 准备批量数据
                values = []
                for data in data_list:
                    values.append((
                        data['user_id'], data['device_sn'], data['heart_rate'],
                        data['blood_oxygen'], data['temperature'], data['pressure_high'],
                        data['pressure_low'], data['stress'], data['step'], 
                        data['distance'], data['calorie'], data['latitude'],
                        data['longitude'], data['sleep'], data['timestamp'],
                        data['upload_method'], data['customer_id'], data['org_id'],
                        data['batch_id']
                    ))
                
                # 执行批量插入
                affected_rows = await cursor.executemany(sql, values)
                return cursor.rowcount
                
    except Exception as e:
        logging.error(f"Bulk insert failed: {str(e)}")
        return 0

# 异步告警处理
async def process_health_alerts_async(data_list: List[dict]):
    """异步处理健康告警"""
    try:
        alert_tasks = []
        for data in data_list:
            task = asyncio.create_task(check_health_alert_single(data))
            alert_tasks.append(task)
            
        # 并发执行告警检查
        await asyncio.gather(*alert_tasks, return_exceptions=True)
        
    except Exception as e:
        logging.error(f"Alert processing failed: {str(e)}")

async def check_health_alert_single(data: dict):
    """单条健康数据告警检查"""
    try:
        alerts_to_send = []
        
        # 心率告警
        if data.get('heart_rate'):
            hr = data['heart_rate']
            if hr > 120 or hr < 50:
                alerts_to_send.append({
                    'type': 'heart_rate_abnormal',
                    'value': hr,
                    'level': 'major' if hr > 140 or hr < 40 else 'minor',
                    'user_id': data['user_id'],
                    'device_sn': data['device_sn']
                })
        
        # 血氧告警
        if data.get('blood_oxygen'):
            spo2 = data['blood_oxygen']
            if spo2 < 90:
                alerts_to_send.append({
                    'type': 'blood_oxygen_low',
                    'value': spo2,
                    'level': 'critical' if spo2 < 85 else 'major',
                    'user_id': data['user_id'],
                    'device_sn': data['device_sn']
                })
        
        # 发送告警
        for alert in alerts_to_send:
            await send_health_alert(alert)
            
    except Exception as e:
        logging.error(f"Single alert check failed: {str(e)}")

async def send_health_alert(alert: dict):
    """发送健康告警"""
    try:
        # 插入告警记录
        async with db_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                sql = """
                INSERT INTO t_alert_info 
                (user_id, device_sn, alert_type, alert_value, level, status, occur_at, create_time)
                VALUES (%s, %s, %s, %s, %s, 'NEW', NOW(), NOW())
                """
                await cursor.execute(sql, (
                    alert['user_id'], alert['device_sn'], alert['type'],
                    alert['value'], alert['level']
                ))
        
        # 异步推送通知（微信、短信等）
        asyncio.create_task(send_notification_async(alert))
        
    except Exception as e:
        logging.error(f"Alert sending failed: {str(e)}")

async def send_notification_async(alert: dict):
    """异步发送通知"""
    # 实现微信、短信等通知逻辑
    pass

# 后台数据处理器
async def background_data_processor():
    """后台数据处理器 - 处理积压数据"""
    while True:
        try:
            # 处理待处理的批次
            await process_pending_batches()
            await asyncio.sleep(5)  # 5秒检查一次
        except Exception as e:
            logging.error(f"Background processor error: {str(e)}")
            await asyncio.sleep(10)

async def process_pending_batches():
    """处理待处理批次"""
    # 实现逻辑：查询待处理批次并处理
    pass

# 监控接口
@app.get("/api/health")
async def health_check():
    """健康检查接口"""
    try:
        # 检查数据库连接
        async with db_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT 1")
                await cursor.fetchone()
        
        # 检查Redis连接
        async with aioredis.Redis(connection_pool=redis_pool) as redis:
            await redis.ping()
            
        return {"status": "healthy", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

@app.get("/api/stats")
async def get_stats():
    """获取系统统计信息"""
    try:
        async with db_pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                # 今日处理统计
                await cursor.execute("""
                    SELECT 
                        COUNT(*) as today_records,
                        COUNT(DISTINCT device_sn) as active_devices,
                        COUNT(DISTINCT user_id) as active_users
                    FROM t_user_health_data 
                    WHERE DATE(create_time) = CURDATE()
                """)
                stats = await cursor.fetchone()
                
                return {
                    "today_records": stats['today_records'],
                    "active_devices": stats['active_devices'], 
                    "active_users": stats['active_users'],
                    "timestamp": datetime.now().isoformat()
                }
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001, workers=4)
```

## 2. 负载均衡配置

### 2.1 Nginx负载均衡配置

```nginx
# /etc/nginx/sites-available/ljwx-bigscreen
upstream ljwx_bigscreen_backend {
    # 健康检查和负载均衡策略
    least_conn;  # 最少连接数算法
    
    # 后端服务器列表
    server 127.0.0.1:8001 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8002 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8003 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8004 weight=1 max_fails=3 fail_timeout=30s;
    
    # 备用服务器
    server 127.0.0.1:8005 backup;
    
    keepalive 300;  # 保持连接
}

server {
    listen 80;
    server_name ljwx-bigscreen.local;
    
    # 日志配置
    access_log /var/log/nginx/ljwx-bigscreen-access.log;
    error_log /var/log/nginx/ljwx-bigscreen-error.log;
    
    # 客户端配置
    client_max_body_size 10M;
    client_body_buffer_size 128k;
    client_header_buffer_size 32k;
    
    # 代理配置
    proxy_connect_timeout 60s;
    proxy_send_timeout 60s;
    proxy_read_timeout 60s;
    proxy_buffer_size 4k;
    proxy_buffers 4 32k;
    proxy_busy_buffers_size 64k;
    
    # 健康数据上传接口
    location ~ ^/(upload_health_data|upload_health_data_batch) {
        proxy_pass http://ljwx_bigscreen_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 限流配置
        limit_req zone=api burst=50 nodelay;
        
        # 缓存配置
        proxy_cache_bypass $http_cache_control;
        add_header X-Cache-Status $upstream_cache_status;
    }
    
    # API接口
    location /api/ {
        proxy_pass http://ljwx_bigscreen_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # 静态资源
    location /static/ {
        alias /opt/ljwx/bigscreen/static/;
        expires 7d;
        add_header Cache-Control "public, immutable";
    }
    
    # 健康检查
    location /health {
        proxy_pass http://ljwx_bigscreen_backend/api/health;
        access_log off;
    }
}

# 限流配置
http {
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/s;
    limit_conn_zone $binary_remote_addr zone=conn:10m;
    
    server {
        limit_conn conn 50;  # 每个IP最多50个连接
    }
}
```

## 3. 数据库优化

### 3.1 MySQL主从配置

```sql
-- 主库配置 (master)
-- /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
# 基础配置
server-id = 1
log-bin = mysql-bin
binlog-format = ROW
expire_logs_days = 7

# 性能优化配置
innodb_buffer_pool_size = 4G
innodb_buffer_pool_instances = 4
innodb_log_file_size = 256M
innodb_log_buffer_size = 64M
innodb_flush_log_at_trx_commit = 2
innodb_flush_method = O_DIRECT

# 连接配置
max_connections = 2000
max_connect_errors = 100000
wait_timeout = 600
interactive_timeout = 600

# 查询缓存
query_cache_type = 1
query_cache_size = 256M
query_cache_limit = 2M

# 临时表配置
tmp_table_size = 128M
max_heap_table_size = 128M

# 批量插入优化
bulk_insert_buffer_size = 64M
innodb_autoinc_lock_mode = 2
```

```sql
-- 从库配置 (slave)
-- /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
server-id = 2
relay-log = mysql-relay-log
read-only = 1
slave-skip-errors = 1062  # 跳过重复键错误

# 其他配置同主库
```

### 3.2 分表优化脚本

```sql
-- 创建分表脚本
DELIMITER $$

CREATE PROCEDURE CreateMonthlyHealthDataTable()
BEGIN
    DECLARE table_suffix VARCHAR(6);
    DECLARE create_sql TEXT;
    
    SET table_suffix = DATE_FORMAT(CURDATE(), '%Y%m');
    SET create_sql = CONCAT('
        CREATE TABLE IF NOT EXISTS t_user_health_data_', table_suffix, ' (
            id BIGINT NOT NULL AUTO_INCREMENT,
            user_id BIGINT NOT NULL,
            device_sn VARCHAR(64) NOT NULL,
            heart_rate INT DEFAULT NULL,
            blood_oxygen INT DEFAULT NULL,
            temperature DECIMAL(4,1) DEFAULT NULL,
            pressure_high INT DEFAULT NULL,
            pressure_low INT DEFAULT NULL,
            stress INT DEFAULT NULL,
            step INT DEFAULT NULL,
            distance INT DEFAULT NULL,
            calorie INT DEFAULT NULL,
            latitude DECIMAL(10,7) DEFAULT NULL,
            longitude DECIMAL(10,7) DEFAULT NULL,
            sleep TEXT DEFAULT NULL,
            timestamp TIMESTAMP NOT NULL,
            upload_method VARCHAR(20) DEFAULT ''api'',
            customer_id BIGINT DEFAULT NULL,
            org_id BIGINT DEFAULT NULL,
            batch_id VARCHAR(64) DEFAULT NULL,
            create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            is_deleted TINYINT(1) DEFAULT 0,
            PRIMARY KEY (id),
            KEY idx_user_timestamp (user_id, timestamp),
            KEY idx_device_timestamp (device_sn, timestamp),
            KEY idx_customer_org (customer_id, org_id),
            KEY idx_batch_id (batch_id),
            KEY idx_create_time (create_time)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
        COMMENT=''健康数据表_', table_suffix, '''
        PARTITION BY RANGE (TO_DAYS(timestamp)) (
            PARTITION p', table_suffix, '_01 VALUES LESS THAN (TO_DAYS(''', YEAR(CURDATE()), '-', LPAD(MONTH(CURDATE()), 2, '0'), '-08'')),
            PARTITION p', table_suffix, '_02 VALUES LESS THAN (TO_DAYS(''', YEAR(CURDATE()), '-', LPAD(MONTH(CURDATE()), 2, '0'), '-15'')),
            PARTITION p', table_suffix, '_03 VALUES LESS THAN (TO_DAYS(''', YEAR(CURDATE()), '-', LPAD(MONTH(CURDATE()), 2, '0'), '-22'')),
            PARTITION p', table_suffix, '_04 VALUES LESS THAN (TO_DAYS(''', YEAR(DATE_ADD(CURDATE(), INTERVAL 1 MONTH)), '-', LPAD(MONTH(DATE_ADD(CURDATE(), INTERVAL 1 MONTH)), 2, '0'), '-01''))
        );
    ');
    
    SET @sql = create_sql;
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    SELECT CONCAT('Created table: t_user_health_data_', table_suffix) as result;
END$$

DELIMITER ;
```

## 4. Redis集群配置

### 4.1 Redis集群部署脚本

```bash
#!/bin/bash
# redis-cluster-setup.sh

# 创建Redis集群目录
mkdir -p /opt/redis-cluster/{7001,7002,7003,7004,7005,7006}

# 生成配置文件
for port in {7001..7006}; do
cat > /opt/redis-cluster/$port/redis.conf << EOF
port $port
cluster-enabled yes
cluster-config-file nodes-$port.conf
cluster-node-timeout 15000
appendonly yes
appendfilename "appendonly-$port.aof"
dbfilename "dump-$port.rdb"
dir /opt/redis-cluster/$port/
logfile "/opt/redis-cluster/$port/redis-$port.log"
daemonize yes
protected-mode no
bind 0.0.0.0

# 性能优化
maxmemory 2gb
maxmemory-policy allkeys-lru
tcp-keepalive 300
timeout 0

# 持久化优化
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error no
rdbcompression yes
rdbchecksum yes

# AOF优化
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
EOF
done

# 启动所有Redis实例
for port in {7001..7006}; do
    redis-server /opt/redis-cluster/$port/redis.conf
done

# 创建集群
redis-cli --cluster create \
127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 \
127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 \
--cluster-replicas 1 --cluster-yes

echo "Redis cluster setup completed!"
```

### 4.2 Redis连接优化

```python
# bigscreen/bigScreen/redis_cluster.py
import aioredis
import asyncio
import logging
from typing import Optional, Any, List

class RedisClusterManager:
    def __init__(self, nodes: List[str]):
        self.nodes = nodes
        self.redis = None
        
    async def initialize(self):
        """初始化Redis集群连接"""
        try:
            self.redis = await aioredis.create_redis_cluster(
                self.nodes,
                encoding='utf-8',
                minsize=10,
                maxsize=100,
                timeout=5,
                retry_on_timeout=True
            )
            logging.info("Redis cluster initialized successfully")
        except Exception as e:
            logging.error(f"Redis cluster initialization failed: {e}")
            raise
            
    async def close(self):
        """关闭Redis连接"""
        if self.redis:
            self.redis.close()
            await self.redis.wait_closed()
            
    async def get(self, key: str) -> Optional[str]:
        """获取值"""
        try:
            return await self.redis.get(key)
        except Exception as e:
            logging.error(f"Redis GET error: {e}")
            return None
            
    async def set(self, key: str, value: Any, expire: int = None) -> bool:
        """设置值"""
        try:
            if expire:
                await self.redis.setex(key, expire, value)
            else:
                await self.redis.set(key, value)
            return True
        except Exception as e:
            logging.error(f"Redis SET error: {e}")
            return False
            
    async def mget(self, keys: List[str]) -> List[Optional[str]]:
        """批量获取"""
        try:
            return await self.redis.mget(*keys)
        except Exception as e:
            logging.error(f"Redis MGET error: {e}")
            return [None] * len(keys)
            
    async def mset(self, mapping: dict, expire: int = None) -> bool:
        """批量设置"""
        try:
            pipe = self.redis.pipeline()
            for key, value in mapping.items():
                if expire:
                    pipe.setex(key, expire, value)
                else:
                    pipe.set(key, value)
            await pipe.execute()
            return True
        except Exception as e:
            logging.error(f"Redis MSET error: {e}")
            return False
            
    async def delete(self, *keys) -> int:
        """删除键"""
        try:
            return await self.redis.delete(*keys)
        except Exception as e:
            logging.error(f"Redis DELETE error: {e}")
            return 0

# 全局Redis集群实例
redis_cluster = RedisClusterManager([
    "redis://127.0.0.1:7001",
    "redis://127.0.0.1:7002", 
    "redis://127.0.0.1:7003",
    "redis://127.0.0.1:7004",
    "redis://127.0.0.1:7005",
    "redis://127.0.0.1:7006"
])
```

## 5. 监控和运维

### 5.1 系统监控脚本

```python
# scripts/system_monitor.py
import asyncio
import aiohttp
import psutil
import time
import logging
from datetime import datetime
from typing import Dict, List

class SystemMonitor:
    def __init__(self, check_interval: int = 10):
        self.check_interval = check_interval
        self.endpoints = [
            "http://127.0.0.1:8001/api/health",
            "http://127.0.0.1:8002/api/health", 
            "http://127.0.0.1:8003/api/health",
            "http://127.0.0.1:8004/api/health"
        ]
        self.metrics_history = []
        
    async def collect_system_metrics(self) -> Dict:
        """收集系统指标"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        network = psutil.net_io_counters()
        
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_used_gb": memory.used / (1024**3),
            "memory_total_gb": memory.total / (1024**3),
            "disk_percent": disk.percent,
            "disk_used_gb": disk.used / (1024**3),
            "network_bytes_sent": network.bytes_sent,
            "network_bytes_recv": network.bytes_recv
        }
        
    async def check_service_health(self) -> Dict:
        """检查服务健康状态"""
        health_status = {}
        
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
            for endpoint in self.endpoints:
                try:
                    start_time = time.time()
                    async with session.get(endpoint) as response:
                        response_time = time.time() - start_time
                        if response.status == 200:
                            data = await response.json()
                            health_status[endpoint] = {
                                "status": "healthy",
                                "response_time_ms": round(response_time * 1000, 2),
                                "data": data
                            }
                        else:
                            health_status[endpoint] = {
                                "status": "unhealthy",
                                "http_status": response.status,
                                "response_time_ms": round(response_time * 1000, 2)
                            }
                except Exception as e:
                    health_status[endpoint] = {
                        "status": "error",
                        "error": str(e)
                    }
                    
        return health_status
        
    async def check_database_performance(self) -> Dict:
        """检查数据库性能"""
        # 实现数据库性能检查逻辑
        return {
            "connections_used": 0,
            "connections_max": 2000,
            "queries_per_second": 0,
            "slow_queries": 0
        }
        
    async def generate_alert(self, metric: str, value: float, threshold: float):
        """生成告警"""
        if value > threshold:
            alert = {
                "timestamp": datetime.now().isoformat(),
                "type": "system_alert",
                "metric": metric,
                "value": value,
                "threshold": threshold,
                "severity": "critical" if value > threshold * 1.2 else "warning"
            }
            logging.warning(f"System Alert: {alert}")
            # 这里可以集成告警推送逻辑
            
    async def run_monitoring(self):
        """运行监控循环"""
        logging.info("System monitoring started")
        
        while True:
            try:
                # 收集指标
                system_metrics = await self.collect_system_metrics()
                health_status = await self.check_service_health()
                db_metrics = await self.check_database_performance()
                
                # 检查告警阈值
                await self.generate_alert("cpu_percent", system_metrics["cpu_percent"], 80.0)
                await self.generate_alert("memory_percent", system_metrics["memory_percent"], 85.0)
                
                # 保存历史数据
                self.metrics_history.append({
                    "system": system_metrics,
                    "health": health_status,
                    "database": db_metrics
                })
                
                # 保持最近100个监控点
                if len(self.metrics_history) > 100:
                    self.metrics_history.pop(0)
                    
                logging.info(f"Monitoring check completed - CPU: {system_metrics['cpu_percent']}%, Memory: {system_metrics['memory_percent']}%")
                
            except Exception as e:
                logging.error(f"Monitoring error: {e}")
                
            await asyncio.sleep(self.check_interval)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    monitor = SystemMonitor(check_interval=30)
    asyncio.run(monitor.run_monitoring())
```

### 5.2 自动部署脚本

```bash
#!/bin/bash
# deploy.sh - 自动化部署脚本

set -e

PROJECT_ROOT="/opt/ljwx"
BACKUP_DIR="/opt/ljwx/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

echo "=== LJWX BigScreen 自动部署脚本 ==="
echo "部署时间: $(date)"
echo "项目路径: $PROJECT_ROOT"

# 1. 创建备份
echo "1. 创建当前版本备份..."
mkdir -p $BACKUP_DIR
if [ -d "$PROJECT_ROOT/bigscreen" ]; then
    tar -czf "$BACKUP_DIR/bigscreen_backup_$TIMESTAMP.tar.gz" -C "$PROJECT_ROOT" bigscreen
    echo "备份完成: $BACKUP_DIR/bigscreen_backup_$TIMESTAMP.tar.gz"
fi

# 2. 停止服务
echo "2. 停止现有服务..."
pkill -f "uvicorn.*bigscreen" || true
pkill -f "python.*bigscreen" || true
sleep 5

# 3. 更新代码
echo "3. 更新应用代码..."
cd $PROJECT_ROOT
git pull origin main

# 4. 安装依赖
echo "4. 安装Python依赖..."
cd $PROJECT_ROOT/ljwx-bigscreen
pip3 install -r requirements.txt

# 5. 数据库迁移
echo "5. 执行数据库迁移..."
mysql -u root -p123456 test < database_migrations.sql || true

# 6. 清理缓存
echo "6. 清理Redis缓存..."
redis-cli FLUSHDB || true

# 7. 启动服务
echo "7. 启动服务..."

# 启动多个实例
for port in 8001 8002 8003 8004; do
    cd $PROJECT_ROOT/ljwx-bigscreen/bigscreen
    nohup python3 -m uvicorn async_bigscreen:app --host 0.0.0.0 --port $port --workers 1 > /tmp/bigscreen_$port.log 2>&1 &
    echo "服务实例启动: 端口 $port"
    sleep 2
done

# 8. 健康检查
echo "8. 执行健康检查..."
sleep 10

for port in 8001 8002 8003 8004; do
    response=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:$port/api/health || echo "000")
    if [ "$response" = "200" ]; then
        echo "✅ 端口 $port 健康检查通过"
    else
        echo "❌ 端口 $port 健康检查失败 (HTTP $response)"
    fi
done

# 9. 重启Nginx
echo "9. 重启Nginx负载均衡..."
nginx -t && systemctl reload nginx

# 10. 启动监控
echo "10. 启动系统监控..."
cd $PROJECT_ROOT/scripts
nohup python3 system_monitor.py > /tmp/system_monitor.log 2>&1 &

echo "=== 部署完成 ==="
echo "访问地址: http://ljwx-bigscreen.local"
echo "监控日志: tail -f /tmp/system_monitor.log"
echo "服务日志: tail -f /tmp/bigscreen_*.log"
```

## 6. 压力测试脚本

### 6.1 高并发压测脚本

```python
# tests/stress_test_5000.py
import asyncio
import aiohttp
import time
import json
import random
from typing import List, Dict
import logging
from concurrent.futures import ThreadPoolExecutor

logging.basicConfig(level=logging.INFO)

class HighConcurrencyTester:
    def __init__(self):
        self.base_url = "http://127.0.0.1"  # Nginx地址
        self.concurrent_users = 5000
        self.test_duration = 300  # 5分钟
        self.batch_size = 100     # 每批次数据量
        self.results = []
        
    async def generate_test_batch(self, batch_id: str) -> Dict:
        """生成测试批次数据"""
        data_list = []
        for i in range(self.batch_size):
            data_list.append({
                "deviceSn": f"TEST_{random.randint(1, 1000):04d}",
                "heartRate": random.randint(60, 120),
                "bloodOxygen": random.randint(95, 100),
                "temperature": round(random.uniform(36.0, 37.5), 1),
                "pressureHigh": random.randint(110, 140),
                "pressureLow": random.randint(70, 90),
                "stress": random.randint(30, 80),
                "step": random.randint(8000, 15000),
                "distance": random.randint(5000, 10000),
                "calorie": random.randint(2000, 4000),
                "latitude": round(random.uniform(22.5, 22.6), 6),
                "longitude": round(random.uniform(114.0, 114.3), 6),
                "timestamp": int(time.time()),
                "uploadMethod": "stress_test"
            })
            
        return {
            "data": data_list,
            "batchId": batch_id
        }
        
    async def send_batch_request(self, session: aiohttp.ClientSession, user_id: int) -> Dict:
        """发送批次请求"""
        batch_id = f"stress_batch_{user_id}_{int(time.time() * 1000000)}"
        
        try:
            start_time = time.time()
            batch_data = await self.generate_test_batch(batch_id)
            
            async with session.post(
                f"{self.base_url}/upload_health_data_batch",
                json=batch_data,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                end_time = time.time()
                response_data = await response.json()
                
                return {
                    "user_id": user_id,
                    "batch_id": batch_id,
                    "status_code": response.status,
                    "response_time": end_time - start_time,
                    "records_sent": len(batch_data["data"]),
                    "response_data": response_data,
                    "timestamp": time.time()
                }
                
        except asyncio.TimeoutError:
            return {
                "user_id": user_id,
                "batch_id": batch_id,
                "status_code": 0,
                "response_time": 30.0,
                "error": "timeout",
                "timestamp": time.time()
            }
        except Exception as e:
            return {
                "user_id": user_id,
                "batch_id": batch_id, 
                "status_code": 0,
                "response_time": 0,
                "error": str(e),
                "timestamp": time.time()
            }
            
    async def simulate_user(self, user_id: int, session: aiohttp.ClientSession):
        """模拟单个用户的请求"""
        start_time = time.time()
        requests_sent = 0
        
        while time.time() - start_time < self.test_duration:
            result = await self.send_batch_request(session, user_id)
            self.results.append(result)
            requests_sent += 1
            
            # 随机间隔 (模拟真实用户行为)
            await asyncio.sleep(random.uniform(1, 5))
            
        logging.info(f"User {user_id} completed: {requests_sent} requests")
        
    async def run_stress_test(self):
        """运行压力测试"""
        logging.info(f"开始压力测试: {self.concurrent_users} 并发用户, 持续 {self.test_duration} 秒")
        start_time = time.time()
        
        # 创建连接器配置
        connector = aiohttp.TCPConnector(
            limit=0,  # 不限制总连接数
            limit_per_host=0,  # 不限制每个主机连接数
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=60)
        ) as session:
            # 创建所有用户任务
            tasks = []
            for user_id in range(self.concurrent_users):
                task = asyncio.create_task(self.simulate_user(user_id, session))
                tasks.append(task)
                
                # 分批启动，避免瞬时冲击
                if (user_id + 1) % 100 == 0:
                    await asyncio.sleep(0.1)
                    
            # 等待所有任务完成
            await asyncio.gather(*tasks, return_exceptions=True)
            
        end_time = time.time()
        total_duration = end_time - start_time
        
        # 分析结果
        await self.analyze_results(total_duration)
        
    async def analyze_results(self, total_duration: float):
        """分析测试结果"""
        if not self.results:
            logging.error("No results to analyze")
            return
            
        # 基础统计
        total_requests = len(self.results)
        successful_requests = len([r for r in self.results if r.get('status_code') == 200])
        failed_requests = total_requests - successful_requests
        
        # 响应时间统计
        response_times = [r['response_time'] for r in self.results if 'response_time' in r]
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            # 计算百分位数
            sorted_times = sorted(response_times)
            p50 = sorted_times[int(len(sorted_times) * 0.5)]
            p90 = sorted_times[int(len(sorted_times) * 0.9)]
            p95 = sorted_times[int(len(sorted_times) * 0.95)]
            p99 = sorted_times[int(len(sorted_times) * 0.99)]
        else:
            avg_response_time = min_response_time = max_response_time = 0
            p50 = p90 = p95 = p99 = 0
            
        # QPS计算
        qps = total_requests / total_duration if total_duration > 0 else 0
        successful_qps = successful_requests / total_duration if total_duration > 0 else 0
        
        # 错误分析
        error_types = {}
        for result in self.results:
            if result.get('status_code') != 200:
                error_type = result.get('error', f"HTTP_{result.get('status_code', 'Unknown')}")
                error_types[error_type] = error_types.get(error_type, 0) + 1
                
        # 输出结果报告
        print("\n" + "="*80)
        print("压力测试结果报告")
        print("="*80)
        print(f"测试配置:")
        print(f"  并发用户数: {self.concurrent_users}")
        print(f"  测试时长: {total_duration:.2f} 秒")
        print(f"  每批次数据量: {self.batch_size} 条")
        print()
        print(f"请求统计:")
        print(f"  总请求数: {total_requests}")
        print(f"  成功请求: {successful_requests}")
        print(f"  失败请求: {failed_requests}")
        print(f"  成功率: {(successful_requests/total_requests*100):.2f}%")
        print()
        print(f"性能指标:")
        print(f"  总QPS: {qps:.2f}")
        print(f"  成功QPS: {successful_qps:.2f}")
        print(f"  平均响应时间: {avg_response_time*1000:.2f} ms")
        print(f"  最小响应时间: {min_response_time*1000:.2f} ms")
        print(f"  最大响应时间: {max_response_time*1000:.2f} ms")
        print()
        print(f"响应时间分布:")
        print(f"  P50: {p50*1000:.2f} ms")
        print(f"  P90: {p90*1000:.2f} ms") 
        print(f"  P95: {p95*1000:.2f} ms")
        print(f"  P99: {p99*1000:.2f} ms")
        
        if error_types:
            print(f"\n错误分析:")
            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                print(f"  {error_type}: {count} 次 ({count/total_requests*100:.2f}%)")
                
        # 验收标准检查
        print(f"\n验收标准检查:")
        print(f"  目标QPS >= 5000: {'✅ PASS' if successful_qps >= 5000 else '❌ FAIL'} (实际: {successful_qps:.2f})")
        print(f"  平均响应时间 < 100ms: {'✅ PASS' if avg_response_time*1000 < 100 else '❌ FAIL'} (实际: {avg_response_time*1000:.2f}ms)")
        print(f"  P99响应时间 < 500ms: {'✅ PASS' if p99*1000 < 500 else '❌ FAIL'} (实际: {p99*1000:.2f}ms)")
        print(f"  错误率 < 0.1%: {'✅ PASS' if failed_requests/total_requests < 0.001 else '❌ FAIL'} (实际: {failed_requests/total_requests*100:.3f}%)")
        
        print("="*80)

if __name__ == "__main__":
    tester = HighConcurrencyTester()
    asyncio.run(tester.run_stress_test())
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u5206\u6790\u73b0\u6709\u5065\u5eb7\u6570\u636e\u4e0a\u4f20\u6d41\u7a0b", "status": "completed", "activeForm": "\u5206\u6790\u73b0\u6709\u5065\u5eb7\u6570\u636e\u4e0a\u4f20\u6d41\u7a0b"}, {"content": "\u7814\u7a76ljwx-watch\u6570\u636e\u4e0a\u4f20\u673a\u5236", "status": "completed", "activeForm": "\u7814\u7a76ljwx-watch\u6570\u636e\u4e0a\u4f20\u673a\u5236"}, {"content": "\u5206\u6790ljwx-bigscreen\u961f\u5217\u5904\u7406\u673a\u5236", "status": "completed", "activeForm": "\u5206\u6790ljwx-bigscreen\u961f\u5217\u5904\u7406\u673a\u5236"}, {"content": "\u5206\u6790ljwx-boot\u5b9a\u65f6\u4efb\u52a1\u5904\u7406", "status": "completed", "activeForm": "\u5206\u6790ljwx-boot\u5b9a\u65f6\u4efb\u52a1\u5904\u7406"}, {"content": "\u7f16\u5199\u6570\u636e\u6d41\u5206\u6790\u6587\u6863", "status": "completed", "activeForm": "\u7f16\u5199\u6570\u636e\u6d41\u5206\u6790\u6587\u6863"}, {"content": "\u8bbe\u8ba15000+\u5e76\u53d1\u4f18\u5316\u65b9\u6848", "status": "completed", "activeForm": "\u8bbe\u8ba15000+\u5e76\u53d1\u4f18\u5316\u65b9\u6848"}]