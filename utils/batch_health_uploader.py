#!/usr/bin/env python3
"""
æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼ ä¼˜åŒ–å™¨
å®ç°æ‰¹é‡æ’å…¥ä¼˜åŒ–ï¼Œæé«˜æ•°æ®åº“å†™å…¥æ€§èƒ½
"""

import asyncio
import aiohttp
import time
import json
import random
import logging
import mysql.connector
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import statistics

@dataclass
class BatchConfig:
    """æ‰¹é‡ä¼˜åŒ–é…ç½®"""
    base_url: str = "http://localhost:5225"
    batch_size: int = 50  # æ¯æ‰¹æ¬¡ä¸Šä¼ æ•°é‡
    concurrent_batches: int = 10  # å¹¶å‘æ‰¹æ¬¡æ•°
    test_duration_minutes: int = 3
    batch_interval_seconds: float = 0.1  # æ‰¹æ¬¡é—´éš”
    timeout_seconds: int = 30
    db_config: dict = None

@dataclass
class BatchStats:
    """æ‰¹é‡æµ‹è¯•ç»Ÿè®¡"""
    total_batches: int = 0
    total_records: int = 0
    successful_batches: int = 0
    failed_batches: int = 0
    successful_records: int = 0
    failed_records: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    batch_response_times: List[float] = None
    error_details: Dict[str, int] = None
    
    def __post_init__(self):
        if self.batch_response_times is None:
            self.batch_response_times = []
        if self.error_details is None:
            self.error_details = {}

class BatchHealthUploader:
    """æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼ ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: BatchConfig = None):
        self.config = config or BatchConfig()
        if not self.config.db_config:
            self.config.db_config = {
                'host': '127.0.0.1',
                'port': 3306,
                'database': 'test',
                'user': 'root',
                'password': '123456',
                'charset': 'utf8mb4'
            }
        
        self.stats = BatchStats()
        self.running = False
        self.user_devices = []
        
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = log_dir / f"batch_health_test_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸš€ æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼ ä¼˜åŒ–å™¨å¼€å§‹")
        self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    def load_real_users_and_devices(self) -> List[Tuple[int, str, str]]:
        """ä»æ•°æ®åº“åŠ è½½çœŸå®çš„ç”¨æˆ·å’Œè®¾å¤‡ä¿¡æ¯"""
        self.logger.info("ğŸ“Š åŠ è½½çœŸå®ç”¨æˆ·å’Œè®¾å¤‡ä¿¡æ¯...")
        
        try:
            connection = mysql.connector.connect(**self.config.db_config)
            cursor = connection.cursor()
            
            sql = """
            SELECT id, user_name, device_sn 
            FROM sys_user 
            WHERE user_name LIKE 'CRFTQ23409%' 
            AND device_sn IS NOT NULL
            ORDER BY id
            LIMIT 1000
            """
            
            cursor.execute(sql)
            results = cursor.fetchall()
            
            user_devices = []
            for user_id, user_name, device_sn in results:
                user_devices.append((user_id, user_name, device_sn))
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(user_devices)} ä¸ªç”¨æˆ·è®¾å¤‡ä¿¡æ¯")
            
            cursor.close()
            connection.close()
            
            return user_devices
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½ç”¨æˆ·è®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def generate_batch_health_data(self, user_devices: List[Tuple[int, str, str]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¸€æ‰¹å¥åº·æ•°æ®"""
        timestamp = datetime.now()
        batch_data = []
        
        for user_id, user_name, device_sn in user_devices:
            # åŸºäºç”¨æˆ·IDç”Ÿæˆä¸ªæ€§åŒ–æ•°æ®
            random.seed(user_id + int(timestamp.timestamp()))
            
            health_record = {
                "deviceSn": device_sn,
                "userId": user_id,
                "heart_rate": random.randint(60, 120),
                "blood_oxygen": random.randint(95, 100) if random.random() > 0.1 else 0,
                "body_temperature": f"{random.uniform(36.0, 37.5):.1f}",
                "step": random.randint(0, 15000),
                "distance": f"{random.uniform(0, 12):.1f}",
                "calorie": f"{random.uniform(0, 600):.1f}",
                "latitude": f"{random.uniform(22.5, 22.6):.6f}",
                "longitude": f"{random.uniform(113.9, 114.1):.6f}",
                "altitude": f"{random.uniform(0, 100):.1f}",
                "stress": random.randint(0, 100),
                "upload_method": random.choice(["wifi", "4g", "bluetooth"]),
                "blood_pressure_systolic": random.randint(110, 140),
                "blood_pressure_diastolic": random.randint(70, 90),
                "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            batch_data.append(health_record)
        
        return batch_data
    
    async def upload_batch_health_data(self, session: aiohttp.ClientSession, batch_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¸Šä¼ ä¸€æ‰¹å¥åº·æ•°æ®"""
        start_time = time.time()
        batch_size = len(batch_data)
        
        try:
            # æ„é€ æ‰¹é‡ä¸Šä¼ çš„æ•°æ®æ ¼å¼
            upload_data = {"data": batch_data}
            url = f"{self.config.base_url}/upload_health_data"
            
            async with session.post(
                url,
                json=upload_data,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout_seconds)
            ) as response:
                response_time = time.time() - start_time
                response_text = await response.text()
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats.total_batches += 1
                self.stats.total_records += batch_size
                self.stats.batch_response_times.append(response_time)
                
                if response.status == 200:
                    self.stats.successful_batches += 1
                    self.stats.successful_records += batch_size
                else:
                    self.stats.failed_batches += 1
                    self.stats.failed_records += batch_size
                    error_key = f"HTTP_{response.status}"
                    self.stats.error_details[error_key] = self.stats.error_details.get(error_key, 0) + 1
                
                return {
                    'batch_size': batch_size,
                    'success': response.status == 200,
                    'status_code': response.status,
                    'response_time': response_time,
                    'response_text': response_text[:100],
                    'timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            response_time = time.time() - start_time
            self.stats.total_batches += 1
            self.stats.total_records += batch_size
            self.stats.failed_batches += 1
            self.stats.failed_records += batch_size
            error_key = type(e).__name__
            self.stats.error_details[error_key] = self.stats.error_details.get(error_key, 0) + 1
            self.stats.batch_response_times.append(response_time)
            
            return {
                'batch_size': batch_size,
                'success': False,
                'error': str(e),
                'response_time': response_time,
                'timestamp': datetime.now().isoformat()
            }
    
    async def run_batch_stress_test(self):
        """è¿è¡Œæ‰¹é‡å‹åŠ›æµ‹è¯•"""
        self.logger.info("ğŸ”¥ å¼€å§‹æ‰¹é‡å¥åº·æ•°æ®å‹åŠ›æµ‹è¯•")
        
        # åŠ è½½çœŸå®ç”¨æˆ·è®¾å¤‡ä¿¡æ¯
        self.user_devices = self.load_real_users_and_devices()
        if not self.user_devices:
            self.logger.error("âŒ æ— æ³•åŠ è½½ç”¨æˆ·è®¾å¤‡ä¿¡æ¯ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return
        
        self.logger.info(f"ğŸ“Š æ‰¹é‡æµ‹è¯•é…ç½®:")
        self.logger.info(f"   - çœŸå®ç”¨æˆ·æ•°: {len(self.user_devices)}")
        self.logger.info(f"   - æ¯æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        self.logger.info(f"   - å¹¶å‘æ‰¹æ¬¡æ•°: {self.config.concurrent_batches}")
        self.logger.info(f"   - æµ‹è¯•æ—¶é•¿: {self.config.test_duration_minutes}åˆ†é’Ÿ")
        self.logger.info(f"   - æ‰¹æ¬¡é—´éš”: {self.config.batch_interval_seconds}ç§’")
        self.logger.info(f"   - ç›®æ ‡URL: {self.config.base_url}")
        
        self.running = True
        self.stats.start_time = datetime.now()
        
        # åˆ›å»ºHTTPä¼šè¯
        connector = aiohttp.TCPConnector(
            limit=self.config.concurrent_batches * 2,
            limit_per_host=self.config.concurrent_batches,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=self.config.timeout_seconds),
            headers={
                'Content-Type': 'application/json',
                'User-Agent': 'BatchHealthUploader/1.0'
            }
        ) as session:
            # å¯åŠ¨ç›‘æ§ä»»åŠ¡
            monitor_task = asyncio.create_task(self._monitor_progress())
            
            try:
                tasks = []
                end_time = datetime.now() + timedelta(minutes=self.config.test_duration_minutes)
                
                while datetime.now() < end_time and self.running:
                    # éšæœºé€‰æ‹©ç”¨æˆ·åˆ›å»ºæ‰¹æ¬¡
                    batch_users = random.sample(
                        self.user_devices,
                        min(self.config.batch_size, len(self.user_devices))
                    )
                    
                    # ç”Ÿæˆæ‰¹é‡æ•°æ®
                    batch_data = self.generate_batch_health_data(batch_users)
                    
                    # æ§åˆ¶å¹¶å‘æ‰¹æ¬¡æ•°
                    if len(tasks) >= self.config.concurrent_batches:
                        done, pending = await asyncio.wait(
                            tasks,
                            timeout=0.1,
                            return_when=asyncio.FIRST_COMPLETED
                        )
                        tasks = list(pending)
                    
                    # åˆ›å»ºæ‰¹é‡ä¸Šä¼ ä»»åŠ¡
                    task = asyncio.create_task(
                        self.upload_batch_health_data(session, batch_data)
                    )
                    tasks.append(task)
                    
                    # æ§åˆ¶æ‰¹æ¬¡é¢‘ç‡
                    await asyncio.sleep(self.config.batch_interval_seconds)
                
                # ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ
                if tasks:
                    self.logger.info(f"ç­‰å¾… {len(tasks)} ä¸ªå‰©ä½™æ‰¹æ¬¡å®Œæˆ...")
                    await asyncio.gather(*tasks, return_exceptions=True)
                
            finally:
                monitor_task.cancel()
                try:
                    await monitor_task
                except asyncio.CancelledError:
                    pass
        
        self.stats.end_time = datetime.now()
        self._print_final_report()
    
    async def _monitor_progress(self):
        """ç›‘æ§æµ‹è¯•è¿›åº¦"""
        last_records = 0
        
        while self.running:
            try:
                await asyncio.sleep(10)  # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡
                
                current_records = self.stats.total_records
                successful_records = self.stats.successful_records
                failed_records = self.stats.failed_records
                
                # è®¡ç®—è®°å½•QPS
                records_delta = current_records - last_records
                qps = records_delta / 10.0
                
                # è®¡ç®—æˆåŠŸç‡
                success_rate = (successful_records / current_records * 100) if current_records > 0 else 0
                
                # è®¡ç®—å¹³å‡æ‰¹æ¬¡å“åº”æ—¶é—´
                avg_batch_time = 0
                if self.stats.batch_response_times:
                    avg_batch_time = statistics.mean(self.stats.batch_response_times[-20:])
                
                elapsed_time = datetime.now() - self.stats.start_time if self.stats.start_time else timedelta(0)
                
                self.logger.info(
                    f"ğŸ“Š æ‰¹é‡æµ‹è¯•è¿›åº¦ - "
                    f"æ€»è®°å½•: {current_records}, "
                    f"æˆåŠŸ: {successful_records}, "
                    f"å¤±è´¥: {failed_records}, "
                    f"æˆåŠŸç‡: {success_rate:.1f}%, "
                    f"è®°å½•QPS: {qps:.1f}, "
                    f"æ‰¹æ¬¡æ•°: {self.stats.total_batches}, "
                    f"å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s, "
                    f"è¿è¡Œæ—¶é—´: {str(elapsed_time).split('.')[0]}"
                )
                
                last_records = current_records
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"ç›‘æ§è¿›åº¦å¼‚å¸¸: {e}")
    
    def _print_final_report(self):
        """æ‰“å°æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼ ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
        self.logger.info("=" * 80)
        
        if self.stats.start_time and self.stats.end_time:
            duration = self.stats.end_time - self.stats.start_time
            self.logger.info(f"â±ï¸  æµ‹è¯•æ—¶é•¿: {duration}")
            
            if self.stats.total_records > 0 and duration.total_seconds() > 0:
                records_qps = self.stats.total_records / duration.total_seconds()
                batches_qps = self.stats.total_batches / duration.total_seconds()
                self.logger.info(f"ğŸš€ è®°å½•QPS: {records_qps:.2f} è®°å½•/ç§’")
                self.logger.info(f"ğŸ“¦ æ‰¹æ¬¡QPS: {batches_qps:.2f} æ‰¹æ¬¡/ç§’")
                self.logger.info(f"ğŸ’ª å¤„ç†èƒ½åŠ›: {records_qps * 60:.0f} è®°å½•/åˆ†é’Ÿ")
        
        # æ‰¹æ¬¡ç»Ÿè®¡
        self.logger.info(f"ğŸ“¦ æ‰¹æ¬¡ç»Ÿè®¡:")
        self.logger.info(f"   - æ€»æ‰¹æ¬¡æ•°: {self.stats.total_batches}")
        self.logger.info(f"   - æˆåŠŸæ‰¹æ¬¡: {self.stats.successful_batches}")
        self.logger.info(f"   - å¤±è´¥æ‰¹æ¬¡: {self.stats.failed_batches}")
        self.logger.info(f"   - å¹³å‡æ‰¹æ¬¡å¤§å°: {self.stats.total_records / self.stats.total_batches:.1f}" if self.stats.total_batches > 0 else "   - å¹³å‡æ‰¹æ¬¡å¤§å°: 0")
        
        # è®°å½•ç»Ÿè®¡
        self.logger.info(f"ğŸ“ˆ è®°å½•ç»Ÿè®¡:")
        self.logger.info(f"   - æ€»è®°å½•æ•°: {self.stats.total_records}")
        self.logger.info(f"   - æˆåŠŸè®°å½•: {self.stats.successful_records}")
        self.logger.info(f"   - å¤±è´¥è®°å½•: {self.stats.failed_records}")
        
        # æˆåŠŸç‡
        if self.stats.total_records > 0:
            success_rate = (self.stats.successful_records / self.stats.total_records) * 100
            self.logger.info(f"âœ… æˆåŠŸç‡: {success_rate:.2f}%")
        
        # æ‰¹æ¬¡å“åº”æ—¶é—´ç»Ÿè®¡
        if self.stats.batch_response_times:
            response_times = self.stats.batch_response_times
            self.logger.info(f"âš¡ æ‰¹æ¬¡å“åº”æ—¶é—´ç»Ÿè®¡:")
            self.logger.info(f"   - å¹³å‡æ‰¹æ¬¡æ—¶é—´: {statistics.mean(response_times):.3f}ç§’")
            self.logger.info(f"   - æœ€å¿«æ‰¹æ¬¡æ—¶é—´: {min(response_times):.3f}ç§’")
            self.logger.info(f"   - æœ€æ…¢æ‰¹æ¬¡æ—¶é—´: {max(response_times):.3f}ç§’")
            if len(response_times) >= 20:
                self.logger.info(f"   - 95%æ‰¹æ¬¡æ—¶é—´: {statistics.quantiles(response_times, n=20)[18]:.3f}ç§’")
        
        # é”™è¯¯è¯¦æƒ…
        if self.stats.error_details:
            self.logger.info(f"âŒ é”™è¯¯è¯¦æƒ…:")
            for error_type, count in self.stats.error_details.items():
                self.logger.info(f"   - {error_type}: {count}æ¬¡")
        
        self._evaluate_batch_performance()
    
    def _evaluate_batch_performance(self):
        """è¯„ä¼°æ‰¹é‡å¤„ç†æ€§èƒ½"""
        self.logger.info("ğŸ¯ æ‰¹é‡å¤„ç†æ€§èƒ½è¯„ä¼°:")
        
        if not self.stats.batch_response_times or self.stats.total_records == 0:
            self.logger.info("   - æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¯„ä¼°")
            return
        
        success_rate = (self.stats.successful_records / self.stats.total_records) * 100
        avg_batch_time = statistics.mean(self.stats.batch_response_times)
        duration = self.stats.end_time - self.stats.start_time if self.stats.start_time and self.stats.end_time else timedelta(0)
        records_qps = self.stats.total_records / duration.total_seconds() if duration.total_seconds() > 0 else 0
        
        # æ‰¹é‡å¤„ç†ä¼˜åŠ¿åˆ†æ
        expected_single_qps = 75  # åŸºäºä¹‹å‰å•ä¸ªä¸Šä¼ æµ‹è¯•çš„ç»“æœ
        improvement_ratio = records_qps / expected_single_qps if expected_single_qps > 0 else 0
        
        self.logger.info(f"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
        self.logger.info(f"   - æ‰¹é‡å¤„ç†QPS: {records_qps:.1f}")
        self.logger.info(f"   - å•ä¸ªå¤„ç†QPS: {expected_single_qps}")
        self.logger.info(f"   - æ€§èƒ½æå‡å€æ•°: {improvement_ratio:.1f}x")
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        performance_grade = "ä¼˜ç§€"
        issues = []
        
        if success_rate < 95:
            performance_grade = "éœ€è¦ä¼˜åŒ–"
            issues.append(f"æˆåŠŸç‡åä½ ({success_rate:.1f}%)")
        
        if records_qps < 200:
            performance_grade = "éœ€è¦ä¼˜åŒ–" if performance_grade != "éœ€è¦ä¼˜åŒ–" else performance_grade
            issues.append(f"QPSä»éœ€æå‡ ({records_qps:.1f})")
        
        self.logger.info(f"   - æ‰¹é‡å¤„ç†ç­‰çº§: {performance_grade}")
        if issues:
            self.logger.info(f"   - å‘ç°é—®é¢˜:")
            for issue in issues:
                self.logger.info(f"     * {issue}")
        
        # ä¼˜åŒ–å»ºè®®
        if improvement_ratio > 2.0:
            self.logger.info("   âœ… æ‰¹é‡å¤„ç†ä¼˜åŒ–æ•ˆæœæ˜¾è‘—!")
        elif improvement_ratio > 1.5:
            self.logger.info("   âœ… æ‰¹é‡å¤„ç†æœ‰æ˜æ˜¾æ”¹å–„")
        else:
            self.logger.info("   âš ï¸ æ‰¹é‡å¤„ç†ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼ ä¼˜åŒ–æµ‹è¯•")
    print("ğŸ¯ éªŒè¯æ‰¹é‡æ’å…¥ä¼˜åŒ–æ•ˆæœ")
    print("=" * 60)
    
    import argparse
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¥åº·æ•°æ®ä¸Šä¼ ä¼˜åŒ–æµ‹è¯•')
    parser.add_argument('--batch-size', type=int, default=50, help='æ¯æ‰¹æ¬¡å¤§å° (é»˜è®¤: 50)')
    parser.add_argument('--concurrent', type=int, default=10, help='å¹¶å‘æ‰¹æ¬¡æ•° (é»˜è®¤: 10)')
    parser.add_argument('--duration', type=int, default=3, help='æµ‹è¯•æ—¶é•¿(åˆ†é’Ÿ) (é»˜è®¤: 3)')
    parser.add_argument('--url', type=str, default='http://localhost:5225', help='æœåŠ¡URL')
    parser.add_argument('--interval', type=float, default=0.1, help='æ‰¹æ¬¡é—´éš”(ç§’) (é»˜è®¤: 0.1)')
    
    args = parser.parse_args()
    
    config = BatchConfig(
        base_url=args.url,
        batch_size=args.batch_size,
        concurrent_batches=args.concurrent,
        test_duration_minutes=args.duration,
        batch_interval_seconds=args.interval
    )
    
    print(f"ğŸ“Š æ‰¹é‡æµ‹è¯•é…ç½®:")
    print(f"   - æ¯æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   - å¹¶å‘æ‰¹æ¬¡æ•°: {config.concurrent_batches}")
    print(f"   - æµ‹è¯•æ—¶é•¿: {config.test_duration_minutes}åˆ†é’Ÿ")
    print(f"   - æœåŠ¡åœ°å€: {config.base_url}")
    print(f"   - æ‰¹æ¬¡é—´éš”: {config.batch_interval_seconds}ç§’")
    print()
    
    try:
        confirm = input("ç¡®è®¤å¼€å§‹æ‰¹é‡ä¼˜åŒ–æµ‹è¯•? (y/N): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return
    except EOFError:
        print("éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨å¼€å§‹æµ‹è¯•...")
    
    uploader = BatchHealthUploader(config)
    
    try:
        asyncio.run(uploader.run_batch_stress_test())
    except KeyboardInterrupt:
        print("\nâ¸ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()