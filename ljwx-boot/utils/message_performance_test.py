#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LJWXæ¶ˆæ¯ç³»ç»ŸV2æ€§èƒ½æµ‹è¯•è„šæœ¬ - Pythonç‰ˆæœ¬

åŠŸèƒ½ç‰¹æ€§:
1. å…¨é¢çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶ (TPS, å»¶è¿Ÿ, ååé‡, å¹¶å‘)
2. V1/V2ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•
3. å®æ—¶æ€§èƒ½ç›‘æ§å’Œå¯è§†åŒ–æŠ¥å‘Š
4. å¤šç§æµ‹è¯•åœºæ™¯æ”¯æŒ
5. è‡ªåŠ¨åŒ–å‹åŠ›æµ‹è¯•
6. è¯¦ç»†çš„HTMLæŠ¥å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•:
python message_performance_test.py --host localhost:8080 --concurrent-users 50 --requests-per-user 100

ä½œè€…: jjgao
é¡¹ç›®: ljwx-boot
åˆ›å»ºæ—¶é—´: 2025-09-10
"""

import asyncio
import aiohttp
import argparse
import json
import time
import statistics
import logging
import sys
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from jinja2 import Template

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('message_performance_test.log')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    host: str = "localhost:8080"
    concurrent_users: int = 50
    requests_per_user: int = 100
    warmup_requests: int = 100
    request_timeout: int = 30
    test_duration_minutes: int = 5
    enable_v1_test: bool = True
    enable_v2_test: bool = True
    enable_stress_test: bool = True
    enable_database_test: bool = True
    output_dir: str = "performance_reports"

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    version: str
    total_requests: int
    success_requests: int
    failed_requests: int
    total_time_ms: int
    tps: float
    success_rate: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    error_codes: Dict[str, int]
    test_time: datetime
    response_times: List[float]

class MessagePerformanceTester:
    """æ¶ˆæ¯ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.base_url = f"http://{config.host}"
        self.v1_api_prefix = "/api/v1/messages"
        self.v2_api_prefix = "/api/v2/messages"
        
        # æ€§èƒ½æŒ‡æ ‡æ”¶é›†
        self.metrics = {}
        self.response_times = []
        self.error_codes = {}
        self.request_lock = threading.Lock()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(exist_ok=True)
        
    async def run_performance_test(self) -> Dict[str, PerformanceMetrics]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        logger.info("ğŸš€ å¯åŠ¨LJWXæ¶ˆæ¯ç³»ç»ŸV2æ€§èƒ½æµ‹è¯•")
        
        results = {}
        
        try:
            # 1. ç³»ç»Ÿé¢„çƒ­
            await self.warmup_system()
            
            # 2. V1ç³»ç»Ÿæµ‹è¯•
            if self.config.enable_v1_test:
                logger.info("ğŸ“Š å¼€å§‹V1ç³»ç»Ÿæ€§èƒ½æµ‹è¯•...")
                results['V1'] = await self.performance_test("V1", self.v1_api_prefix)
            
            # 3. V2ç³»ç»Ÿæµ‹è¯•  
            if self.config.enable_v2_test:
                logger.info("ğŸ“Š å¼€å§‹V2ç³»ç»Ÿæ€§èƒ½æµ‹è¯•...")
                results['V2'] = await self.performance_test("V2", self.v2_api_prefix)
            
            # 4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
            if len(results) > 1:
                self.generate_comparison_report(results)
            
            # 5. æ•°æ®åº“æ€§èƒ½æµ‹è¯•
            if self.config.enable_database_test:
                await self.database_performance_test()
            
            # 6. å‹åŠ›æµ‹è¯•
            if self.config.enable_stress_test:
                await self.stress_test()
            
            logger.info("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise
            
        return results
    
    async def warmup_system(self):
        """ç³»ç»Ÿé¢„çƒ­"""
        logger.info(f"ğŸ”¥ ç³»ç»Ÿé¢„çƒ­ä¸­... ({self.config.warmup_requests} ä¸ªè¯·æ±‚)")
        
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
            tasks = []
            for _ in range(self.config.warmup_requests):
                task = self.send_request(session, "GET", "/actuator/health", None)
                tasks.append(task)
            
            # æ‰§è¡Œé¢„çƒ­è¯·æ±‚ï¼ˆå¿½ç•¥ç»“æœå’Œé”™è¯¯ï¼‰
            await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç­‰å¾…ç³»ç»Ÿç¨³å®š
        await asyncio.sleep(5)
        logger.info("âœ… ç³»ç»Ÿé¢„çƒ­å®Œæˆ")
    
    async def performance_test(self, version: str, api_prefix: str) -> PerformanceMetrics:
        """æ‰§è¡Œæ€§èƒ½æµ‹è¯•"""
        logger.info(f"ğŸ§ª å¼€å§‹ {version} ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
        
        # é‡ç½®è®¡æ•°å™¨
        self.reset_counters()
        
        start_time = time.time()
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.request_timeout)
        ) as session:
            tasks = []
            for user_id in range(self.config.concurrent_users):
                task = self.execute_user_scenario(session, user_id, api_prefix)
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time_ms = int((end_time - start_time) * 1000)
        
        # æ„å»ºæ€§èƒ½ç»“æœ
        return self.build_performance_metrics(version, total_time_ms)
    
    async def execute_user_scenario(self, session: aiohttp.ClientSession, user_id: int, api_prefix: str):
        """æ‰§è¡Œå•ç”¨æˆ·æµ‹è¯•åœºæ™¯"""
        import random
        
        for i in range(self.config.requests_per_user):
            try:
                # éšæœºé€‰æ‹©æµ‹è¯•åœºæ™¯
                scenario = random.randint(0, 4)
                
                if scenario == 0:
                    await self.test_create_message(session, api_prefix, user_id)
                elif scenario == 1:
                    await self.test_query_messages(session, api_prefix, user_id)
                elif scenario == 2:
                    await self.test_batch_operations(session, api_prefix, user_id)
                elif scenario == 3:
                    await self.test_message_status(session, api_prefix, user_id)
                else:
                    await self.test_message_statistics(session, api_prefix, user_id)
                
                # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºé—´éš”
                await asyncio.sleep(random.uniform(0.05, 0.15))
                
            except Exception as e:
                logger.debug(f"ç”¨æˆ· {user_id} è¯·æ±‚å¤±è´¥: {e}")
    
    async def test_create_message(self, session: aiohttp.ClientSession, api_prefix: str, user_id: int):
        """æµ‹è¯•æ¶ˆæ¯åˆ›å»º"""
        message_data = {
            "deviceSn": f"TEST_DEVICE_{user_id}",
            "title": f"æ€§èƒ½æµ‹è¯•æ¶ˆæ¯ {int(time.time() * 1000)}",
            "message": f"è¿™æ˜¯ç”¨æˆ· {user_id} çš„æ€§èƒ½æµ‹è¯•æ¶ˆæ¯å†…å®¹",
            "messageType": "notification",
            "senderType": "system", 
            "receiverType": "device",
            "customerId": 1,
            "urgency": "medium",
            "priority": 3,
            "requireAck": False
        }
        
        await self.send_request(session, "POST", api_prefix, message_data)
    
    async def test_query_messages(self, session: aiohttp.ClientSession, api_prefix: str, user_id: int):
        """æµ‹è¯•æ¶ˆæ¯æŸ¥è¯¢"""
        query_params = f"?customerId=1&page=1&pageSize=20&deviceSn=TEST_DEVICE_{user_id}"
        await self.send_request(session, "GET", api_prefix + query_params, None)
    
    async def test_batch_operations(self, session: aiohttp.ClientSession, api_prefix: str, user_id: int):
        """æµ‹è¯•æ‰¹é‡æ“ä½œ"""
        batch_data = []
        for i in range(10):
            batch_data.append({
                "deviceSn": f"BATCH_DEVICE_{user_id}_{i}",
                "title": f"æ‰¹é‡æµ‹è¯•æ¶ˆæ¯ {i}",
                "message": f"æ‰¹é‡æµ‹è¯•å†…å®¹ {i}",
                "messageType": "notification",
                "senderType": "system",
                "receiverType": "device",
                "customerId": 1
            })
        
        await self.send_request(session, "POST", api_prefix + "/batch", batch_data)
    
    async def test_message_status(self, session: aiohttp.ClientSession, api_prefix: str, user_id: int):
        """æµ‹è¯•æ¶ˆæ¯çŠ¶æ€æ›´æ–°"""
        message_id = user_id + 1000
        status_data = {
            "messageStatus": "acknowledged",
            "updateTime": datetime.now().isoformat()
        }
        
        await self.send_request(session, "PUT", f"{api_prefix}/{message_id}", status_data)
    
    async def test_message_statistics(self, session: aiohttp.ClientSession, api_prefix: str, user_id: int):
        """æµ‹è¯•æ¶ˆæ¯ç»Ÿè®¡"""
        query_params = "?customerId=1&startDate=2025-09-01&endDate=2025-09-10"
        await self.send_request(session, "GET", api_prefix + "/statistics" + query_params, None)
    
    async def send_request(self, session: aiohttp.ClientSession, method: str, endpoint: str, data: Any):
        """å‘é€HTTPè¯·æ±‚å¹¶è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        request_start = time.time()
        
        with self.request_lock:
            self.metrics.setdefault('total_requests', 0)
            self.metrics['total_requests'] += 1
        
        try:
            url = self.base_url + endpoint
            
            if method == "GET":
                async with session.get(url) as response:
                    await response.text()
                    status_code = response.status
            else:
                json_data = json.dumps(data) if data else None
                headers = {'Content-Type': 'application/json'} if json_data else {}
                
                async with session.request(method, url, data=json_data, headers=headers) as response:
                    await response.text()
                    status_code = response.status
            
            # è®°å½•å“åº”æ—¶é—´
            response_time = (time.time() - request_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            with self.request_lock:
                self.response_times.append(response_time)
                
                # è®°å½•æˆåŠŸ/å¤±è´¥
                if 200 <= status_code < 300:
                    self.metrics.setdefault('success_requests', 0)
                    self.metrics['success_requests'] += 1
                else:
                    self.metrics.setdefault('failed_requests', 0)
                    self.metrics['failed_requests'] += 1
                    self.error_codes[str(status_code)] = self.error_codes.get(str(status_code), 0) + 1
            
        except Exception as e:
            response_time = (time.time() - request_start) * 1000
            
            with self.request_lock:
                self.response_times.append(response_time)
                self.metrics.setdefault('failed_requests', 0)
                self.metrics['failed_requests'] += 1
                self.error_codes['EXCEPTION'] = self.error_codes.get('EXCEPTION', 0) + 1
    
    async def database_performance_test(self):
        """æ•°æ®åº“æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸ—ƒï¸ å¼€å§‹æ•°æ®åº“æ€§èƒ½æµ‹è¯•...")
        
        # æµ‹è¯•æ‰¹é‡æ’å…¥
        await self.test_bulk_insert()
        
        # æµ‹è¯•å¤æ‚æŸ¥è¯¢
        await self.test_complex_queries()
        
        # æµ‹è¯•å¹¶å‘è¯»å†™
        await self.test_concurrent_read_write()
        
        logger.info("âœ… æ•°æ®åº“æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    async def test_bulk_insert(self):
        """æµ‹è¯•å¤§æ‰¹é‡æ’å…¥"""
        logger.info("æµ‹è¯•æ‰¹é‡æ’å…¥æ€§èƒ½ (1000æ¡è®°å½•)...")
        
        batch_data = []
        for i in range(1000):
            batch_data.append({
                "deviceSn": f"BULK_TEST_{i}",
                "title": f"æ‰¹é‡æ’å…¥æµ‹è¯• {i}",
                "message": f"æ‰¹é‡æ’å…¥æµ‹è¯•å†…å®¹ {i}",
                "messageType": "notification",
                "senderType": "system",
                "receiverType": "device", 
                "customerId": 1
            })
        
        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            await self.send_request(session, "POST", self.v2_api_prefix + "/bulk", batch_data)
            end_time = time.time()
            
            duration_ms = (end_time - start_time) * 1000
            tps = 1000 * 1000 / duration_ms if duration_ms > 0 else 0
            logger.info(f"æ‰¹é‡æ’å…¥å®Œæˆ: {duration_ms:.0f} ms, TPS: {tps:.2f}")
    
    async def test_complex_queries(self):
        """æµ‹è¯•å¤æ‚æŸ¥è¯¢"""
        logger.info("æµ‹è¯•å¤æ‚æŸ¥è¯¢æ€§èƒ½...")
        
        complex_queries = [
            "?customerId=1&messageType=notification&messageStatus=pending&startDate=2025-09-01&endDate=2025-09-10",
            "?deviceSn=TEST_DEVICE_1&urgency=high&priority=5", 
            "?orgId=1&senderType=system&receiverType=device&requireAck=true"
        ]
        
        async with aiohttp.ClientSession() as session:
            for query in complex_queries:
                start_time = time.time()
                await self.send_request(session, "GET", self.v2_api_prefix + query, None)
                end_time = time.time()
                logger.info(f"å¤æ‚æŸ¥è¯¢å®Œæˆ: {(end_time - start_time) * 1000:.0f} ms")
    
    async def test_concurrent_read_write(self):
        """æµ‹è¯•å¹¶å‘è¯»å†™"""
        logger.info("æµ‹è¯•å¹¶å‘è¯»å†™æ€§èƒ½...")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            # å¹¶å‘å†™æ“ä½œ
            for i in range(20):
                task = self.test_create_message(session, self.v2_api_prefix, i)
                tasks.append(task)
            
            # å¹¶å‘è¯»æ“ä½œ
            for i in range(30):
                task = self.test_query_messages(session, self.v2_api_prefix, i)
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.info("å¹¶å‘è¯»å†™æµ‹è¯•å®Œæˆ")
    
    async def stress_test(self):
        """å‹åŠ›æµ‹è¯•"""
        logger.info(f"ğŸ”¥ å¼€å§‹å‹åŠ›æµ‹è¯• (æŒç»­{self.config.test_duration_minutes}åˆ†é’Ÿ)...")
        
        test_duration = self.config.test_duration_minutes * 60  # è½¬æ¢ä¸ºç§’
        start_time = time.time()
        stress_requests = 0
        request_lock = threading.Lock()
        
        async def stress_worker(session: aiohttp.ClientSession, worker_id: int):
            nonlocal stress_requests
            import random
            
            while time.time() - start_time < test_duration:
                try:
                    await self.test_create_message(session, self.v2_api_prefix, random.randint(0, 1000))
                    with request_lock:
                        stress_requests += 1
                    await asyncio.sleep(0.1)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                except Exception:
                    pass  # å‹åŠ›æµ‹è¯•ä¸­å¿½ç•¥ä¸ªåˆ«é”™è¯¯
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(20):  # 20ä¸ªå¹¶å‘å·¥ä½œè€…
                task = stress_worker(session, i)
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)
        
        actual_duration = time.time() - start_time
        avg_tps = stress_requests / actual_duration if actual_duration > 0 else 0
        
        logger.info(f"å‹åŠ›æµ‹è¯•å®Œæˆ: æ€»è¯·æ±‚ {stress_requests}, å¹³å‡TPS {avg_tps:.2f}")
    
    def build_performance_metrics(self, version: str, total_time_ms: int) -> PerformanceMetrics:
        """æ„å»ºæ€§èƒ½æŒ‡æ ‡"""
        total_requests = self.metrics.get('total_requests', 0)
        success_requests = self.metrics.get('success_requests', 0) 
        failed_requests = self.metrics.get('failed_requests', 0)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        response_times_sorted = sorted(self.response_times)
        
        avg_response_time = statistics.mean(self.response_times) if self.response_times else 0
        min_response_time = min(self.response_times) if self.response_times else 0
        max_response_time = max(self.response_times) if self.response_times else 0
        
        p95_index = int(len(response_times_sorted) * 0.95)
        p99_index = int(len(response_times_sorted) * 0.99)
        p95_response_time = response_times_sorted[p95_index] if response_times_sorted else 0
        p99_response_time = response_times_sorted[p99_index] if response_times_sorted else 0
        
        tps = total_requests * 1000 / total_time_ms if total_time_ms > 0 else 0
        success_rate = success_requests * 100 / total_requests if total_requests > 0 else 0
        
        return PerformanceMetrics(
            version=version,
            total_requests=total_requests,
            success_requests=success_requests,
            failed_requests=failed_requests,
            total_time_ms=total_time_ms,
            tps=tps,
            success_rate=success_rate,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            error_codes=self.error_codes.copy(),
            test_time=datetime.now(),
            response_times=self.response_times.copy()
        )
    
    def reset_counters(self):
        """é‡ç½®è®¡æ•°å™¨"""
        self.metrics.clear()
        self.response_times.clear()
        self.error_codes.clear()
    
    def generate_comparison_report(self, results: Dict[str, PerformanceMetrics]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š...")
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self.generate_text_report(results)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_charts(results)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self.generate_html_report(results)
    
    def generate_text_report(self, results: Dict[str, PerformanceMetrics]):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("â•" * 70)
        report_lines.append("              LJWXæ¶ˆæ¯ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        report_lines.append("â•" * 70)
        report_lines.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"å¹¶å‘ç”¨æˆ·: {self.config.concurrent_users} ç”¨æˆ·")
        report_lines.append(f"æ€»è¯·æ±‚æ•°: {self.config.concurrent_users * self.config.requests_per_user} è¯·æ±‚")
        report_lines.append("â”€" * 70)
        
        # å„ç‰ˆæœ¬æ€§èƒ½æŒ‡æ ‡
        for version, metrics in results.items():
            report_lines.append(f"\nğŸ“Š {version}ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡:")
            report_lines.append(f"   TPS:           {metrics.tps:.2f} è¯·æ±‚/ç§’")
            report_lines.append(f"   æˆåŠŸç‡:        {metrics.success_rate:.2f}%")
            report_lines.append(f"   å¹³å‡å“åº”æ—¶é—´:  {metrics.avg_response_time:.2f} ms")
            report_lines.append(f"   P95å“åº”æ—¶é—´:   {metrics.p95_response_time:.0f} ms")
            report_lines.append(f"   P99å“åº”æ—¶é—´:   {metrics.p99_response_time:.0f} ms")
            report_lines.append(f"   æœ€å¤§å“åº”æ—¶é—´:  {metrics.max_response_time:.0f} ms")
            report_lines.append(f"   æ€»æ‰§è¡Œæ—¶é—´:    {metrics.total_time_ms / 1000:.2f} ç§’")
        
        # æ€§èƒ½å¯¹æ¯”ï¼ˆå¦‚æœæœ‰å¤šä¸ªç‰ˆæœ¬ï¼‰
        if len(results) >= 2:
            versions = list(results.keys())
            v1_metrics = results[versions[0]]
            v2_metrics = results[versions[1]]
            
            report_lines.append("\nğŸš€ æ€§èƒ½æå‡å¯¹æ¯”:")
            
            tps_improvement = self.calculate_improvement(v1_metrics.tps, v2_metrics.tps)
            response_improvement = self.calculate_improvement(
                v1_metrics.avg_response_time, v2_metrics.avg_response_time, lower_is_better=True
            )
            p95_improvement = self.calculate_improvement(
                v1_metrics.p95_response_time, v2_metrics.p95_response_time, lower_is_better=True
            )
            
            report_lines.append(f"   TPSæå‡:       {tps_improvement:.1f}% ({v1_metrics.tps:.2f} â†’ {v2_metrics.tps:.2f})")
            report_lines.append(f"   å“åº”æ—¶é—´ä¼˜åŒ–:  {response_improvement:.1f}% ({v1_metrics.avg_response_time:.2f} ms â†’ {v2_metrics.avg_response_time:.2f} ms)")
            report_lines.append(f"   P95å»¶è¿Ÿä¼˜åŒ–:   {p95_improvement:.1f}% ({v1_metrics.p95_response_time:.0f} ms â†’ {v2_metrics.p95_response_time:.0f} ms)")
            
            overall_improvement = (tps_improvement + response_improvement + p95_improvement) / 3
            grade = self.get_performance_grade(overall_improvement)
            report_lines.append(f"   æ•´ä½“æ€§èƒ½æå‡:  {overall_improvement:.1f}% ({grade})")
        
        report_lines.append("\n" + "â•" * 70)
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        report_text = "\n".join(report_lines)
        logger.info(report_text)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        report_file = Path(self.config.output_dir) / f"performance-report-{timestamp}.txt"
        report_file.write_text(report_text, encoding='utf-8')
        logger.info(f"ğŸ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def generate_charts(self, results: Dict[str, PerformanceMetrics]):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('LJWXæ¶ˆæ¯ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
            
            versions = list(results.keys())
            colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']
            
            # TPSå¯¹æ¯”
            ax1 = axes[0, 0]
            tps_values = [results[v].tps for v in versions]
            bars1 = ax1.bar(versions, tps_values, color=colors[:len(versions)])
            ax1.set_title('TPS (äº‹åŠ¡/ç§’)')
            ax1.set_ylabel('TPS')
            for i, v in enumerate(tps_values):
                ax1.text(i, v + max(tps_values) * 0.01, f'{v:.1f}', ha='center')
            
            # å“åº”æ—¶é—´å¯¹æ¯”
            ax2 = axes[0, 1]
            response_times = {
                'Average': [results[v].avg_response_time for v in versions],
                'P95': [results[v].p95_response_time for v in versions],
                'P99': [results[v].p99_response_time for v in versions]
            }
            
            x = range(len(versions))
            width = 0.25
            for i, (label, values) in enumerate(response_times.items()):
                ax2.bar([xi + i * width for xi in x], values, width, 
                       label=label, color=colors[i])
            
            ax2.set_title('å“åº”æ—¶é—´å¯¹æ¯” (ms)')
            ax2.set_ylabel('å“åº”æ—¶é—´ (ms)')
            ax2.set_xticks([xi + width for xi in x])
            ax2.set_xticklabels(versions)
            ax2.legend()
            
            # æˆåŠŸç‡å¯¹æ¯”
            ax3 = axes[1, 0]
            success_rates = [results[v].success_rate for v in versions]
            bars3 = ax3.bar(versions, success_rates, color=colors[:len(versions)])
            ax3.set_title('æˆåŠŸç‡ (%)')
            ax3.set_ylabel('æˆåŠŸç‡ (%)')
            ax3.set_ylim(0, 105)
            for i, v in enumerate(success_rates):
                ax3.text(i, v + 1, f'{v:.1f}%', ha='center')
            
            # å“åº”æ—¶é—´åˆ†å¸ƒå›¾
            ax4 = axes[1, 1]
            for i, version in enumerate(versions):
                metrics = results[version]
                if len(metrics.response_times) > 0:
                    ax4.hist(metrics.response_times, bins=50, alpha=0.7, 
                            label=f'{version} å“åº”æ—¶é—´', color=colors[i])
            
            ax4.set_title('å“åº”æ—¶é—´åˆ†å¸ƒ')
            ax4.set_xlabel('å“åº”æ—¶é—´ (ms)')
            ax4.set_ylabel('é¢‘æ¬¡')
            ax4.legend()
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            chart_file = Path(self.config.output_dir) / f"performance-charts-{timestamp}.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {chart_file}")
            
        except ImportError:
            logger.warning("âš ï¸  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    
    def generate_html_report(self, results: Dict[str, PerformanceMetrics]):
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        html_template = '''
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LJWXæ¶ˆæ¯ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body { font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1 { color: #333; text-align: center; border-bottom: 2px solid #4ecdc4; padding-bottom: 10px; }
        h2 { color: #555; margin-top: 30px; }
        .metrics-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
        .metrics-card { background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #4ecdc4; }
        .metric-item { display: flex; justify-content: space-between; margin: 8px 0; }
        .metric-label { font-weight: bold; color: #666; }
        .metric-value { color: #333; }
        .improvement { color: #28a745; font-weight: bold; }
        .degradation { color: #dc3545; font-weight: bold; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #4ecdc4; color: white; }
        .status { padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; }
        .success { background-color: #28a745; color: white; }
        .warning { background-color: #ffc107; color: black; }
        .error { background-color: #dc3545; color: white; }
        .footer { margin-top: 40px; text-align: center; color: #666; font-size: 12px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ LJWXæ¶ˆæ¯ç³»ç»ŸV2æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
        
        <div style="text-align: center; margin: 20px 0; color: #666;">
            <p>æµ‹è¯•æ—¶é—´: {{ test_time }} | å¹¶å‘ç”¨æˆ·: {{ concurrent_users }} | æ€»è¯·æ±‚: {{ total_requests }}</p>
        </div>
        
        <h2>ğŸ“Š æ€§èƒ½æŒ‡æ ‡æ¦‚è§ˆ</h2>
        <div class="metrics-grid">
            {% for version, metrics in results.items() %}
            <div class="metrics-card">
                <h3>{{ version }} ç³»ç»Ÿ</h3>
                <div class="metric-item">
                    <span class="metric-label">TPS (äº‹åŠ¡/ç§’):</span>
                    <span class="metric-value">{{ "%.2f"|format(metrics.tps) }}</span>
                </div>
                <div class="metric-item">
                    <span class="metric-label">æˆåŠŸç‡:</span>
                    <span class="metric-value">{{ "%.2f"|format(metrics.success_rate) }}%</span>
                </div>
                <div class="metric-item">
                    <span class="metric-label">å¹³å‡å“åº”æ—¶é—´:</span>
                    <span class="metric-value">{{ "%.2f"|format(metrics.avg_response_time) }} ms</span>
                </div>
                <div class="metric-item">
                    <span class="metric-label">P95å“åº”æ—¶é—´:</span>
                    <span class="metric-value">{{ "%.0f"|format(metrics.p95_response_time) }} ms</span>
                </div>
                <div class="metric-item">
                    <span class="metric-label">P99å“åº”æ—¶é—´:</span>
                    <span class="metric-value">{{ "%.0f"|format(metrics.p99_response_time) }} ms</span>
                </div>
            </div>
            {% endfor %}
        </div>
        
        {% if comparison %}
        <h2>ğŸš€ æ€§èƒ½æå‡åˆ†æ</h2>
        <table>
            <tr>
                <th>æŒ‡æ ‡</th>
                <th>V1ç³»ç»Ÿ</th>
                <th>V2ç³»ç»Ÿ</th>
                <th>æå‡å¹…åº¦</th>
                <th>çŠ¶æ€</th>
            </tr>
            <tr>
                <td>TPS (äº‹åŠ¡/ç§’)</td>
                <td>{{ "%.2f"|format(comparison.v1_tps) }}</td>
                <td>{{ "%.2f"|format(comparison.v2_tps) }}</td>
                <td>{{ "%.1f"|format(comparison.tps_improvement) }}%</td>
                <td><span class="status {{ 'success' if comparison.tps_improvement > 0 else 'error' }}">{{ 'UP' if comparison.tps_improvement > 0 else 'DOWN' }}</span></td>
            </tr>
            <tr>
                <td>å¹³å‡å“åº”æ—¶é—´</td>
                <td>{{ "%.2f"|format(comparison.v1_response_time) }} ms</td>
                <td>{{ "%.2f"|format(comparison.v2_response_time) }} ms</td>
                <td>{{ "%.1f"|format(comparison.response_improvement) }}%</td>
                <td><span class="status {{ 'success' if comparison.response_improvement > 0 else 'error' }}">{{ 'BETTER' if comparison.response_improvement > 0 else 'WORSE' }}</span></td>
            </tr>
            <tr>
                <td>P95å“åº”æ—¶é—´</td>
                <td>{{ "%.0f"|format(comparison.v1_p95) }} ms</td>
                <td>{{ "%.0f"|format(comparison.v2_p95) }} ms</td>
                <td>{{ "%.1f"|format(comparison.p95_improvement) }}%</td>
                <td><span class="status {{ 'success' if comparison.p95_improvement > 0 else 'error' }}">{{ 'BETTER' if comparison.p95_improvement > 0 else 'WORSE' }}</span></td>
            </tr>
        </table>
        
        <div style="margin: 20px 0; padding: 15px; background: #e8f4f8; border-radius: 8px;">
            <strong>æ•´ä½“æ€§èƒ½è¯„çº§:</strong> 
            <span class="status success">{{ comparison.grade }}</span>
            (å¹³å‡æå‡: {{ "%.1f"|format(comparison.overall_improvement) }}%)
        </div>
        {% endif %}
        
        <div class="footer">
            <p>LJWXæ¶ˆæ¯ç³»ç»ŸV2 - æ€§èƒ½æµ‹è¯•æŠ¥å‘Š | ç”Ÿæˆæ—¶é—´: {{ test_time }}</p>
        </div>
    </div>
</body>
</html>
        '''
        
        try:
            # å‡†å¤‡æ¨¡æ¿æ•°æ®
            template_data = {
                'results': results,
                'test_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'concurrent_users': self.config.concurrent_users,
                'total_requests': self.config.concurrent_users * self.config.requests_per_user,
                'comparison': None
            }
            
            # å¦‚æœæœ‰å¤šä¸ªç‰ˆæœ¬ï¼Œæ·»åŠ å¯¹æ¯”æ•°æ®
            if len(results) >= 2:
                versions = list(results.keys())
                v1_metrics = results[versions[0]]
                v2_metrics = results[versions[1]]
                
                tps_improvement = self.calculate_improvement(v1_metrics.tps, v2_metrics.tps)
                response_improvement = self.calculate_improvement(
                    v1_metrics.avg_response_time, v2_metrics.avg_response_time, lower_is_better=True
                )
                p95_improvement = self.calculate_improvement(
                    v1_metrics.p95_response_time, v2_metrics.p95_response_time, lower_is_better=True
                )
                overall_improvement = (tps_improvement + response_improvement + p95_improvement) / 3
                
                template_data['comparison'] = {
                    'v1_tps': v1_metrics.tps,
                    'v2_tps': v2_metrics.tps,
                    'tps_improvement': tps_improvement,
                    'v1_response_time': v1_metrics.avg_response_time,
                    'v2_response_time': v2_metrics.avg_response_time,
                    'response_improvement': response_improvement,
                    'v1_p95': v1_metrics.p95_response_time,
                    'v2_p95': v2_metrics.p95_response_time,
                    'p95_improvement': p95_improvement,
                    'overall_improvement': overall_improvement,
                    'grade': self.get_performance_grade(overall_improvement)
                }
            
            # æ¸²æŸ“æ¨¡æ¿
            template = Template(html_template)
            html_content = template.render(**template_data)
            
            # ä¿å­˜HTMLæŠ¥å‘Š
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            html_file = Path(self.config.output_dir) / f"performance-report-{timestamp}.html"
            html_file.write_text(html_content, encoding='utf-8')
            
            logger.info(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ä¿å­˜: {html_file}")
            
        except ImportError:
            logger.warning("âš ï¸  Jinja2 æœªå®‰è£…ï¼Œè·³è¿‡HTMLæŠ¥å‘Šç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}")
    
    def calculate_improvement(self, old_value: float, new_value: float, lower_is_better: bool = False) -> float:
        """è®¡ç®—æ€§èƒ½æå‡ç™¾åˆ†æ¯”"""
        if old_value == 0:
            return 0.0
        
        if lower_is_better:
            improvement = (old_value - new_value) / old_value * 100
        else:
            improvement = (new_value - old_value) / old_value * 100
        
        return round(improvement, 1)
    
    def get_performance_grade(self, improvement: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if improvement >= 50:
            return "ğŸ† å“è¶Š"
        elif improvement >= 30:
            return "ğŸ¥‡ ä¼˜ç§€"
        elif improvement >= 15:
            return "ğŸ¥ˆ è‰¯å¥½"
        elif improvement >= 5:
            return "ğŸ¥‰ ä¸€èˆ¬"
        else:
            return "âŒ éœ€æ”¹è¿›"

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='LJWXæ¶ˆæ¯ç³»ç»ŸV2æ€§èƒ½æµ‹è¯•')
    parser.add_argument('--host', default='localhost:8080', help='æµ‹è¯•ç›®æ ‡ä¸»æœº')
    parser.add_argument('--concurrent-users', type=int, default=50, help='å¹¶å‘ç”¨æˆ·æ•°')
    parser.add_argument('--requests-per-user', type=int, default=100, help='æ¯ç”¨æˆ·è¯·æ±‚æ•°')
    parser.add_argument('--warmup-requests', type=int, default=100, help='é¢„çƒ­è¯·æ±‚æ•°')
    parser.add_argument('--test-duration', type=int, default=5, help='å‹åŠ›æµ‹è¯•æŒç»­æ—¶é—´(åˆ†é’Ÿ)')
    parser.add_argument('--output-dir', default='performance_reports', help='æŠ¥å‘Šè¾“å‡ºç›®å½•')
    parser.add_argument('--disable-v1', action='store_true', help='ç¦ç”¨V1æµ‹è¯•')
    parser.add_argument('--disable-v2', action='store_true', help='ç¦ç”¨V2æµ‹è¯•')
    parser.add_argument('--disable-stress', action='store_true', help='ç¦ç”¨å‹åŠ›æµ‹è¯•')
    parser.add_argument('--disable-database', action='store_true', help='ç¦ç”¨æ•°æ®åº“æµ‹è¯•')
    
    args = parser.parse_args()
    
    # æ„å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        host=args.host,
        concurrent_users=args.concurrent_users,
        requests_per_user=args.requests_per_user,
        warmup_requests=args.warmup_requests,
        test_duration_minutes=args.test_duration,
        output_dir=args.output_dir,
        enable_v1_test=not args.disable_v1,
        enable_v2_test=not args.disable_v2,
        enable_stress_test=not args.disable_stress,
        enable_database_test=not args.disable_database
    )
    
    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•å™¨
    tester = MessagePerformanceTester(config)
    
    try:
        results = await tester.run_performance_test()
        logger.info("ğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•å·²å®Œæˆ")
        return 0
    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == '__main__':
    # æ£€æŸ¥ä¾èµ–
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        import pandas as pd
        from jinja2 import Template
    except ImportError as e:
        logger.warning(f"âš ï¸  å¯é€‰ä¾èµ–ç¼ºå¤±: {e}")
        logger.warning("è¿è¡Œ 'pip install matplotlib seaborn pandas jinja2' ä»¥å¯ç”¨å®Œæ•´åŠŸèƒ½")
    
    # è¿è¡Œæµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code)